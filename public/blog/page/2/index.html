<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
  <meta charset="utf-8">
  <title>Squid's Blog</title>
  <meta name="author" content="Carin Meier">

  
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://gigasquid.github.io/blog/page/2/">
  <link href="/favicon.png" type="image/png" rel="icon">
  <link href="/atom.xml" rel="alternate" title="Squid's Blog" type="application/atom+xml">

  <!-- http://opengraphprotocol.org/ -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://gigasquid.github.io/blog/page/2/">
  <meta property="og:title" content="Squid's Blog">
  

  <script src="/javascripts/libs/jquery/jquery-2.1.3.min.js"></script>

<link href="/assets/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/assets/bootstrap/dist/css/bootstrap-theme.min.css" rel="stylesheet" type="text/css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">


  
  <link href="/stylesheets/screen.css" media="screen, projection, print" rel="stylesheet" type="text/css">

  

</head>

  <body   >
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="wrap">
      
      <div id="banner-image">
        <div class="layer">
          <div id="banner-text">
            <h1 class="page-title">Squid's Blog</h1>
            <p class="page-desc">
              Random and Sometimes Useful Thoughts About Technology.<br>
            </p>
            <br>
            <br>
            <div class="page-links center">
              <ul class="index-nav">
                  <li ><a href="/blog/archives">Archives</a></li>
<li ><a href="/about">About</a></li>
<li ><a href="/books">Books</a></li>
<li ><a href="/speaking">Speaking</a></li>

              </ul>
            </div>
          </div>
        </div>
      </div>
      
      <div id="main" role="main" class="container">
        <div id="content">
          <div class="row">
  <div class="page-content">
    <div class="blog-index" itemscope itemtype="http://schema.org/Blog">
    <meta itemprop="name" content="Squid's Blog" />
    
    <meta itemprop="url" content="http://gigasquid.github.io" />
      
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2019-04-26T15:51:00-04:00"  data-updated="true" itemprop="datePublished dateCreated">Fri 26 Apr 2019,  3:51 PM</time>
        
           | <a href="/blog/2019/04/26/clojure-mxnet-april-update/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2019/04/26/clojure-mxnet-april-update/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2019/04/26/clojure-mxnet-april-update/" itemprop="url">Clojure MXNet April Update</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p>Spring is bringing some beautiful new things to the  <a href="http://mxnet.incubator.apache.org/">Clojure MXNet</a>. Here are some highlights for the month of April.</p>

<h2>Shipped</h2>

<p>We&rsquo;ve merged <a href="https://github.com/apache/incubator-mxnet/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+is%3Aclosed+clojure">10 PRs</a> over the last month. Many of them focus on core improvements to documentation and usability which is very important.</p>

<p>The MXNet project is also preparing a new release <code>1.4.1</code>, so keep on the lookout for that to hit in the near future.</p>

<h2>Clojure MXNet Made Simple Article Series</h2>

<p><a href="https://arthurcaillau.com/about/">Arthur Caillau</a> added another post to his fantastic series - <a href="https://arthurcaillau.com/mxnet-made-simple-pretrained-models/">MXNet made simple: Pretrained Models for image classification - Inception and VGG</a></p>

<h2>Cool Stuff in Development</h2>

<h3>New APIs</h3>

<p>Great progress was made on the <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103092678">new version of the API for the Clojure NDArray and Symbol APIs</a> by <a href="https://github.com/kedarbellare">Kedar Bellare</a>. We now have an experimental new version of the apis that are generated more directly from the C code so that we can have more control over the output.</p>

<p>For example the new version of the generated api for NDArray looks like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">defn</span>
</span><span class='line'> <span class="nv">activation</span>
</span><span class='line'> <span class="s">&quot;Applies an activation function element-wise to the input.</span>
</span><span class='line'><span class="s">  </span>
</span><span class='line'><span class="s">  The following activation functions are supported:</span>
</span><span class='line'><span class="s">  </span>
</span><span class='line'><span class="s">  - `relu`: Rectified Linear Unit, :math:`y = max(x, 0)`</span>
</span><span class='line'><span class="s">  - `sigmoid`: :math:`y = \\frac{1}{1 + exp(-x)}`</span>
</span><span class='line'><span class="s">  - `tanh`: Hyperbolic tangent, :math:`y = \\frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}`</span>
</span><span class='line'><span class="s">  - `softrelu`: Soft ReLU, or SoftPlus, :math:`y = log(1 + exp(x))`</span>
</span><span class='line'><span class="s">  - `softsign`: :math:`y = \\frac{x}{1 + abs(x)}`</span>
</span><span class='line'><span class="s">  </span>
</span><span class='line'><span class="s">  </span>
</span><span class='line'><span class="s">  </span>
</span><span class='line'><span class="s">  Defined in src/operator/nn/activation.cc:L167</span>
</span><span class='line'><span class="s">  </span>
</span><span class='line'><span class="s">  `data`: The input array.</span>
</span><span class='line'><span class="s">  `act-type`: Activation function to be applied.</span>
</span><span class='line'><span class="s">  `out`: Output array. (optional)&quot;</span>
</span><span class='line'> <span class="p">([</span><span class="nv">data</span> <span class="nv">act-type</span><span class="p">]</span> <span class="p">(</span><span class="nf">activation</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span>, <span class="ss">:act-type</span> <span class="nv">act-type</span><span class="p">}))</span>
</span><span class='line'> <span class="p">([{</span><span class="ss">:keys</span> <span class="p">[</span><span class="nv">data</span> <span class="nv">act-type</span> <span class="nv">out</span><span class="p">]</span>, <span class="ss">:or</span> <span class="p">{</span><span class="nv">out</span> <span class="nv">nil</span><span class="p">}</span>, <span class="ss">:as</span> <span class="nv">opts</span><span class="p">}]</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">util/coerce-return</span>
</span><span class='line'>   <span class="p">(</span><span class="nf">NDArrayAPI/Activation</span> <span class="nv">data</span> <span class="nv">act-type</span> <span class="p">(</span><span class="nf">util/-&gt;option</span> <span class="nv">out</span><span class="p">)))))</span>
</span></code></pre></td></tr></table></div></figure>


<p>as opposed to:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">defn</span>
</span><span class='line'> <span class="nv">activation</span>
</span><span class='line'> <span class="p">([</span><span class="o">&amp;</span> <span class="nv">nd-array-and-params</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">util/coerce-return</span>
</span><span class='line'>   <span class="p">(</span><span class="nf">NDArray/Activation</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">util/coerce-param</span>
</span><span class='line'>     <span class="nv">nd-array-and-params</span>
</span><span class='line'>     <span class="o">#</span><span class="p">{</span><span class="s">&quot;scala.collection.Seq&quot;</span><span class="p">})))))</span>
</span></code></pre></td></tr></table></div></figure>


<p>So much nicer!!!</p>

<h3>BERT (State of the Art for NLP)</h3>

<p>We also have some really exciting examples for BERT in a <a href="https://github.com/apache/incubator-mxnet/pull/14769">PR</a> that will be merged soon. If you are not familiar with BERT, this <a href="http://jalammar.github.io/illustrated-bert/">blog post</a> is a good overview. Basically, it&rsquo;s the state of the art in NLP right now. With the help of exported models from <a href="https://github.com/dmlc/gluon-nlp">GluonNLP</a>, we can do both inference and fine tuning of BERT models in MXNet with Clojure! This is an excellent example of cross fertilization across the GluonNLP, Scala, and Clojure MXNet projects.</p>

<p>There are two examples.</p>

<p>1) BERT question and answer inference based off of a fine tuned model of the <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD Dataset</a> in GluonNLP which is then exported. It allows one to actually do some natural language question and answering like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="nv">Question</span> <span class="nv">Answer</span> <span class="nv">Data</span>
</span><span class='line'><span class="p">{</span><span class="ss">:input-answer</span>
</span><span class='line'> <span class="s">&quot;Rich Hickey is the creator of the Clojure language. Before Clojure, he developed dotLisp, a similar project based on the .NET platform, and three earlier attempts to provide interoperability between Lisp and Java: a Java foreign language interface for Common Lisp, A Foreign Object Interface for Lisp, and a Lisp-friendly interface to Java Servlets.&quot;</span>,
</span><span class='line'> <span class="ss">:input-question</span> <span class="s">&quot;Who created Clojure?&quot;</span>,
</span><span class='line'> <span class="ss">:ground-truth-answers</span> <span class="p">[</span><span class="s">&quot;rich&quot;</span> <span class="s">&quot;hickey&quot;</span><span class="p">]}</span>
</span><span class='line'>
</span><span class='line'>  <span class="nv">Predicted</span> <span class="nv">Answer</span><span class="err">:</span>  <span class="p">[</span><span class="nv">rich</span> <span class="nv">hickey</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>2) The second example is using the exported BERT base model and then fine tuning it in Clojure to do a task with sentence pair classification to see if two sentences are equivalent or not.</p>

<p>The nice thing about this is that we were able to convert the existing <a href="https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html">tutorial in GluonNLP</a> over to a Clojure Jupyter notebook with the <code>lein-jupyter</code> plugin. I didn&rsquo;t realize that there is a nifty <code>save-as</code> command in Jupyter that can generate a markdown file, which makes for very handy documentation. Take a peek at the tutorial <a href="https://github.com/apache/incubator-mxnet/blob/d062d46f1c351dc9b70a038511b564dab5c43266/contrib/clojure-package/examples/bert/fine-tune-bert.md">here</a>. It might make its way into a blog post on its own in the next week or two.</p>

<h2>Upcoming Events</h2>

<ul>
<li><p>I&rsquo;ll be speaking about Clojure MXNet at the next <a href="https://twitter.com/scicloj">Scicloj Event</a> on May 15th at 10PM UTC. Please join us and get involved in making Clojure a great place for Data Science.</p></li>
<li><p>I&rsquo;m also really excited to attend <a href="https://iclr.cc/">ICLR</a> in a couple weeks. It is a <em>huge conference</em> that I&rsquo;m sure will melt my mind with the latest research in Deep Learning. If anyone else is planning to attend, please say hi :)</p></li>
</ul>


<h2>Get Involved</h2>

<p>As always, we welcome involvement in the true Apache tradition. If you have questions or want to say hi, head on over the the closest #mxnet room on your preferred server. We are on Clojurian&rsquo;s slack and Zulip</p>

<h2>Cat Picture of the Month</h2>

<p>To close out, let&rsquo;s take a lesson from my cats and don&rsquo;t forget the importance of naps.</p>

<p><img src="https://live.staticflickr.com/65535/47707608431_5c5d0c73f8_c.jpg"></p>

<p>Have a great rest of April!</p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2019-03-22T10:42:00-04:00"  data-updated="true" itemprop="datePublished dateCreated">Fri 22 Mar 2019, 10:42 AM</time>
        
           | <a href="/blog/2019/03/22/clojure-mxnet-march-update/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2019/03/22/clojure-mxnet-march-update/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2019/03/22/clojure-mxnet-march-update/" itemprop="url">Clojure MXNet March Update</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p>I&rsquo;m starting a monthly update for <a href="http://mxnet.incubator.apache.org/">Clojure MXNet</a>. The goal is to share the progress and exciting things that are happening in the project and our community.</p>

<p>Here&rsquo;s some highlights for the month of March.</p>

<h2>Shipped</h2>

<p>Under the shipped heading, the 1.4.0 release of MXNet has been released, along with the <a href="https://search.maven.org/search?q=clojure%20mxnet">Clojure MXNet Jars</a>. There have been improvements to the JVM memory management and an Image API addition. You can see the full list of changes <a href="https://github.com/apache/incubator-mxnet/releases/tag/1.4.0#clojure">here</a></p>

<h2>Clojure MXNet Made Simple Article Series</h2>

<p><a href="https://arthurcaillau.com/about/">Arthur Caillau</a> authored a really nice series of blog posts to help get people started with Clojure MXNet.</p>

<ul>
<li><a href="https://arthurcaillau.com/mxnet-clojure-aws/">Getting started with Clojure and MXNet on AWS</a></li>
<li><a href="https://arthurcaillau.com/mxnet-made-simple-ndarrays-api/">MXNet made simple: Clojure NDArray API</a></li>
<li><a href="https://arthurcaillau.com/mxnet-made-simple-symbol-api/">MXNet made simple: Clojure Symbol API</a></li>
<li><a href="https://arthurcaillau.com/mxnet-made-simple-module-api/">MXNet made simple: Clojure Module API</a></li>
<li><a href="https://arthurcaillau.com/mxnet-made-simple-symbol-visualization/">MXNet made simple: Clojure Symbol Visualization API</a></li>
<li><a href="https://arthurcaillau.com/mxnet-made-simple-image-manipulation/">MXNet made simple: Image Manipulation with OpenCV and MXNet</a></li>
</ul>


<h2>Lein Template &amp; Docker file</h2>

<p><a href="https://github.com/hellonico/">Nicolas Modrzyk</a> created a Leiningen template that allows you to easily get a MXNet project started - with a notebook too! It&rsquo;s a great way to take Clojure MXNet for a spin</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># create project
</span><span class='line'>lein new clj-mxnet hello
</span><span class='line'>
</span><span class='line'># run included sample
</span><span class='line'>lein run
</span><span class='line'>
</span><span class='line'># start notebook engine
</span><span class='line'>lein notebook
</span><span class='line'>
</span><span class='line'># open notebook
</span><span class='line'>http://0.0.0.0:10000/worksheet.html?filename=notes/practice.clj
</span><span class='line'># open empty notebook with all namespaces
</span><span class='line'>http://0.0.0.0:10000/worksheet.html?filename=notes/empty.clj</span></code></pre></td></tr></table></div></figure>


<p>There also is a docker file as well</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -it -p 10000:10000 hellonico/mxnet
</span><span class='line'>
</span><span class='line'>After starting the container, you can open the same notebooks as above:
</span><span class='line'>
</span><span class='line'># open notebook
</span><span class='line'>http://0.0.0.0:10000/worksheet.html?filename=notes/practice.clj
</span><span class='line'># open empty notebook with all namespaces
</span><span class='line'>http://0.0.0.0:10000/worksheet.html?filename=notes/empty.clj</span></code></pre></td></tr></table></div></figure>


<h2>Cool Stuff in Development</h2>

<p>There are a few really interesting things cooking for the future.</p>

<p>One is a <a href="https://github.com/apache/incubator-mxnet/pull/14372">PR for memory fixes</a> from the Scala team that is getting really close to merging. This will be a solution to some the the memory problems that were encountered by early adopters of the Module API.</p>

<p>Another, is the <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103092678">new version of the API for the Clojure NDArray and Symbol APIs</a> that is being spearheaded by Kedar Bellare</p>

<p>Finally, work is being started to create a <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103089990">Gluon API for the Clojure package</a> which is quite exciting.</p>

<h2>Get Involved</h2>

<p>As always, we welcome involvement in the true Apache tradition. If you have questions or want to say hi, head on over the the closest #mxnet room on your preferred server. We are on Clojurian&rsquo;s slack and Zulip.</p>

<h2>Cat Picture of the Month</h2>

<p>There is no better way to close out an update than a cat picture, so here is a picture of my family cat watching birds at the window.</p>

<p><img src="https://farm8.staticflickr.com/7862/46718997174_13bf6e88ea_z.jpg"></p>

<p>Have a great rest of March!</p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2019-01-19T13:34:00-05:00"  data-updated="true" itemprop="datePublished dateCreated">Sat 19 Jan 2019,  1:34 PM</time>
        
           | <a href="/blog/2019/01/19/object-detection-with-clojure-mxnet/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2019/01/19/object-detection-with-clojure-mxnet/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2019/01/19/object-detection-with-clojure-mxnet/" itemprop="url">Object Detection With Clojure MXNet</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p><img src="https://c1.staticflickr.com/8/7837/32928474208_4960caafb3.jpg" alt="" /></p>

<p>Object detection just landed in MXNet thanks to the work of contributors <a href="https://github.com/kedarbellare">Kedar Bellare</a> and <a href="https://github.com/hellonico/">Nicolas Modrzyk</a>. Kedar ported over the <code>infer</code> package to Clojure, making inference and prediction much easier for users and Nicolas integrated in his <a href="https://github.com/hellonico/origami">Origami</a> OpenCV library into the the examples to make the visualizations happen.</p>

<p>We&rsquo;ll walk through the main steps to use the <code>infer</code> object detection which include creating the detector with a model and then loading the image and running the inference on it.</p>

<h3>Creating the Detector</h3>

<p>To create the detector you need to define a couple of things:</p>

<ul>
<li>How big is your image?</li>
<li>What model are you going to be using for object detection?</li>
</ul>


<p>In the code below, we are going to be giving it an color image of size 512 x 512.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">defn </span><span class="nv">create-detector</span> <span class="p">[]</span>
</span><span class='line'>  <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">descriptors</span> <span class="p">[{</span><span class="ss">:name</span> <span class="s">&quot;data&quot;</span>
</span><span class='line'>                      <span class="ss">:shape</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">3</span> <span class="mi">512</span> <span class="mi">512</span><span class="p">]</span>
</span><span class='line'>                      <span class="ss">:layout</span> <span class="nv">layout/NCHW</span>
</span><span class='line'>                      <span class="ss">:dtype</span> <span class="nv">dtype/FLOAT32</span><span class="p">}]</span>
</span><span class='line'>        <span class="nv">factory</span> <span class="p">(</span><span class="nf">infer/model-factory</span> <span class="nv">model-path-prefix</span> <span class="nv">descriptors</span><span class="p">)]</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">infer/create-object-detector</span> <span class="nv">factory</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>The shape is going to be <code>[1 3 512 512]</code>.

<ul>
<li>The <code>1</code> is for the batch size which in our case is a single image.</li>
<li>The <code>3</code> is for the channels in the image which for a RGB image is <code>3</code></li>
<li>The <code>512</code> is for the image height and width.</li>
</ul>
</li>
<li>The <code>layout</code> specifies that the shape given is in terms of <code>NCHW</code> which is batch size, channel size, height, and width.</li>
<li>The <code>dtype</code> is the image data type which will be the standard <code>FLOAT32</code></li>
<li>The <code>model-path-prefix</code> points to the place where the trained model we are using for object detection lives.</li>
</ul>


<p>The model we are going to use is the <a href="https://arxiv.org/abs/1512.02325">Single Shot Multiple Box Object Detector (SSD)</a>. You can download the model yourself using this <a href="https://github.com/apache/incubator-mxnet/blob/master/contrib/clojure-package/examples/infer/objectdetector/scripts/get_ssd_data.sh">script</a>.</p>

<h3>How to Load an Image and Run the Detector</h3>

<p>Now that we have a model and a detector, we can load an image up and run the object detection.</p>

<p>To load the image use <code>load-image</code> which will load the image from the path.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">infer/load-image-from-file</span> <span class="nv">input-image</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then run the detection using <code>infer/detect-objects</code> which will give you the top five predictions by default.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">infer/detect-objects</span> <span class="nv">detector</span> <span class="nv">image</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>It will give an output something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">[[{</span><span class="ss">:class</span> <span class="s">&quot;person&quot;</span>,
</span><span class='line'>   <span class="ss">:prob</span> <span class="mf">0.9657765</span>,
</span><span class='line'>   <span class="ss">:x-min</span> <span class="mf">0.021868259</span>,
</span><span class='line'>   <span class="ss">:y-min</span> <span class="mf">0.049295247</span>,
</span><span class='line'>   <span class="ss">:x-max</span> <span class="mf">0.9975169</span>,
</span><span class='line'>   <span class="ss">:y-max</span> <span class="mf">0.9734151</span><span class="p">}</span>
</span><span class='line'>  <span class="p">{</span><span class="ss">:class</span> <span class="s">&quot;dog&quot;</span>,
</span><span class='line'>   <span class="ss">:prob</span> <span class="mf">0.17513266</span>,
</span><span class='line'>   <span class="ss">:x-min</span> <span class="mf">0.16772352</span>,
</span><span class='line'>   <span class="ss">:y-min</span> <span class="mf">0.45792937</span>,
</span><span class='line'>   <span class="ss">:x-max</span> <span class="mf">0.55409217</span>,
</span><span class='line'>   <span class="ss">:y-max</span> <span class="mf">0.72507095</span><span class="p">}</span>
</span><span class='line'>   <span class="nv">...</span>
</span><span class='line'><span class="p">]]</span>
</span></code></pre></td></tr></table></div></figure>


<p>which you can then use to draw bounding boxes on the image.</p>

<h3>Try Running the Example</h3>

<p><img src="https://c1.staticflickr.com/8/7804/31862638207_61be3a6e3c_b.jpg" alt="" /></p>

<p>One of the best ways to explore using it is with the <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/infer/objectdetector">object detection example</a> in the MXNet repo. It will be coming out officially in the <code>1.5.0</code> release, but you can get an early peek at it by building the project and running the example with the nightly snapshot.</p>

<p>You can do this by cloning the <a href="https://github.com/apache/incubator-mxnet">MXNet Repo</a> and changing directory to <code>contrib/clojure-package</code>.</p>

<p>Next, edit the <code>project.clj</code> to look like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">defproject </span><span class="nv">org.apache.mxnet.contrib.clojure/clojure-mxnet</span> <span class="s">&quot;1.5.0-SNAPSHOT&quot;</span>
</span><span class='line'>  <span class="ss">:description</span> <span class="s">&quot;Clojure package for MXNet&quot;</span>
</span><span class='line'>  <span class="ss">:url</span> <span class="s">&quot;https://github.com/apache/incubator-mxnet&quot;</span>
</span><span class='line'>  <span class="ss">:license</span> <span class="p">{</span><span class="ss">:name</span> <span class="s">&quot;Apache License&quot;</span>
</span><span class='line'>            <span class="ss">:url</span> <span class="s">&quot;http://www.apache.org/licenses/LICENSE-2.0&quot;</span><span class="p">}</span>
</span><span class='line'>  <span class="ss">:dependencies</span> <span class="p">[[</span><span class="nv">org.clojure/clojure</span> <span class="s">&quot;1.9.0&quot;</span><span class="p">]</span>
</span><span class='line'>                 <span class="p">[</span><span class="nv">t6/from-scala</span> <span class="s">&quot;0.3.0&quot;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'>                 <span class="c1">;; To use with nightly snapshot</span>
</span><span class='line'>                 <span class="c1">;[org.apache.mxnet/mxnet-full_2.11-osx-x86_64-cpu &quot;&lt;insert-snapshot-version&gt;&quot;]</span>
</span><span class='line'>                 <span class="c1">;[org.apache.mxnet/mxnet-full_2.11-linux-x86_64-cpu &quot;&lt;insert-snapshot-version&gt;&quot;]</span>
</span><span class='line'>                 <span class="c1">;[org.apache.mxnet/mxnet-full_2.11-linux-x86_64-gpu &quot;&lt;insert-snapshot-version&quot;]</span>
</span><span class='line'>
</span><span class='line'>                 <span class="p">[</span><span class="nv">org.apache.mxnet/mxnet-full_2.11-osx-x86_64-cpu</span> <span class="s">&quot;1.5.0-SNAPSHOT&quot;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'>                 <span class="c1">;;; CI</span>
</span><span class='line'>                 <span class="o">#</span><span class="nv">_</span><span class="p">[</span><span class="nv">org.apache.mxnet/mxnet-full_2.11</span> <span class="s">&quot;INTERNAL&quot;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'>                 <span class="p">[</span><span class="nv">org.clojure/tools.logging</span> <span class="s">&quot;0.4.0&quot;</span><span class="p">]</span>
</span><span class='line'>                 <span class="p">[</span><span class="nv">org.apache.logging.log4j/log4j-core</span> <span class="s">&quot;2.8.1&quot;</span><span class="p">]</span>
</span><span class='line'>                 <span class="p">[</span><span class="nv">org.apache.logging.log4j/log4j-api</span> <span class="s">&quot;2.8.1&quot;</span><span class="p">]</span>
</span><span class='line'>                 <span class="p">[</span><span class="nv">org.slf4j/slf4j-log4j12</span> <span class="s">&quot;1.7.25&quot;</span> <span class="ss">:exclusions</span> <span class="p">[</span><span class="nv">org.slf4j/slf4j-api</span><span class="p">]]]</span>
</span><span class='line'>  <span class="ss">:pedantic?</span> <span class="ss">:skip</span>
</span><span class='line'>  <span class="ss">:plugins</span> <span class="p">[[</span><span class="nv">lein-codox</span> <span class="s">&quot;0.10.3&quot;</span> <span class="ss">:exclusions</span> <span class="p">[</span><span class="nv">org.clojure/clojure</span><span class="p">]]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">lein-cloverage</span> <span class="s">&quot;1.0.10&quot;</span> <span class="ss">:exclusions</span> <span class="p">[</span><span class="nv">org.clojure/clojure</span><span class="p">]]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">lein-cljfmt</span> <span class="s">&quot;0.5.7&quot;</span><span class="p">]]</span>
</span><span class='line'>  <span class="ss">:codox</span> <span class="p">{</span><span class="ss">:namespaces</span> <span class="p">[</span><span class="o">#</span><span class="s">&quot;^org\.apache\.clojure-mxnet\.(?!gen).*&quot;</span><span class="p">]}</span>
</span><span class='line'>  <span class="ss">:aot</span> <span class="p">[</span><span class="nv">dev.generator</span><span class="p">]</span>
</span><span class='line'>  <span class="ss">:repositories</span> <span class="p">[[</span><span class="s">&quot;staging&quot;</span> <span class="p">{</span><span class="ss">:url</span> <span class="s">&quot;https://repository.apache.org/content/repositories/staging&quot;</span>                  <span class="ss">:snapshots</span> <span class="nv">true</span>
</span><span class='line'>                             <span class="ss">:update</span> <span class="ss">:always</span><span class="p">}]</span>
</span><span class='line'>                 <span class="p">[</span><span class="s">&quot;snapshots&quot;</span> <span class="p">{</span><span class="ss">:url</span> <span class="s">&quot;https://repository.apache.org/content/repositories/snapshots&quot;</span>               <span class="ss">:snapshots</span> <span class="nv">true</span>
</span><span class='line'>                              <span class="ss">:update</span> <span class="ss">:always</span><span class="p">}]])</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you are running on linux, you should change the <code>mxnet-full_2.11-osx-x86_64-cpu</code> to <code>mxnet-full_2.11-linux-x86_64-cpu</code>.</p>

<p>Next, go ahead and do <code>lein test</code> to make sure that everything builds ok. If you run into any trouble please refer to <a href="https://github.com/apache/incubator-mxnet/blob/master/contrib/clojure-package/README.md">README</a> for any missing dependencies.</p>

<p>After that do a <code>lein install</code> to install the <code>clojure-mxnet</code> jar to your local maven. Now you are ready to <code>cd examples/infer/object-detection</code> to try it out. Refer to the README for more details.</p>

<p>If you run into any problems getting started, feel free to reach out in the Clojurian #mxnet slack room or open an issue at the MXNet project. We are a friendly group and happy to help out.</p>

<p>Thanks again to the community for the contributions to make this possible. It&rsquo;s great seeing new things coming to life.</p>

<p>Happy Object Detecting!</p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2018-12-18T16:34:00-05:00"  data-updated="true" itemprop="datePublished dateCreated">Tue 18 Dec 2018,  4:34 PM</time>
        
           | <a href="/blog/2018/12/18/how-to-gan-a-flan/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2018/12/18/how-to-gan-a-flan/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2018/12/18/how-to-gan-a-flan/" itemprop="url">How to GAN a Flan</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p>It&rsquo;s holiday time and that means parties and getting together with friends. Bringing a baked good or dessert to a gathering is a time honored tradition. But what if this year, you could take it to the next level? Everyone brings actual food. But with the help of Deep Learning, you can bring something completely different -  you can bring the <em>image</em> of baked good! I&rsquo;m not talking about just any old image that someone captured with a camera or created with a pen and paper. I&rsquo;m talking about the computer itself <strong>creating</strong>. This image would be never before seen, totally unique, and crafted by the creative process of the machine.</p>

<p>That is exactly what we are going to do. We are going to create a <em>flan</em></p>

<p><img src="https://c1.staticflickr.com/5/4065/4339500429_aa9c55f246_n.jpg" alt="Photo by Lucia Sanchez on Flickr" /></p>

<p>If you&rsquo;ve never had a flan before, it&rsquo;s a yummy dessert made of a baked custard with caramel sauce on it.</p>

<p>&ldquo;Why a flan?&rdquo;, you may ask. There are quite a few reasons:</p>

<ul>
<li>It&rsquo;s tasty in real life.</li>
<li>Flan rhymes with GAN, <em>(unless you pronounce it &ldquo;Gaaahn&rdquo;)</em>.</li>
<li>Why not?</li>
</ul>


<p>Onto the recipe. How are we actually going to make this work? We need some ingredients:</p>

<ul>
<li><a href="https://clojure.org/">Clojure</a> - the most advanced programming language to create generative desserts.</li>
<li><a href="https://mxnet.apache.org">Apache MXNet</a> - a flexible and efficient deep learning library that has a Clojure package.</li>
<li>1000-5000 pictures of flans - for Deep Learning you need data!</li>
</ul>


<h2>Gather Flan Pictures</h2>

<p>The first thing you want to do is gather your 1000 or more images with a <a href="https://github.com/montoyamoraga/scrapers">scraper</a>. The scraper will crawl google, bing, or instagram and download pictures of <em>mostly</em> flans to your computer. You may have to eyeball and remove any clearly wrong ones from your stash.</p>

<p>Next, you need to gather all these images in a directory and run a tool called <a href="https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py">im2rec.py</a> on them to turn them into an <a href="https://mxnet.incubator.apache.org/tutorials/basic/data.html#loading-data-using-image-iterators">image record iterator</a> for use with MXNet. This will produce an optimized format that will allow our deep learning program to efficiently cycle through them.</p>

<p>Run:</p>

<pre><code>python3 im2rec.py --resize 28 root flan
</code></pre>

<p>to produce a <code>flan.rec</code> file with images resized to 28x28 that we can use next.</p>

<h2>Load Flan Pictures into MXNet</h2>

<p>The next step is to import the image record iterator into the MXNet with the <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">Clojure API</a>. We can do this with the <code>io</code> namespace.</p>

<p>Add this to your require:</p>

<pre><code>[org.apache.clojure-mxnet.io :as mx-io]
</code></pre>

<p>Now, we can load our images:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">flan-iter</span> <span class="p">(</span><span class="nf">mx-io/image-record-iter</span> <span class="p">{</span><span class="ss">:path-imgrec</span> <span class="s">&quot;flan.rec&quot;</span>
</span><span class='line'>                                         <span class="ss">:data-shape</span> <span class="p">[</span><span class="mi">3</span> <span class="mi">28</span> <span class="mi">28</span><span class="p">]</span>
</span><span class='line'>                                         <span class="ss">:batch-size</span> <span class="nv">batch-size</span><span class="p">}))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, that we have the images, we need to create our <code>model</code>. This is what is actually going to do the learning and creating of images.</p>

<h2>Creating a GAN model.</h2>

<p>GAN stands for <em>Generative Adversarial Network</em>. This is a incredibly cool deep learning technique that has two different models pitted against each, yet both learning and getting better at the same time. The two models are a generator and a discriminator. The generator model creates a new image from a random noise vector. The discriminator then tries to tell whether the image is a real image or a fake image. We need to create both of these models for our network.</p>

<p>First, the discriminator model. We are going to use the <code>symbol</code> namespace for the clojure package:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">defn </span><span class="nv">discriminator</span> <span class="p">[]</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">as-&gt;</span> <span class="p">(</span><span class="nf">sym/variable</span> <span class="s">&quot;data&quot;</span><span class="p">)</span> <span class="nv">data</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">sym/convolution</span> <span class="s">&quot;d1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span>
</span><span class='line'>                           <span class="ss">:kernel</span> <span class="p">[</span><span class="mi">4</span> <span class="mi">4</span><span class="p">]</span>
</span><span class='line'>                           <span class="ss">:pad</span> <span class="p">[</span><span class="mi">3</span> <span class="mi">3</span><span class="p">]</span>
</span><span class='line'>                           <span class="ss">:stride</span> <span class="p">[</span><span class="mi">2</span> <span class="mi">2</span><span class="p">]</span>
</span><span class='line'>                           <span class="ss">:num-filter</span> <span class="nv">ndf</span>
</span><span class='line'>                           <span class="ss">:no-bias</span> <span class="nv">true</span><span class="p">})</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">sym/batch-norm</span> <span class="s">&quot;dbn1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:fix-gamma</span> <span class="nv">true</span> <span class="ss">:eps</span> <span class="nv">eps</span><span class="p">})</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">sym/leaky-re-lu</span> <span class="s">&quot;dact1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:act-type</span> <span class="s">&quot;leaky&quot;</span> <span class="ss">:slope</span> <span class="mf">0.2</span><span class="p">})</span>
</span><span class='line'>
</span><span class='line'>  <span class="nv">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>There is a variable for the <code>data</code> coming in, (which is the picture of the flan), it then flows through the other layers which consist of convolutions, normalization, and activation layers. The last three layers actually repeat another two times before ending in the output, which tells whether it thinks the image was a fake or not.</p>

<p>The generator model looks similar:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">defn </span><span class="nv">generator</span> <span class="p">[]</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">as-&gt;</span> <span class="p">(</span><span class="nf">sym/variable</span> <span class="s">&quot;rand&quot;</span><span class="p">)</span> <span class="nv">data</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">sym/deconvolution</span> <span class="s">&quot;g1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span>
</span><span class='line'>                             <span class="ss">:kernel</span> <span class="p">[</span><span class="mi">4</span> <span class="mi">4</span><span class="p">]</span>
</span><span class='line'>                             <span class="ss">:pad</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
</span><span class='line'>                             <span class="ss">:stride</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">1</span><span class="p">]</span>
</span><span class='line'>                             <span class="ss">:num-filter</span>
</span><span class='line'>                             <span class="p">(</span><span class="nb">* </span><span class="mi">4</span> <span class="nv">ndf</span><span class="p">)</span> <span class="ss">:no-bias</span> <span class="nv">true</span><span class="p">})</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">sym/batch-norm</span> <span class="s">&quot;gbn1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:fix-gamma</span> <span class="nv">true</span> <span class="ss">:eps</span> <span class="nv">eps</span><span class="p">})</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">sym/activation</span> <span class="s">&quot;gact1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:act-type</span> <span class="s">&quot;relu&quot;</span><span class="p">})</span>
</span><span class='line'>  
</span><span class='line'>  <span class="nv">...</span>
</span><span class='line'>  
</span></code></pre></td></tr></table></div></figure>


<p>There is a variable for the <code>data</code> coming in, but this time it is a random noise vector. Another interesting point that is is using a <code>deconvolution</code> layer instead of a <code>convolution</code> layer. The generator is basically the inverse of the discriminator. It starts with a random noise vector, but that is translated up through the layers until it is expanded to a image output.</p>

<p>Next, we iterate through all of our training images in our <code>flan-iter</code> with <code>reduce-batches</code>. Here is just an excerpt where we get a random noise vector and have the generator run the data through and produce the output image:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">mx-io/reduce-batches</span>
</span><span class='line'>       <span class="nv">flan-iter</span>
</span><span class='line'>       <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">n</span> <span class="nv">batch</span><span class="p">]</span>
</span><span class='line'>         <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">rbatch</span> <span class="p">(</span><span class="nf">mx-io/next</span> <span class="nv">rand-noise-iter</span><span class="p">)</span>
</span><span class='line'>               <span class="nv">dbatch</span> <span class="p">(</span><span class="nf">mapv</span> <span class="nv">normalize-rgb-ndarray</span> <span class="p">(</span><span class="nf">mx-io/batch-data</span> <span class="nv">batch</span><span class="p">))</span>
</span><span class='line'>               <span class="nv">out-g</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">mod-g</span>
</span><span class='line'>                         <span class="p">(</span><span class="nf">m/forward</span> <span class="nv">rbatch</span><span class="p">)</span>
</span><span class='line'>                         <span class="p">(</span><span class="nf">m/outputs</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>The whole code is <a href="https://github.com/gigasquid/mxnet-gan-flan">here</a> for reference, but let&rsquo;s skip forward and run it and see what happens.</p>

<p><img src="http://gigasquid.github.io/images/gout-96-0.jpg" alt="" /></p>

<p>FLANS!! Well, they could be flans if you squint a bit.</p>

<p>Now that we have them kinda working for a small image size 28x28, let&rsquo;s biggerize it.</p>

<h2>Turn on the Oven and Bake</h2>

<p>Turning up the size to 128x128 requires some alterations in the layers&#8217; parameters to make sure that it processes and generates the correct size, but other than that we are good to go.</p>

<p>Here comes the fun part, watching it train and learn:</p>

<h3>Epoch 0</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-128-0-0.jpg" alt="" /></p>

<p>In the beginning there was nothing but random noise.</p>

<h3>Epoch 10</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-128-10-0.jpg" alt="" /></p>

<p>It&rsquo;s beginning to learn colors! Red, yellow, brown seem to be important to flans.</p>

<h3>Epoch 23</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-128-23-0.jpg" alt="" /></p>

<p>It&rsquo;s learning shapes! It has learned that flans seem to be blob shaped.</p>

<h3>Epoch 33</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-128-33-0.jpg" alt="" /></p>

<p>It is moving into its surreal phase. Salvidor Dali would be proud of these flans.</p>

<h3>Epoch 45</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-128-45.jpg" alt="" /></p>

<p>Things take a weird turn. Does that flan have eyes?</p>

<h3>Epoch 68</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-128-68-0.jpg" alt="" /></p>

<p>Even worse. Are those demonic flans? Should we even continue down this path?</p>

<p>Answer: Yes - <strong>the training must go on..</strong></p>

<h3>Epoch 161</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-161-0.jpg" alt="" /></p>

<p>Big moment here. It looks like something that could possibly be edible.</p>

<h3>Epoch 170</h3>

<p><img src="http://gigasquid.github.io/images/flan-random-170-0.jpg" alt="" /></p>

<p>Ick! Green Flans! No one is going to want that.</p>

<h3>Epoch 195</h3>

<p><img src="http://gigasquid.github.io/images/explore-195.jpg" alt="" /></p>

<p>We&rsquo;ve achieved maximum flan, (for the time being).</p>

<h2>Explore</h2>

<p>If you are interested in playing around with the pretrained model, you can check it out <a href="https://github.com/gigasquid/mxnet-gan-flan/blob/master/src/mxnet_gan_flan/gan.clj#L355">here with the pretrained function</a>.
It will load up the trained model and generate flans for you to explore and bring to your dinner parties.</p>

<p>Wrapping up, training GANs is a <em>lot</em> of fun. With MXNet, you can bring the fun with you to Clojure.</p>

<p>Want more, check out this Clojure Conj video -  <a href="https://www.youtube.com/watch?v=yzfnlcHtwiY">Can You GAN?</a>.</p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2018-07-05T19:39:00-04:00"  data-updated="true" itemprop="datePublished dateCreated">Thu  5 Jul 2018,  7:39 PM</time>
        
           | <a href="/blog/2018/07/05/clojure-mxnet-the-module-api/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2018/07/05/clojure-mxnet-the-module-api/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2018/07/05/clojure-mxnet-the-module-api/" itemprop="url">Clojure MXNet - the Module API</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p><img src="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg"></p>

<p>This is an introduction to the high level Clojure API for deep learning library <a href="http://mxnet.incubator.apache.org/">MXNet</a>.</p>

<p>The module API provides an intermediate and high-level interface for performing computation with neural networks in MXNet.</p>

<p>To follow along with this documentation, you can use this namespace to with the needed requires:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">ns </span><span class="nv">docs.module</span>
</span><span class='line'>  <span class="p">(</span><span class="ss">:require</span> <span class="p">[</span><span class="nv">clojure.java.io</span> <span class="ss">:as</span> <span class="nv">io</span><span class="p">]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">clojure.java.shell</span> <span class="ss">:refer</span> <span class="p">[</span><span class="nv">sh</span><span class="p">]]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">org.apache.clojure-mxnet.eval-metric</span> <span class="ss">:as</span> <span class="nv">eval-metric</span><span class="p">]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">org.apache.clojure-mxnet.io</span> <span class="ss">:as</span> <span class="nv">mx-io</span><span class="p">]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">org.apache.clojure-mxnet.module</span> <span class="ss">:as</span> <span class="nv">m</span><span class="p">]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">org.apache.clojure-mxnet.symbol</span> <span class="ss">:as</span> <span class="nv">sym</span><span class="p">]</span>
</span><span class='line'>            <span class="p">[</span><span class="nv">org.apache.clojure-mxnet.ndarray</span> <span class="ss">:as</span> <span class="nv">ndarray</span><span class="p">]))</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Prepare the Data</h2>

<p>In this example, we are going to use the MNIST data set. If you have cloned the MXNet repo and <code>cd contrib/clojure-package</code>, we can run some helper scripts to download the data for us.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">data-dir</span> <span class="s">&quot;data/&quot;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nb">when-not </span><span class="p">(</span><span class="nf">.exists</span> <span class="p">(</span><span class="nf">io/file</span> <span class="p">(</span><span class="nb">str </span><span class="nv">data-dir</span> <span class="s">&quot;train-images-idx3-ubyte&quot;</span><span class="p">)))</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">sh</span> <span class="s">&quot;../../scripts/get_mnist_data.sh&quot;</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>MXNet provides function in the <code>io</code> namespace to load the MNIST datasets into training and test data iterators that we can use with our module.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">train-data</span> <span class="p">(</span><span class="nf">mx-io/mnist-iter</span> <span class="p">{</span><span class="ss">:image</span> <span class="p">(</span><span class="nb">str </span><span class="nv">data-dir</span> <span class="s">&quot;train-images-idx3-ubyte&quot;</span><span class="p">)</span>
</span><span class='line'>                                   <span class="ss">:label</span> <span class="p">(</span><span class="nb">str </span><span class="nv">data-dir</span> <span class="s">&quot;train-labels-idx1-ubyte&quot;</span><span class="p">)</span>
</span><span class='line'>                                   <span class="ss">:label-name</span> <span class="s">&quot;softmax_label&quot;</span>
</span><span class='line'>                                   <span class="ss">:input-shape</span> <span class="p">[</span><span class="mi">784</span><span class="p">]</span>
</span><span class='line'>                                   <span class="ss">:batch-size</span> <span class="mi">10</span>
</span><span class='line'>                                   <span class="ss">:shuffle</span> <span class="nv">true</span>
</span><span class='line'>                                   <span class="ss">:flat</span> <span class="nv">true</span>
</span><span class='line'>                                   <span class="ss">:silent</span> <span class="nv">false</span>
</span><span class='line'>                                   <span class="ss">:seed</span> <span class="mi">10</span><span class="p">}))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">test-data</span> <span class="p">(</span><span class="nf">mx-io/mnist-iter</span> <span class="p">{</span><span class="ss">:image</span> <span class="p">(</span><span class="nb">str </span><span class="nv">data-dir</span> <span class="s">&quot;t10k-images-idx3-ubyte&quot;</span><span class="p">)</span>
</span><span class='line'>                                  <span class="ss">:label</span> <span class="p">(</span><span class="nb">str </span><span class="nv">data-dir</span> <span class="s">&quot;t10k-labels-idx1-ubyte&quot;</span><span class="p">)</span>
</span><span class='line'>                                  <span class="ss">:input-shape</span> <span class="p">[</span><span class="mi">784</span><span class="p">]</span>
</span><span class='line'>                                  <span class="ss">:batch-size</span> <span class="mi">10</span>
</span><span class='line'>                                  <span class="ss">:flat</span> <span class="nv">true</span>
</span><span class='line'>                                  <span class="ss">:silent</span> <span class="nv">false</span><span class="p">}))</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Preparing a Module for Computation</h2>

<p>To construct a module, we need to have a symbol as input. This symbol takes input data in the first layer and then has subsequent layers of fully connected and relu activation layers, ending up in a softmax layer for output.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">data</span> <span class="p">(</span><span class="nf">sym/variable</span> <span class="s">&quot;data&quot;</span><span class="p">)</span>
</span><span class='line'>      <span class="nv">fc1</span> <span class="p">(</span><span class="nf">sym/fully-connected</span> <span class="s">&quot;fc1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:num-hidden</span> <span class="mi">128</span><span class="p">})</span>
</span><span class='line'>      <span class="nv">act1</span> <span class="p">(</span><span class="nf">sym/activation</span> <span class="s">&quot;relu1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">fc1</span> <span class="ss">:act-type</span> <span class="s">&quot;relu&quot;</span><span class="p">})</span>
</span><span class='line'>      <span class="nv">fc2</span> <span class="p">(</span><span class="nf">sym/fully-connected</span> <span class="s">&quot;fc2&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">act1</span> <span class="ss">:num-hidden</span> <span class="mi">64</span><span class="p">})</span>
</span><span class='line'>      <span class="nv">act2</span> <span class="p">(</span><span class="nf">sym/activation</span> <span class="s">&quot;relu2&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">fc2</span> <span class="ss">:act-type</span> <span class="s">&quot;relu&quot;</span><span class="p">})</span>
</span><span class='line'>      <span class="nv">fc3</span> <span class="p">(</span><span class="nf">sym/fully-connected</span> <span class="s">&quot;fc3&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">act2</span> <span class="ss">:num-hidden</span> <span class="mi">10</span><span class="p">})</span>
</span><span class='line'>      <span class="nv">out</span> <span class="p">(</span><span class="nf">sym/softmax-output</span> <span class="s">&quot;softmax&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">fc3</span><span class="p">})]</span>
</span><span class='line'>  <span class="nv">out</span><span class="p">)</span>
</span><span class='line'>  <span class="c1">;=&gt;#object[org.apache.mxnet.Symbol 0x1f43a406 &quot;org.apache.mxnet.Symbol@1f43a406&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can also write this with the <code>as-&gt;</code> threading macro.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">out</span> <span class="p">(</span><span class="nf">as-&gt;</span> <span class="p">(</span><span class="nf">sym/variable</span> <span class="s">&quot;data&quot;</span><span class="p">)</span> <span class="nv">data</span>
</span><span class='line'>           <span class="p">(</span><span class="nf">sym/fully-connected</span> <span class="s">&quot;fc1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:num-hidden</span> <span class="mi">128</span><span class="p">})</span>
</span><span class='line'>           <span class="p">(</span><span class="nf">sym/activation</span> <span class="s">&quot;relu1&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:act-type</span> <span class="s">&quot;relu&quot;</span><span class="p">})</span>
</span><span class='line'>           <span class="p">(</span><span class="nf">sym/fully-connected</span> <span class="s">&quot;fc2&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:num-hidden</span> <span class="mi">64</span><span class="p">})</span>
</span><span class='line'>           <span class="p">(</span><span class="nf">sym/activation</span> <span class="s">&quot;relu2&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:act-type</span> <span class="s">&quot;relu&quot;</span><span class="p">})</span>
</span><span class='line'>           <span class="p">(</span><span class="nf">sym/fully-connected</span> <span class="s">&quot;fc3&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span> <span class="ss">:num-hidden</span> <span class="mi">10</span><span class="p">})</span>
</span><span class='line'>           <span class="p">(</span><span class="nf">sym/softmax-output</span> <span class="s">&quot;softmax&quot;</span> <span class="p">{</span><span class="ss">:data</span> <span class="nv">data</span><span class="p">})))</span>
</span><span class='line'><span class="c1">;=&gt; #&#39;tutorial.module/out</span>
</span></code></pre></td></tr></table></div></figure>


<p>By default, <code>context</code> is the CPU. If you need data parallelization, you can specify a GPU context or an array of GPU contexts like this <code>(m/module out {:contexts [(context/gpu)]})</code></p>

<p>Before you can compute with a module, you need to call <code>bind</code> to allocate the device memory and <code>init-params</code> or <code>set-params</code> to initialize the parameters. If you simply want to fit a module, you dont need to call <code>bind</code> and <code>init-params</code> explicitly, because the <code>fit</code> function automatically calls them if they are needed.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">mod</span> <span class="p">(</span><span class="nf">m/module</span> <span class="nv">out</span><span class="p">)]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">mod</span>
</span><span class='line'>      <span class="p">(</span><span class="nf">m/bind</span> <span class="p">{</span><span class="ss">:data-shapes</span> <span class="p">(</span><span class="nf">mx-io/provide-data</span> <span class="nv">train-data</span><span class="p">)</span>
</span><span class='line'>               <span class="ss">:label-shapes</span> <span class="p">(</span><span class="nf">mx-io/provide-label</span> <span class="nv">train-data</span><span class="p">)})</span>
</span><span class='line'>      <span class="p">(</span><span class="nf">m/init-params</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now you can compute with the module using functions like <code>forward</code>, <code>backward</code>, etc.</p>

<h2>Training and Predicting</h2>

<p>Modules provide high-level APIs for training, predicting, and evaluating. To fit a module, call the <code>fit</code> function with some data iterators:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">mod</span> <span class="p">(</span><span class="nf">m/fit</span> <span class="p">(</span><span class="nf">m/module</span> <span class="nv">out</span><span class="p">)</span> <span class="p">{</span><span class="ss">:train-data</span> <span class="nv">train-data</span> <span class="ss">:eval-data</span> <span class="nv">test-data</span> <span class="ss">:num-epoch</span> <span class="mi">1</span><span class="p">}))</span>
</span><span class='line'><span class="c1">;; Epoch  0  Train- [accuracy 0.12521666]</span>
</span><span class='line'><span class="c1">;; Epoch  0  Time cost- 8392</span>
</span><span class='line'><span class="c1">;; Epoch  0  Validation-  [accuracy 0.2227]</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can pass in batch-end callbacks using batch-end-callback and epoch-end callbacks using epoch-end-callback in the <code>fit-params</code>. You can also set parameters using functions like in the fit-params like optimizer and eval-metric. To learn more about the fit-params, see the fit-param function options. To predict with a module, call <code>predict</code> with a DataIter:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">results</span> <span class="p">(</span><span class="nf">m/predict</span> <span class="nv">mod</span> <span class="p">{</span><span class="ss">:eval-data</span> <span class="nv">test-data</span><span class="p">}))</span>
</span><span class='line'><span class="p">(</span><span class="nb">first </span><span class="nv">results</span><span class="p">)</span> <span class="c1">;=&gt;#object[org.apache.mxnet.NDArray 0x3540b6d3 &quot;org.apache.mxnet.NDArray@a48686ec&quot;]</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nb">first </span><span class="p">(</span><span class="nf">ndarray/-&gt;vec</span> <span class="p">(</span><span class="nb">first </span><span class="nv">results</span><span class="p">)))</span> <span class="c1">;=&gt;0.08261358</span>
</span></code></pre></td></tr></table></div></figure>


<p>The module collects and returns all of the prediction results. For more details about the format of the return values, see the documentation for the <code>predict</code> function.</p>

<p>When prediction results might be too large to fit in memory, use the <code>predict-every-batch</code> API.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">preds</span> <span class="p">(</span><span class="nf">m/predict-every-batch</span> <span class="nv">mod</span> <span class="p">{</span><span class="ss">:eval-data</span> <span class="nv">test-data</span><span class="p">})]</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">mx-io/reduce-batches</span> <span class="nv">test-data</span>
</span><span class='line'>                        <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">i</span> <span class="nv">batch</span><span class="p">]</span>
</span><span class='line'>                          <span class="p">(</span><span class="nb">println </span><span class="p">(</span><span class="nb">str </span><span class="s">&quot;pred is &quot;</span> <span class="p">(</span><span class="nb">first </span><span class="p">(</span><span class="nb">get </span><span class="nv">preds</span> <span class="nv">i</span><span class="p">))))</span>
</span><span class='line'>                          <span class="p">(</span><span class="nb">println </span><span class="p">(</span><span class="nb">str </span><span class="s">&quot;label is &quot;</span> <span class="p">(</span><span class="nf">mx-io/batch-label</span> <span class="nv">batch</span><span class="p">)))</span>
</span><span class='line'>                          <span class="c1">;;; do something</span>
</span><span class='line'>                          <span class="p">(</span><span class="nb">inc </span><span class="nv">i</span><span class="p">))))</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you need to evaluate on a test set and dont need the prediction output, call the <code>score</code> function with a data iterator and an eval metric:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">m/score</span> <span class="nv">mod</span> <span class="p">{</span><span class="ss">:eval-data</span> <span class="nv">test-data</span> <span class="ss">:eval-metric</span> <span class="p">(</span><span class="nf">eval-metric/accuracy</span><span class="p">)})</span> <span class="c1">;=&gt;[&quot;accuracy&quot; 0.2227]</span>
</span></code></pre></td></tr></table></div></figure>


<p>This runs predictions on each batch in the provided data iterator and computes the evaluation score using the provided eval metric. The evaluation results are stored in metric so that you can query later.</p>

<h2>Saving and Loading</h2>

<p>To save the module parameters in each training epoch, use a <code>checkpoint</code> function:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">save-prefix</span> <span class="s">&quot;my-model&quot;</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">doseq </span><span class="p">[</span><span class="nv">epoch-num</span> <span class="p">(</span><span class="nb">range </span><span class="mi">3</span><span class="p">)]</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">mx-io/do-batches</span> <span class="nv">train-data</span> <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">batch</span>
</span><span class='line'>                                          <span class="c1">;; do something</span>
</span><span class='line'><span class="p">]))</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">m/save-checkpoint</span> <span class="nv">mod</span> <span class="p">{</span><span class="ss">:prefix</span> <span class="nv">save-prefix</span> <span class="ss">:epoch</span> <span class="nv">epoch-num</span> <span class="ss">:save-opt-states</span> <span class="nv">true</span><span class="p">})))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0000.params</span>
</span><span class='line'><span class="c1">;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0000.states</span>
</span><span class='line'><span class="c1">;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0001.params</span>
</span><span class='line'><span class="c1">;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0001.states</span>
</span><span class='line'><span class="c1">;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0002.params</span>
</span><span class='line'><span class="c1">;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0002.states</span>
</span></code></pre></td></tr></table></div></figure>


<p>To load the saved module parameters, call the <code>load-checkpoint</code> function:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">new-mod</span> <span class="p">(</span><span class="nf">m/load-checkpoint</span> <span class="p">{</span><span class="ss">:prefix</span> <span class="s">&quot;my-model&quot;</span> <span class="ss">:epoch</span> <span class="mi">1</span> <span class="ss">:load-optimizer-states</span> <span class="nv">true</span><span class="p">}))</span>
</span><span class='line'>
</span><span class='line'><span class="nv">new-mod</span> <span class="c1">;=&gt; #object[org.apache.mxnet.module.Module 0x5304d0f4 &quot;org.apache.mxnet.module.Module@5304d0f4&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>To initialize parameters, Bind the symbols to construct executors first with bind function. Then, initialize the parameters and auxiliary states by calling <code>init-params</code> function.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nb">-&gt; </span><span class="nv">new-mod</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">m/bind</span> <span class="p">{</span><span class="ss">:data-shapes</span> <span class="p">(</span><span class="nf">mx-io/provide-data</span> <span class="nv">train-data</span><span class="p">)</span> <span class="ss">:label-shapes</span> <span class="p">(</span><span class="nf">mx-io/provide-label</span> <span class="nv">train-data</span><span class="p">)})</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">m/init-params</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>To get current parameters, use <code>params</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">let </span><span class="p">[[</span><span class="nv">arg-params</span> <span class="nv">aux-params</span><span class="p">]</span> <span class="p">(</span><span class="nf">m/params</span> <span class="nv">new-mod</span><span class="p">)]</span>
</span><span class='line'>  <span class="p">{</span><span class="ss">:arg-params</span> <span class="nv">arg-params</span>
</span><span class='line'>   <span class="ss">:aux-params</span> <span class="nv">aux-params</span><span class="p">})</span>
</span><span class='line'>
</span><span class='line'><span class="c1">;; {:arg-params</span>
</span><span class='line'><span class="c1">;;  {&quot;fc3_bias&quot;</span>
</span><span class='line'><span class="c1">;;   #object[org.apache.mxnet.NDArray 0x39adc3b0 &quot;org.apache.mxnet.NDArray@49caf426&quot;],</span>
</span><span class='line'><span class="c1">;;   &quot;fc2_weight&quot;</span>
</span><span class='line'><span class="c1">;;   #object[org.apache.mxnet.NDArray 0x25baf623 &quot;org.apache.mxnet.NDArray@a6c8f9ac&quot;],</span>
</span><span class='line'><span class="c1">;;   &quot;fc1_bias&quot;</span>
</span><span class='line'><span class="c1">;;   #object[org.apache.mxnet.NDArray 0x6e089973 &quot;org.apache.mxnet.NDArray@9f91d6eb&quot;],</span>
</span><span class='line'><span class="c1">;;   &quot;fc3_weight&quot;</span>
</span><span class='line'><span class="c1">;;   #object[org.apache.mxnet.NDArray 0x756fd109 &quot;org.apache.mxnet.NDArray@2dd0fe3c&quot;],</span>
</span><span class='line'><span class="c1">;;   &quot;fc2_bias&quot;</span>
</span><span class='line'><span class="c1">;;   #object[org.apache.mxnet.NDArray 0x1dc69c8b &quot;org.apache.mxnet.NDArray@d128f73d&quot;],</span>
</span><span class='line'><span class="c1">;;   &quot;fc1_weight&quot;</span>
</span><span class='line'><span class="c1">;;   #object[org.apache.mxnet.NDArray 0x20abc769 &quot;org.apache.mxnet.NDArray@b8e1c5e8&quot;]},</span>
</span><span class='line'><span class="c1">;;  :aux-params {}}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To assign parameter and aux state values, use <code>set-params</code> function.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">m/set-params</span> <span class="nv">new-mod</span> <span class="p">{</span><span class="ss">:arg-params</span> <span class="p">(</span><span class="nf">m/arg-params</span> <span class="nv">new-mod</span><span class="p">)</span> <span class="ss">:aux-params</span> <span class="p">(</span><span class="nf">m/aux-params</span> <span class="nv">new-mod</span><span class="p">)})</span>
</span><span class='line'><span class="c1">;=&gt; #object[org.apache.mxnet.module.Module 0x5304d0f4 &quot;org.apache.mxnet.module.Module@5304d0f4&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>To resume training from a saved checkpoint, instead of calling <code>set-params</code>, directly call <code>fit</code>, passing the loaded parameters, so that <code>fit</code> knows to start from those parameters instead of initializing randomly</p>

<p>Create fit-params, and then use it to set <code>begin-epoch</code> so that <code>fit</code> knows to resume from a saved epoch.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="c1">;; reset the training data before calling fit or you will get an error</span>
</span><span class='line'><span class="p">(</span><span class="nf">mx-io/reset</span> <span class="nv">train-data</span><span class="p">)</span>
</span><span class='line'><span class="p">(</span><span class="nf">mx-io/reset</span> <span class="nv">test-data</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nf">m/fit</span> <span class="nv">new-mod</span> <span class="p">{</span><span class="ss">:train-data</span> <span class="nv">train-data</span> <span class="ss">:eval-data</span> <span class="nv">test-data</span> <span class="ss">:num-epoch</span> <span class="mi">2</span>
</span><span class='line'>                <span class="ss">:fit-params</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">m/fit-params</span> <span class="p">{</span><span class="ss">:begin-epoch</span> <span class="mi">1</span><span class="p">}))})</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you are interested in checking out MXNet and exploring on your own, check out the main page <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">here</a> with instructions on how to install and other information.</p>

<h3>See other blog posts about MXNet</h3>

<ul>
<li><a href="http://gigasquidsoftware.com/blog/2018/06/03/meet-clojure-mxnet-ndarray/">Clojure MXNet - NDArray</a></li>
<li><a href="http://gigasquidsoftware.com/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/">Clojure MXNet Joins Apache MXNet</a></li>
</ul>

</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2018-07-01T10:44:00-04:00"  data-updated="true" itemprop="datePublished dateCreated">Sun  1 Jul 2018, 10:44 AM</time>
        
           | <a href="/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/" itemprop="url">Clojure MXNet Joins the Apache MXNet Project</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p><img src="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg"></p>

<p>I&rsquo;m delighted to share the news that the Clojure package for <a href="https://mxnet.apache.org/">MXNet</a> has now joined the main Apache MXNet project. A big thank you to the efforts of everyone involved to make this possible. Having it as part of the main project is a great place for growth and collaboration that will benefit both MXNet and the Clojure community.</p>

<h2>Invitation to Join and Contribute</h2>

<p>The Clojure package has been brought in as a <em>contrib</em> <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">clojure-package</a>. It is still very new and will go through a period of feedback, stabilization, and improvement before it graduates out of contrib.</p>

<p>We welcome contributors and people getting involved to make it better.</p>

<p>Are you interested in Deep Learning and Clojure? Great - Join us!</p>

<p>There are a few ways to get involved.</p>

<ul>
<li>Check out the current state of the Clojure package some contribution needs here <a href="https://cwiki.apache.org/confluence/display/MXNET/Clojure+Package+Contribution+Needs">https://cwiki.apache.org/confluence/display/MXNET/Clojure+Package+Contribution+Needs</a></li>
<li>Join the Clojurian Slack #mxnet channel</li>
<li>Join the <a href="https://lists.apache.org/list.html?dev@mxnet.apache.org">MXNet dev mailing list</a> by sending an email to <code>dev-subscribe@mxnet.apache.org.</code>.</li>
<li>Join the MXNET Slack channel - You have to join the MXnet dev mailing list first, but after that says you would like to join the slack and someone will add you.</li>
<li>Join the <a href="https://discuss.mxnet.io/">MXNet Discussion Forum</a></li>
</ul>


<h3>Want to Learn More?</h3>

<p>There are lots of examples in the package to check out, but a good place to start are the tutorials here <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial">https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial</a></p>

<p>There is a blog walkthough here as well - <a href="http://gigasquidsoftware.com/blog/2018/07/05/clojure-mxnet-the-module-api/">Clojure MXNet Module API</a></p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2018-06-03T16:13:00-04:00"  data-updated="true" itemprop="datePublished dateCreated">Sun  3 Jun 2018,  4:13 PM</time>
        
           | <a href="/blog/2018/06/03/meet-clojure-mxnet-ndarray/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2018/06/03/meet-clojure-mxnet-ndarray/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2018/06/03/meet-clojure-mxnet-ndarray/" itemprop="url">Meet Clojure MXNet - NDArray</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p><img src="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg"></p>

<p>This is the beginning of a series of blog posts to get to know the <a href="https://mxnet.apache.org/">Apache MXNet</a> Deep Learning project and the new Clojure language binding <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">clojure-package</a></p>

<p>MXNet is a first class, modern deep learning library that AWS has officially picked as its chosen library. It supports multiple languages on a first class basis and is incubating as an Apache project.</p>

<p>The motivation for creating a Clojure package is to be able to open the deep learning library to the Clojure ecosystem and build bridges for future development and innovation for the community. It provides all the needed tools including low level and high level apis, dynamic graphs, and things like GAN and natural language support.</p>

<p>So let&rsquo;s get on with our introduction with one of the basic building blocks of MXNet, the <code>NDArray</code>.</p>

<h2>Meet NDArray</h2>

<p>The <code>NDArray</code> is the tensor data structure in MXNet. Let&rsquo;s start of by creating one. First we need to require the <code>ndarray</code> namespace:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">ns </span><span class="nv">tutorial.ndarray</span>
</span><span class='line'>  <span class="p">(</span><span class="ss">:require</span> <span class="p">[</span><span class="nv">org.apache.clojure-mxnet.ndarray</span> <span class="ss">:as</span> <span class="nv">ndarray</span><span class="p">]))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now let&rsquo;s create an all zero array of dimension 100 x 50</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/zeros</span> <span class="p">[</span><span class="mi">100</span> <span class="mi">50</span><span class="p">])</span>
</span><span class='line'><span class="c1">;=&gt; #object[org.apache.mxnet.NDArray 0x3e396d0 &quot;org.apache.mxnet.NDArray@aeea40b6&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>We can check the shape of this by using <code>shape-vec</code></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/shape-vec</span> <span class="p">(</span><span class="nf">ndarray/zeros</span> <span class="p">[</span><span class="mi">100</span> <span class="mi">50</span><span class="p">]))</span>
</span><span class='line'><span class="c1">;=&gt; [100 50]</span>
</span></code></pre></td></tr></table></div></figure>


<p>There is also a quick way to create an ndarray of ones with the <code>ones</code> function:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/ones</span> <span class="p">[</span><span class="mi">256</span> <span class="mi">32</span> <span class="mi">128</span> <span class="mi">1</span><span class="p">])</span>
</span></code></pre></td></tr></table></div></figure>


<p>Ones and zeros are nice, but what an array with specific contents? There is an <code>array</code> function for that. Specific the contents of the array first and the shape second:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">c</span> <span class="p">(</span><span class="nf">ndarray/array</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span> <span class="mi">3</span><span class="p">]))</span>
</span><span class='line'><span class="p">(</span><span class="nf">ndarray/shape-vec</span> <span class="nv">c</span><span class="p">)</span>  <span class="c1">;=&gt; [2 3]</span>
</span></code></pre></td></tr></table></div></figure>


<p>To convert it back to a vector format, we can use the <code>-&gt;vec</code> function.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/-&gt;vec</span> <span class="nv">c</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt; [1.0 2.0 3.0 4.0 5.0 6.0]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now that we know how to create NDArrays, we can get to do something interesting like operations on them.</p>

<h3>Operations</h3>

<p>There are all the standard arithmetic operations:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">a</span> <span class="p">(</span><span class="nf">ndarray/ones</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">5</span><span class="p">]))</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">b</span> <span class="p">(</span><span class="nf">ndarray/ones</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">5</span><span class="p">]))</span>
</span><span class='line'><span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">ndarray/+</span> <span class="nv">a</span> <span class="nv">b</span><span class="p">)</span> <span class="p">(</span><span class="nf">ndarray/-&gt;vec</span><span class="p">))</span>
</span><span class='line'><span class="c1">;=&gt;  [2.0 2.0 2.0 2.0 2.0]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that the original ndarrays are unchanged.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/-&gt;vec</span> <span class="nv">a</span><span class="p">)</span> <span class="c1">;=&gt; [1.0 1.0 1.0 1.0 1.0]</span>
</span><span class='line'><span class="p">(</span><span class="nf">ndarray/-&gt;vec</span> <span class="nv">b</span><span class="p">)</span> <span class="c1">;=&gt; [1.0 1.0 1.0 1.0 1.0]</span>
</span></code></pre></td></tr></table></div></figure>


<p>But, we can change that if we use the inplace operators:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/+=</span> <span class="nv">a</span> <span class="nv">b</span><span class="p">)</span>
</span><span class='line'><span class="p">(</span><span class="nf">ndarray/-&gt;vec</span> <span class="nv">a</span><span class="p">)</span> <span class="c1">;=&gt;  [2.0 2.0 2.0 2.0 2.0]</span>
</span></code></pre></td></tr></table></div></figure>


<p>There are many more operations, but just to give you a taste, we&rsquo;ll take a look a the <code>dot</code> product operation:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">arr1</span> <span class="p">(</span><span class="nf">ndarray/array</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">]</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">]))</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">arr2</span> <span class="p">(</span><span class="nf">ndarray/array</span> <span class="p">[</span><span class="mi">3</span> <span class="mi">4</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span> <span class="mi">1</span><span class="p">]))</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">res</span> <span class="p">(</span><span class="nf">ndarray/dot</span> <span class="nv">arr1</span> <span class="nv">arr2</span><span class="p">))</span>
</span><span class='line'><span class="p">(</span><span class="nf">ndarray/shape-vec</span> <span class="nv">res</span><span class="p">)</span> <span class="c1">;=&gt; [1 1]</span>
</span><span class='line'><span class="p">(</span><span class="nf">ndarray/-&gt;vec</span> <span class="nv">res</span><span class="p">)</span> <span class="c1">;=&gt; [11.0]</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you are curious about the other operators available in NDArray API check out the <a href="https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html">MXNet project documentation page</a></p>

<p>Now that we have ndarrays and can do calculations on them, we might want to save and load them.</p>

<h3>Saving and Loading</h3>

<p>You can save ndarrays with a name as a map like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/save</span> <span class="s">&quot;filename&quot;</span> <span class="p">{</span><span class="s">&quot;arr1&quot;</span> <span class="nv">arr1</span> <span class="s">&quot;arr2&quot;</span> <span class="nv">arr2</span><span class="p">})</span>
</span></code></pre></td></tr></table></div></figure>


<p>To load them, you just specify the filename and the map is returned.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">ndarray/load</span> <span class="s">&quot;filename&quot;</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt; {&quot;arr1&quot; #object[org.apache.mxnet.NDArray 0x1b629ff4 &quot;org.apache.mxnet.NDArray@63da08cb&quot;]</span>
</span><span class='line'><span class="c1">;=&gt;  &quot;arr2&quot; #object[org.apache.mxnet.NDArray 0x25d994e3 &quot;org.apache.mxnet.NDArray@5bbaf2c3&quot;]}</span>
</span></code></pre></td></tr></table></div></figure>


<p>One more cool thing, we can even due our operations on the cpu or gpu.</p>

<h3>Multi-Device Support</h3>

<p>When creating an <code>ndarray</code> you can use a context argument to specify the device. To do this, we will need the help of the <code>context</code> namespace.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">require</span> <span class="o">&#39;</span><span class="p">[</span><span class="nv">org.apache.clojure-mxnet.context</span> <span class="ss">:as</span> <span class="nv">context</span><span class="p">])</span>
</span></code></pre></td></tr></table></div></figure>


<p>By default, the <code>ndarray</code> is created on the cpu context.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">cpu-a</span> <span class="p">(</span><span class="nf">ndarray/zeros</span> <span class="p">[</span><span class="mi">100</span> <span class="mi">200</span><span class="p">]))</span>
</span><span class='line'><span class="p">(</span><span class="nf">ndarray/context</span> <span class="nv">cpu-a</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt; #object[ml.dmlc.mxnet.Context 0x3f376123 &quot;cpu(0)&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>But we can specify the gpu instead, (if we have a gpu enabled build).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">gpu-b</span> <span class="p">(</span><span class="nf">ndarray/zeros</span> <span class="p">[</span><span class="mi">100</span> <span class="mi">200</span><span class="p">]</span> <span class="p">{</span><span class="ss">:ctx</span> <span class="p">(</span><span class="nf">context/gpu</span> <span class="mi">0</span><span class="p">)}))</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>Note: Operations among different contexts are currently not allowed, but there is a <code>copy-to</code> function that can help copy the content from one device to another and then continue on with the computation.</em></p>

<h2>Wrap up</h2>

<p>I hope you&rsquo;ve enjoyed the brief introduction to the MXNet library, there is much more to explore in future posts. If you are interested in giving it a try, there are native jars for OSX cpu and Linux cpu/gpu available and the code for the ndarray tutorial can be found <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial">here</a></p>

<p><em>Please remember that the library is in a experimential state, so if you encounter any problems or have any other feedback, please log an issue so bugs and rough edges can be fixed :).</em></p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2018-03-04T11:03:00-05:00"  data-updated="true" itemprop="datePublished dateCreated">Sun  4 Mar 2018, 11:03 AM</time>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2018/03/04/on-staying-technical/" itemprop="url">On Staying Technical</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p>I was 10 years into my career when I met her. I could count the number of other women programmers I had worked with on one hand and none of them had young children at home like me. She was not only incredibly experienced and competent, but also had a son in college. I was curious about her career path so I asked her one day at lunch why she was still programming and hadnt become a manager instead.</p>

<p>She smiled at me kindly and replied, &ldquo;Ive worked very hard to stay exactly where I am&rdquo;,  and I was enlightened.</p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2017-11-07T18:51:00-05:00"  data-updated="true" itemprop="datePublished dateCreated">Tue  7 Nov 2017,  6:51 PM</time>
        
           | <a href="/blog/2017/11/07/cats-and-dogs-with-cortex-redux/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2017/11/07/cats-and-dogs-with-cortex-redux/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2017/11/07/cats-and-dogs-with-cortex-redux/" itemprop="url">Cats and Dogs With Cortex Redux</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p>I wrote a <a href="http://gigasquidsoftware.com/blog/2016/12/27/deep-learning-in-clojure-with-cortex/">blog post</a> a while back about using a Clojure machine learning library called <a href="https://github.com/thinktopic/cortex">Cortex</a> to do the Kaggle Cats and Dogs classification challenge.</p>

<p>I wanted to revisit it for a few reasons. The first one is that the Cortex library has progressed and improved considerably over the last year. It&rsquo;s still not at version 1.0, but it my eyes, it&rsquo;s really starting to shine. The second reason is that they recently published an <a href="https://github.com/thinktopic/cortex/tree/master/examples/resnet-retrain">example</a> of using the RESNET50 model, (I&rsquo;ll explain later on), to do fine-tuning or transfer learning. The third reason, is that there is a great new plugin for leiningen the supports using <a href="https://github.com/didiercrunch/lein-jupyter">Jupyter notebooks with Clojure projects</a>. These notebooks are a great way of doing walkthroughs and tutorials.</p>

<p>Putting all these things together, I felt like I was finally at a stage where I could somewhat replicate the first lesson in the <a href="https://github.com/fastai/courses/blob/master/deeplearning1/nbs/dogs_cats_redux.ipynb">Practical Deep Learning Course for Coders</a> with Cats and Dogs - although this time all in Clojure!</p>

<h2>Where to Start?</h2>

<p><img src="http://kaggle2.blob.core.windows.net/competitions/kaggle/3362/media/woof_meow.jpg"></p>

<p>In the last blog post, we created our deep learning network and trained the data on scaled down images (like 50x50) from scratch. This time we are much smarter.</p>

<p>We are still of course going to have to get a hold of all the training data from <a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data">Kaggle Cats vs Dogs Challenge</a>. The big difference is this time, we are just going to have to train our model for <em>1 epoch</em>. What&rsquo;s more, the results will be way better than before.</p>

<p>How is this possible? We are going to use an already trained model, RESNET50. This model has already been painstakingly trained with a gigantic network that is 50 layers deep on the ImageNet challenge. That&rsquo;s a challenge that has models try to classify a 1000 different categories. The theory is that the inner layers of the network have already learned about the features that make up cats and dogs, all we would need to do is peel off the final layer of the network and graft on a new layers that just learns the final classification for our 2 categories of cats and dogs. This is called <em>transfer learning</em> or <em>retraining</em>.</p>

<h2>Plan of Action</h2>

<ul>
<li>Get all the cats and dogs pictures in the right directory format for training</li>
<li>Train the model with all but the last layer in the RESNET model. The last layer we are going to replace with our own layer that will finetune it to classify only cats and dogs</li>
<li>Run the test data and come up with a spreadsheet of results to submit to Kaggle.</li>
</ul>


<h3>Getting all the data pictures in the right format</h3>

<p>This is the generally the most time consuming step of most deep learning. I&rsquo;ll spare you the gritty details but we want to get all the pictures from the <code>train.zip</code> into the format</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-data
</span><span class='line'>  -cats-dogs-training
</span><span class='line'>      -cat
</span><span class='line'>          1110.png
</span><span class='line'>          ...
</span><span class='line'>      -dog
</span><span class='line'>          12416.png
</span><span class='line'>          ...
</span><span class='line'>  -cats-dogs-testing
</span><span class='line'>      -cat
</span><span class='line'>          11.png
</span><span class='line'>          ...
</span><span class='line'>      -dog
</span><span class='line'>          12.png
</span><span class='line'>          ...</span></code></pre></td></tr></table></div></figure>


<p>The image sizes must also all be resized to match the input of the RESNET50. That means they all have to be 224x224.</p>

<h3>Train the model</h3>

<p>The cortex functions allow you to load the resnet50 model, remove the last layer, freeze all the other layers so that they will not be retrained, and add new layers.</p>

<p>I was surprised that I could actually train the model with all the images at 224x244 with the huge RESNET50 model. I built the uberjar and ran it which helped the performance.</p>

<p><code>lein uberjar</code></p>

<p><code>java -jar target/cats-dogs-cortex-redux.jar</code></p>

<p>Training one epoch took me approximately 6 minutes. Not bad, especially considering that&rsquo;s all the training I really needed to do.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Loss for epoch 1: (current) 0.05875186542016347 (best) null
</span><span class='line'>Saving network to trained-network.nippy</span></code></pre></td></tr></table></div></figure>


<p>The key point is that it saved the fine tuned network to trained-network.nippy</p>

<h3>Run the Kaggle test results and submit the results</h3>

<p>You will need to do a bit more setup for this. First, you need to get the Kaggle test images for classification. There are 12500 of these in the test.zip file from the site. Under the data directory, create a new directory called kaggle-test. Now unzip the contents of test.zip inside that folder. The full directory with all the test images should now be:</p>

<p><code>data/kaggle-test/test</code></p>

<p>This step takes a long time and you might have to tweak the batch size again depending on your memory. There are 12500 predications to be made. The main logic for this is in function called <code>(kaggle-results batch-size)</code>. It will take a long time to run. It will print the results as it goes along to the kaggle-results.csv file. If you want to check progress you can do wc -l kaggle-results.csv</p>

<p>For me locally, with <code>(cats-dogs/kaggle-results 100)</code> it took me 28 minutes locally.</p>

<h3>Compare the results</h3>

<p><img src="http://c1.staticflickr.com/5/4518/26477015609_1af781b8da_b.jpg"></p>

<p>My one epoch of fine tuning beat my best results of going through the Practical Deep Learning exercise with the fine tuning the VGG16 model. Not bad at all.</p>

<h2>Summary</h2>

<p>For those of you that are interested in checking out the code, it&rsquo;s out there on <a href="https://github.com/gigasquid/cats-dogs-cortex-redux">github</a></p>

<p>Even more exciting, there is a <a href="https://github.com/gigasquid/cats-dogs-cortex-redux/blob/master/Cats%20and%20Dogs%20in%20Cortex%20(Redux).ipynb">walkthrough in a jupyter notebook</a> with a lein-jupyter plugin.</p>

<p>The Deep Learning world in Clojure is an exciting place to be and gaining tools and traction more and more.</p>
</div>
  
  


        </article>
      
      
        <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
          
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        










<span class="glyphicon glyphicon-calendar"></span> <time datetime="2017-10-22T16:02:00-04:00"  data-updated="true" itemprop="datePublished dateCreated">Sun 22 Oct 2017,  4:02 PM</time>
        
           | <a href="/blog/2017/10/22/embedded-interop-between-clojure-r-and-python-with-graalvm/#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io/blog/2017/10/22/embedded-interop-between-clojure-r-and-python-with-graalvm/">Comments</a>
        
      </p>
    
    
      <h2 class="entry-title" itemprop="name headline"><a href="/blog/2017/10/22/embedded-interop-between-clojure-r-and-python-with-graalvm/" itemprop="url">Embedded Interop Between Clojure, R, and Python With GraalVM</a></h2>
    
  </header>


  <div class="entry-content clearfix" itemprop="articleBody description"><p><img src="https://images-na.ssl-images-amazon.com/images/M/MV5BOTViY2Y0ZGItMTg2OC00YzEzLWJhYjYtZjg4OTMyOWE4YzM1XkEyXkFqcGdeQXVyNTQ1NzU4Njk@._V1_.jpg" title="" ></p>

<p>In my talk at <a href="https://www.youtube.com/watch?v=eLl6_k_fZn4">Clojure Conj</a> I mentioned how a project from Oracle Labs named GraalVM might have to potential for Clojure to interop with Python on the same VM. At the time of the talk, I had just learned about it so I didn&rsquo;t have time to take a look at it. Over the last week, I&rsquo;ve managed to take it for a test drive and I wanted to share what I found.</p>

<h3>Are you ready?</h3>

<p>In this example, we will be using an ordinary Leinengen project and using the REPL we will interop with both R and python.</p>

<p>But first will need a bit of setup.</p>

<p>We will download the <a href="http://www.oracle.com/technetwork/oracle-labs/program-languages/downloads/index.html">Graal project</a> so we can use its <code>java</code> instead of our own.</p>

<p>Once we have it downloaded we will configure our PATH to use Graal&rsquo;s java instead of our own.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># export PATH=/path/to/graalAndTruffle/bin:$PATH</span></code></pre></td></tr></table></div></figure>


<p>Now, we can create a new lein project and run <code>lein repl</code> and begin the fun.</p>

<h3>The Polyglot Context</h3>

<p>In our new namespace, we just need to import the <a href="http://graalvm.github.io/graal/truffle/javadoc/org/graalvm/polyglot/Context.html">Polyglot Context</a> to get started:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">ns </span><span class="nv">graal-test.core</span>
</span><span class='line'>  <span class="p">(</span><span class="ss">:import</span> <span class="p">(</span><span class="nf">org.graalvm.polyglot</span> <span class="nv">Context</span><span class="p">)))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">;; note that is also supports Ruby, LLVM, and JS</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">context</span> <span class="p">(</span><span class="nf">Context/create</span> <span class="p">(</span><span class="nb">into-array </span><span class="p">[</span><span class="s">&quot;python&quot;</span> <span class="s">&quot;R&quot;</span><span class="p">])))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, we are ready to actually try to run some R and Python code right in our REPL. Let&rsquo;s start first with R.</p>

<h3>Interoping with R</h3>

<p>The main function we are going to use is the <code>eval</code> function in the context. Let&rsquo;s start small with some basic math.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">.eval</span> <span class="nv">context</span> <span class="s">&quot;R&quot;</span> <span class="s">&quot;</span>
</span><span class='line'><span class="s">3^2 + 2^2</span>
</span><span class='line'><span class="s">&quot;</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt; #object[org.graalvm.polyglot.Value 0x7ff40e4d &quot;13.0&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Wow! It actually did something. It returned something called a <a href="https://github.com/graalvm/graal/blob/master/sdk/src/org.graalvm.polyglot/src/org/graalvm/polyglot/Value.java">Polyglot Value</a> with what looks like the right answer in it.</p>

<p>Emboldened by our early success, let&rsquo;s try something a little more complicated like calling a function.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">result1</span> <span class="p">(</span><span class="nf">.eval</span> <span class="nv">context</span> <span class="s">&quot;R&quot;</span> <span class="s">&quot;</span>
</span><span class='line'><span class="s">sum.of.squares &lt;- function(x,y) {</span>
</span><span class='line'><span class="s">  x^2 + y^2</span>
</span><span class='line'><span class="s">}</span>
</span><span class='line'><span class="s">sum.of.squares(3,4)</span>
</span><span class='line'><span class="s">&quot;</span><span class="p">))</span>
</span><span class='line'><span class="c1">;=&gt; #object[org.graalvm.polyglot.Value 0xc3edd92 &quot;25.0&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Again, it looks like it worked. Let&rsquo;s try to get the result back into Clojure as a value we can work with. We could ask the result what sort of type it is with</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">.isNumber</span> <span class="nv">result1</span><span class="p">)</span> <span class="c1">;=&gt; true </span>
</span></code></pre></td></tr></table></div></figure>


<p>but let&rsquo;s just use <code>clojure.edn</code> to read the string and save some time.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="kd">defn </span><span class="nv">-&gt;clojure</span> <span class="p">[</span><span class="nv">polyglot-value</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">polyglot-value</span>
</span><span class='line'>      <span class="p">(</span><span class="nf">.toString</span><span class="p">)</span>
</span><span class='line'>      <span class="p">(</span><span class="nf">clojure.edn/read-string</span><span class="p">)))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nf">-&gt;clojure</span> <span class="nv">result1</span><span class="p">)</span> <span class="c1">;=&gt; 25</span>
</span></code></pre></td></tr></table></div></figure>


<p>It would be nice to have a easier way to export symbols and import symbols to and from the guest and host language. In fact, Graal provides a way to do this but to do this in Clojure, we would need something else called <a href="https://github.com/graalvm/graal/tree/master/truffle">Truffle</a>.</p>

<p>Truffle is part of the Graal project and is a framework for implementing languages with the Graal compliler.
There are quite a few languages implemented with the Truffle framework. R is one of them.</p>

<p><img src="https://image.slidesharecdn.com/polyglotonthejvmwithgraalenglish-170521104613/95/polyglot-on-the-jvm-with-graal-english-14-638.jpg?cb=1495364615"></p>

<p>My understanding is that if Clojure was implemented as a truffle lang, then interop could be much more seamless like this example in Ruby</p>

<p><img src="https://image.slidesharecdn.com/polyglotonthejvmwithgraalenglish-170521104613/95/polyglot-on-the-jvm-with-graal-english-37-638.jpg?cb=1495364615"></p>

<p>But let&rsquo;s continue in our exploration. What about doing something more interesting, like importing a useful R library and using it. How about the <a href="https://www.rdocumentation.org/packages/numDeriv/versions/2016.8-1">numDeriv</a> package that supports Accurate Numerical Derivatives?</p>

<p>First we import the package using cran.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">.eval</span> <span class="nv">context</span> <span class="s">&quot;R&quot;</span> <span class="s">&quot;</span>
</span><span class='line'><span class="s">install.packages(\&quot;numDeriv\&quot;, repos = \&quot;http://cran.case.edu/\&quot;)</span>
</span><span class='line'><span class="s">&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you are doing this at your REPL, you can will see lots of text going on in your <code>lein repl</code> process at this point. It&rsquo;s going out and figuring out what deps you need and installing them in your <code>/graalvm-0.28.2/jre/languages/R</code> directory structure.</p>

<p>After it is done, we can actually use it!</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="k">def </span><span class="nv">result2</span> <span class="p">(</span><span class="nf">.eval</span> <span class="nv">context</span> <span class="s">&quot;R&quot;</span> <span class="s">&quot;</span>
</span><span class='line'><span class="s">library(numDeriv)</span>
</span><span class='line'><span class="s">grad(sin, (0:10)*2*pi/10)</span>
</span><span class='line'><span class="s">&quot;</span><span class="p">))</span>
</span><span class='line'><span class="nv">result2</span> <span class="c1">;=&gt; #object[org.graalvm.polyglot.Value 0x76765898 &quot;c(1,</span>
</span><span class='line'>        <span class="c1">;0.809016994367249, 0.309016994372158, -0.309016994373567,</span>
</span><span class='line'>        <span class="c1">;-0.809016994368844, -0.999999999993381, -0.809016994370298,</span>
</span><span class='line'>        <span class="c1">;-0.309016994373312, 0.309016994372042, 0.809016994369185,</span>
</span><span class='line'>        <span class="c1">;0.999999999993381)&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>This has a bit more interesting result as an array. But the Context has ways of dealing with it.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">.hasArrayElements</span> <span class="nv">result2</span><span class="p">)</span> <span class="c1">;=&gt; true</span>
</span><span class='line'><span class="p">(</span><span class="nf">.getArraySize</span> <span class="nv">result2</span><span class="p">)</span> <span class="c1">;=&gt; 11</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nb">for </span><span class="p">[</span><span class="nv">i</span> <span class="p">(</span><span class="nb">range </span><span class="mi">10</span><span class="p">)]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">.getArrayElement</span> <span class="nv">result2</span> <span class="nv">i</span><span class="p">)</span> <span class="p">(</span><span class="nf">-&gt;clojure</span><span class="p">)))</span>
</span><span class='line'><span class="c1">;=&gt; (1.0 0.8090169943672489 0.3090169943721585 -0.3090169943735675</span>
</span><span class='line'><span class="c1">;-0.8090169943688436 -0.9999999999933814</span>
</span><span class='line'><span class="c1">; -0.8090169943702977 -0.3090169943733122 0.30901699437204233</span>
</span><span class='line'><span class="c1">; 0.8090169943691851)</span>
</span></code></pre></td></tr></table></div></figure>


<p>So, we&rsquo;ve showed basic interop with R - which is pretty neat. What about Python?</p>

<h3>Interoping with Python</h3>

<p>Truffle is scheduled to fully support Python in 2018, but there is already an early alpha version in the Graal download that we can play with.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span></span><span class="p">(</span><span class="nf">.eval</span> <span class="nv">context</span> <span class="s">&quot;python&quot;</span> <span class="s">&quot;</span>
</span><span class='line'><span class="s">import time;</span>
</span><span class='line'><span class="s">time.clock()</span>
</span><span class='line'><span class="s">&quot;</span><span class="p">)</span>
</span><span class='line'> <span class="c1">;=&gt; #object[org.graalvm.polyglot.Value 0x4a6b3b70 &quot;1.508202803249E9&quot;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Neat!</p>

<p>It is still a long way for <code>import numpy</code> or <code>import tensorflow</code> but cPython compatibility is the goal. Although the c-extensions are the really tricky part.</p>

<p>So keep an eye on Graal and Truffle for the future and wish the Oracle Labs team the best on their mission to make the JVM Polyglot.</p>

<h3>Footnotes</h3>

<p>If you are interested in playing with the code. I have a github repo here <a href="https://github.com/gigasquid/graal-test">graal-test</a>. If you are interested in watching a video, I really liked <a href="https://www.youtube.com/watch?v=TQMKPRc6cbE">this one</a>. There are also some really nice examples of running in polyglot mode with R and Java and JS here <a href="https://github.com/graalvm/examples">https://github.com/graalvm/examples</a>.</p>
</div>
  
  


        </article>
      
    </div>

    <ul class="pager">
      
        <li class="previous"><a href="">&larr;&nbsp;Older</a></li>
      
      <li><a href="/blog/archives">Blog Archives</a></li>
      
        <li class="next"><a href="">Newer&nbsp;&rarr;</a></li>
      
    </ul>
  </div>
</div>

        </div>
      </div>
    </div>
    <footer role="contentinfo"><div class="container">
    <p class="text-muted credits">
  Copyright &copy; 2023 - Carin Meier<br>
  <small>
      <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>,
      <span class="credit">customized with <a href="https://github.com/bhrigu123/abacus">abacus theme</a></span>.
  </small>
</p>

</div>
</footer>
    

<script type="text/javascript">
      var disqus_shortname = 'squidsblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>


<script src="/assets/bootstrap/dist/js/bootstrap.min.js"></script>
<script src="/javascripts/modernizr.js"></script>


  </body>
</html>
