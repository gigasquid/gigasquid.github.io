<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Clojure | Squid's Blog]]></title>
  <link href="http://gigasquid.github.io/blog/categories/clojure/atom.xml" rel="self"/>
  <link href="http://gigasquid.github.io/"/>
  <updated>2016-02-11T11:33:37-05:00</updated>
  <id>http://gigasquid.github.io/</id>
  <author>
    <name><![CDATA[Carin Meier]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Fairy Tale Word Vectors]]></title>
    <link href="http://gigasquid.github.io/blog/2016/02/10/fairy-tale-word-vectors/"/>
    <updated>2016-02-10T21:27:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2016/02/10/fairy-tale-word-vectors</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="http://c2.staticflickr.com/2/1558/24654386380_bda44419a8_n.jpg">http://c2.staticflickr.com/2/1558/24654386380_bda44419a8_n.jpg</a>"></p>

<p>This post continues our exploration from the last blog post <a href="http://gigasquidsoftware.com/blog/2016/02/06/why-hyperdimensional-socks-never-match/">Why Hyperdimensional Socks Never Match</a>.  We are still working our way through <a href="http://redwood.berkeley.edu/pkanerva/papers/kanerva09-hyperdimensional.pdf">Kanerva&rsquo;s paper</a>.  This time, with the basics of hypervectors under our belts, we&rsquo;re ready to explore how words can be expressed as context vectors.  Once in a high dimensional form, you can compare two words to see how similar they are and even perform reasoning.</p>

<p>To kick off our word vector adventure, we need some words.  Preferring whimsy over the Google news, our text will be taken from ten freely available fairy tale books on <a href="http://www.gutenberg.org/">http://www.gutenberg.org/</a>.</p>

<h3>Gather ye nouns</h3>

<p>Our goal is to assemble a frequency matrix, with all the different nouns as the rows and the columns will be the counts of if the word appears or not in the document.  Our matrix will be binary with just 1s and 0s.  The <em>document</em> will be a sentence or fragment of words.  A small visualization is below.</p>

<table>
<thead>
<tr>
<th></th>
<th> Noun          </th>
<th align="center"> Doc1</th>
<th> Doc2</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> flower        </td>
<td align="center"> 1   </td>
<td>0</td>
</tr>
<tr>
<td></td>
<td> king          </td>
<td align="center"> 0   </td>
<td>1</td>
</tr>
<tr>
<td></td>
<td> fairy         </td>
<td align="center"> 1   </td>
<td>0</td>
</tr>
<tr>
<td></td>
<td> gold          </td>
<td align="center"> 1   </td>
<td>1</td>
</tr>
</tbody>
</table>


<p>The size of the matrix will be big enough to support hypervector behavior, but not so big as to make computation too annoyingly slow.  It will be nouns x 10,000.</p>

<p>The first task is to get a set of nouns to fill out the rows.  Although, there are numerous online sources for linguistic nouns, they unfortunately do not cover the same language spectrum as old fairy tale books.  So we are going to collect our own.  Using <a href="http://stanfordnlp.github.io/CoreNLP/">Stanford CoreNLP</a>, we can collect a set of nouns using Grimm&rsquo;s Book as a guide.  There are about 2500 nouns there to give us a nice sample to play with.  This makes our total matrix size ~ 2500 x 10,000.</p>

<p>Now that we have our nouns, let&rsquo;s get down to business.  We want to create an index to row to make a <code>noun-idx</code> and then create a sparse matrix for our word frequency matrix.</p>

<p>```clojure
(ns hyperdimensional-playground.context-vectors
  (:require [clojure.core.matrix :as m]</p>

<pre><code>        [clojure.core.matrix.linear :as ml]
        [clojure.string :as string]
        [hyperdimensional-playground.core :refer [rand-hv cosine-sim mean-add inverse xor-mul]]
        [hyperdimensional-playground.fairytale-nouns :refer [fairy-tales-nouns book-list]]))
</code></pre>

<p>(m/set-current-implementation :vectorz)
;; size of the hypervectors and freq matrix columns
(def sz 10000)
;; The nouns come from a sampling of Grimm&rsquo;s fairy tale nouns these will
;; make up the rows in the frequency matrix
(def noun-idx (zipmap fairy-tales-nouns (range)))
(def freq-matrix (m/new-sparse-array [(count fairy-tales-nouns) sz]))
```</p>

<p>The next thing we need to do is to have some functions to take a book, read it in, split it into documents and then update the frequency matrix.</p>

<h3>Random indexing for the win</h3>

<p>The interesting thing about the update method is that we can use <em>random indexing</em>.  We don&rsquo;t need to worry about having a column for each document.  Because of the nature of hyperdimensions, we can randomly assign 10 columns for each document.</p>

<p>```clojure
(defn update-doc!
  &ldquo;Given a document &ndash; upate the frequency matrix using random indexing&rdquo;
  [doc]
  (let [known-nouns (clojure.set/intersection fairy-tales-nouns (set doc))]</p>

<pre><code>; use positive random indexing
(doall (repeatedly 10 #(doseq [noun known-nouns]
                   (m/mset! freq-matrix (get noun-idx noun) (rand-int sz) 1))))))
</code></pre>

<p>```</p>

<p>The whole book is processed by slurping in the contents and using a regex to split it up into docs to update the matrix.</p>

<p>```clojure
(defn process-book
  &ldquo;Load a book and break it into sentence like documents and update the frequency matrix&rdquo;
  [book-str]
  (let [book-text (slurp book-str)</p>

<pre><code>    docs (partition 25 (map string/lower-case
                            (string/split book-text #"\s|\.|\,|\;|\!|\?")))
    doc-count (count docs)]
(println "processing:" book-str "(with" doc-count "docs)")
(doall (map-indexed (fn [idx doc]
                      (when (zero? (mod idx 1000)) (println "doc:" idx))
                      (update-doc! doc))
                    docs))
(println "DONE with " book-str)))
</code></pre>

<p>```</p>

<p>We can now run the whole processing with:</p>

<p>```clojure
(doseq [book book-list]</p>

<pre><code>(process-book book))
</code></pre>

<p>```</p>

<p>On my system, it only takes about 3 seconds.</p>

<p>Great!  Now we have hypervectors associated with word frequencies.  They are now <em>context word vectors</em>.  What can we do with them.</p>

<h3>How close is a king to a goat?</h3>

<p>One of the things that we can do with them is find out a measure of how closely related the context of two words are by a measure of their cosine similarity.  First, we need a handy function to turn a string word into a word vector by getting it out of our frequency matrix.</p>

<p>```clojure
(defn wv [word]
  &ldquo;Get a hypervector for the word from the frequency matrix&rdquo;
  (let [i (get noun-idx word)]</p>

<pre><code>(assert (not (nil? i)) (str word " not found"))
(m/slice freq-matrix i)))
</code></pre>

<p>```</p>

<p>Then we can make another nice function to compare two words and give a informational map back.</p>

<p>```clojure
(defn compare-wvs
  &ldquo;Compare two words and give the cosine distance info map&rdquo;
  [word1 word2]
  (let [wv1 (wv word1)</p>

<pre><code>    wv2 (wv word2)]
(when (not= word1 word2)
  {:word1 word1
   :word2 word2
   :cosine (cosine-sim wv1 wv2)})))
</code></pre>

<p>```</p>

<p>Let&rsquo;s take a look at the similarities of some words to <em>king</em>.</p>

<p>```clojure
(sort-by :cosine[(compare-wvs &ldquo;king&rdquo; &ldquo;queen&rdquo;)</p>

<pre><code>             (compare-wvs "king" "prince")
             (compare-wvs "king" "princess")
             (compare-wvs "king" "guard")
             (compare-wvs "king" "goat")])
</code></pre>

<p>  ;; ({:word1 &ldquo;king&rdquo;, :word2 &ldquo;goat&rdquo;, :cosine 0.1509151478896664}
  ;;  {:word1 &ldquo;king&rdquo;, :word2 &ldquo;guard&rdquo;, :cosine 0.16098893367403827}
  ;;  {:word1 &ldquo;king&rdquo;, :word2 &ldquo;queen&rdquo;, :cosine 0.49470535530616655}
  ;;  {:word1 &ldquo;king&rdquo;, :word2 &ldquo;prince&rdquo;, :cosine 0.5832521795716931}
  ;;  {:word1 &ldquo;king&rdquo;, :word2 &ldquo;princess&rdquo;, :cosine 0.5836922474743367})
```</p>

<p>As expected, the royal family is closer to the king then a guard or goat is.</p>

<p>One of the interesting things is that now we can do addition and subtraction with these word vectors and see how it affects the relation with other words.</p>

<h3>Boy + Gold = King, Boy + Giant = Jack</h3>

<p>We can take a look at how close <em>boy</em> and <em>king</em> are together by themselves.</p>

<p><code>clojure
(cosine-sim (wv "boy") (wv "king")) ;=&gt; 0.42996397142253145
</code></p>

<p>Now we can add some gold to the boy and that new word vector will be closer to king than boy was alone.</p>

<p>```clojure
(cosine-sim (mean-add (wv &ldquo;boy&rdquo;) (wv &ldquo;gold&rdquo;))</p>

<pre><code>          (wv "king")) ;=&gt; 0.5876251031366048
</code></pre>

<p>```</p>

<p>Doing the same for <em>boy</em> and <em>jack</em>, we find that adding a giant moves the context closer.</p>

<p>```clojure
(cosine-sim (wv &ldquo;boy&rdquo;) (wv &ldquo;jack&rdquo;)) ;=> 0.33102858702785953
;; boy + giant = jack
(cosine-sim (mean-add (wv &ldquo;giant&rdquo;) (wv &ldquo;boy&rdquo;))</p>

<pre><code>          (wv "jack")) ;=&gt;0.4491473187787431
</code></pre>

<p>```</p>

<p>Amusingly, a frog and a princess make a prince.</p>

<p><code>clojure
;;; frog + princess = prince
  (cosine-sim (wv-add "frog" "princess") (wv "prince")) ;=&gt; 0.5231641991974249
</code></p>

<p>We can take this even farther by subtracting words and adding others.  For example a similarity to the word queen can be obtained by subtracting man from king and adding woman.</p>

<p>```clojure
;;; queen= (king-man) + woman
  (cosine-sim (wv &ldquo;queen&rdquo;)</p>

<pre><code>          (mean-add (wv "woman") (wv-subtract "king" "man"))) ;=&gt;0.5659832204544486
</code></pre>

<p>```</p>

<p>Similarly, a contextual closeness to father can be gotten from subtracting woman from mother and adding man.</p>

<p>```clojure
(cosine-sim (wv &ldquo;father&rdquo;)</p>

<pre><code>          (mean-add (wv "man") (wv-subtract "mother" "woman"))) ;=&gt;0.5959841177719538
</code></pre>

<p>```</p>

<p>But wait, that&rsquo;s not all.  We can also do express facts with these word vectors and <em>reason</em> about them.</p>

<h3>Reasoning with word vector with the database as a hyperdimensional value</h3>

<p>The curious nature of hypervectors allows the storage of multiple entity, attributes in it and allow the retrieval of the likeness of them later by simple linear math &ndash; using only xor multiplication and addition.  This gives us the <em>database as a value</em> in the form of a high dimensional vector.</p>

<p>For an example, say we want to express the fact that Hansel is a brother of Gretel.  We can do this by adding the xor product of brother with hansel and the product of brother with Gretel.</p>

<p>```clojure
;; hansel is the brother of gretel
;; B<em>H + B</em>G
(def hansel-brother-of-gretel
   (mean-add</p>

<pre><code>(xor-mul (wv "brother") (wv "hansel"))
(xor-mul (wv "brother") (wv "gretel"))))
</code></pre>

<p>```</p>

<p>Also we can express that Jack is a bother of Hansel.</p>

<p>```clojure
(def jack-brother-of-hansel
  (mean-add</p>

<pre><code> (xor-mul (wv "brother") (wv "jack"))
 (xor-mul (wv "brother") (wv "hansel"))))
</code></pre>

<p>```</p>

<p>We can add these two facts together to make a new hypervector value.</p>

<p>```clojure
(def facts (mean-add hansel-brother-of-gretel</p>

<pre><code>                   jack-brother-of-hansel))
</code></pre>

<p>```</p>

<p>Now we can actually <em>reason</em> about them and ask questions.  Is Jack a brother of Hansel?  With a high cosine similarity, we can assume the answer is likely.</p>

<p>```clojure
 ;; is jack the brother of hansel?
  (cosine-sim
   (wv &ldquo;jack&rdquo;)
   (xor-mul (mean-add (wv &ldquo;brother&rdquo;) (wv &ldquo;gretel&rdquo;))</p>

<pre><code>        facts)) ;=&gt;0.8095270629815969
</code></pre>

<p>```</p>

<p>What about someone unrelated.  Is Cinderella the brother of Gretel? &ndash; No</p>

<p>```clojure
 ;; is cinderella the brother of gretel ?
  (cosine-sim
   (wv &ldquo;cinderella&rdquo;)
   (xor-mul (mean-add (wv &ldquo;brother&rdquo;) (wv &ldquo;gretel&rdquo;))</p>

<pre><code>        facts)) ;=&gt;0.1451799916656951
</code></pre>

<p>```</p>

<p>Is Jack the brother of Gretel &ndash; Yes</p>

<p>```clojure
 ;; is jack the brother of gretel ?
  (cosine-sim
   (wv &ldquo;jack&rdquo;)
   (xor-mul (mean-add (wv &ldquo;brother&rdquo;) (wv &ldquo;gretel&rdquo;))</p>

<pre><code>        facts)) ;=&gt; 0.8095270629815969
</code></pre>

<p>```</p>

<p>We can take this further by adding more facts and inventing a relation of our own.</p>

<h3>Siblings in Hyperspace</h3>

<p>Let&rsquo;s invent a new word vector that is not in our nouns &ndash; <em>siblings</em>.  We are going to create new random hypervector to represent it.</p>

<p><code>clojure
(def siblings (rand-hv))
</code></p>

<p>We will define it in terms of word vectors that we already have.  That is, siblings will be a the sum of brother + sister.  We XOR multiply it by siblings to associate it with the hypervector.</p>

<p>```clojure
(def siblings-brother-sister</p>

<pre><code>(mean-add (xor-mul siblings (wv "brother")) (xor-mul siblings (wv "sister"))))
</code></pre>

<p>```</p>

<p>Now we can add some more facts.  Gretel is a sister of Hansel.</p>

<p>```clojure
 ;; gretel is the sister of hansel
  ;; S<em>G + S</em>H
  (def gretel-sister-of-hansel</p>

<pre><code>(mean-add
 (xor-mul (wv "sister") (wv "gretel"))
 (xor-mul (wv "sister") (wv "hansel"))))
</code></pre>

<p>```</p>

<p>Gretel is also a sister of Jack.</p>

<p>```clojure
  ;; gretel is the sister of jack
  ; S<em>G + S</em>H
  (def gretel-sister-of-jack</p>

<pre><code>(mean-add
 (xor-mul (wv "sister") (wv "gretel"))
 (xor-mul (wv "sister") (wv "jack"))))
</code></pre>

<p>```</p>

<p>Collecting all of our facts into one hypervector (as a database).</p>

<p>```clojure
 (def facts (mean-add hansel-brother-of-gretel</p>

<pre><code>                   jack-brother-of-hansel
                   gretel-sister-of-jack
                   gretel-sister-of-hansel
                   siblings-brother-sister))
</code></pre>

<p>```</p>

<p>Now we can ask some for questions.</p>

<p>Are Hansel and Gretel siblings? &ndash; Yes</p>

<p><code>clojure
;; are hansel and gretel siblings?
  (cosine-sim
   (mean-add (wv "hansel") (wv "gretel"))
   (xor-mul siblings facts)) ;=&gt;0.627015379034067
</code></p>

<p>Are John and Roland siblings &ndash; No</p>

<p><code>clojure
;; are john and roland siblings?
  (cosine-sim
   (mean-add (wv "roland") (wv "john"))
   (xor-mul siblings facts)) ;=&gt; 0.1984017637065277
</code></p>

<p>Are Jack and Hansel siblings? &ndash; Yes</p>

<p>```clojure
  (cosine-sim</p>

<pre><code>(mean-add (wv "jack") (wv "hansel"))
(xor-mul siblings facts)) ;=&gt;0.48003572523507465
</code></pre>

<p>```</p>

<p>It is interesting to think of that nothing is stopping us at this point from <em>retracting</em> facts by simply subtracting the fact encoded word vectors from our &ldquo;database&rdquo; value and making a new value from it.</p>

<h3>Conclusions</h3>

<p>In this fun, but casual exploration of word vector we have seen the potential for reasoning about language in a way that uses nothing more complicated than addition and multiplication.  The ability to store dense information in hypervectors, extract it with simple methods, and flexibly collect it randomly, shows its versatility and power.  Hyperdimensional vectors  might hold the key to unlocking a deeper understanding of cognitive computing or perhaps even true artificial intelligence.</p>

<p>It is interesting to note that this technique is not limited to words. Other applications can be done the same way.  For example a video recommendation using a hypervector with movie titles.  Or perhaps even anomaly detection using sensor readings over a regular weekly time period.</p>

<p>Looking over our journey with word vectors.  At the beginning it seemed that word vectors were magical.  Now, after an understanding of the basics, it still seems like magic.</p>

<p>If you are interested in exploring further, feel free to use my github <a href="https://github.com/gigasquid/hyperdimensional-playground">hyperdimensional-playground</a> as a starting point.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why Hyperdimensional Socks Never Match]]></title>
    <link href="http://gigasquid.github.io/blog/2016/02/06/why-hyperdimensional-socks-never-match/"/>
    <updated>2016-02-06T14:26:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2016/02/06/why-hyperdimensional-socks-never-match</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="http://c2.staticflickr.com/8/7238/7188420611_a99f936971_n.jpg">http://c2.staticflickr.com/8/7238/7188420611_a99f936971_n.jpg</a>"></p>

<p>The nature of computing in hyperdimensions is a strange and wonderful place.   I have only started to scratch the surface by reading a paper by <a href="http://redwood.berkeley.edu/pkanerva/papers/kanerva09-hyperdimensional.pdf">Kanerva</a>. Not only is it interesting from a computer science standpoint, it&rsquo;s also interesting from a cognitive science point of view.  In fact, it could hold the key to better model AI and general reasoning.  This blog is a casual stroll through some of the main points of Kanerva&rsquo;s paper along with examples in Clojure to make it tangible.  First things first, what is a hyperdimension?</p>

<h3>What is a Hyperdimension and Where Are My Socks?</h3>

<p>When we are talking about hyperdimensions, we are really talking about <em>lots</em> of dimensions.  A vector has dimensions.  A regular vector could have three dimensions <code>[0 1 1]</code>, but a hyperdimensional vector has tons more, like 10,000 or 100,000.  We call these big vectors <em>hypervectors</em> for short, which makes them sound really cool. Although the vectors could be make up of anything, we are going to use vectors made up of zeros and ones.  To handle big computing with big vectors in a reasonable amount of time, we are also going to use <em>sparse</em> vectors.  What makes them sparse is that most of the space is empty, (zeros).  In fact, Clojure has a nice library to handle these sparse vectors.  The <a href="https://github.com/mikera/core.matrix">core.matrix</a> project from Mike Anderson is what we will use in our examples.  Let&rsquo;s go ahead and make some random hypervectors.</p>

<p>First we import the core.matrix libraries and set the implementation to vectorz which provides fast double precision vector math.</p>

<p>```clojure
(ns hyperdimensional-playground.core
  (:require [clojure.core.matrix :as m]</p>

<pre><code>        [clojure.core.matrix.linear :as ml]))
</code></pre>

<p>(m/set-current-implementation :vectorz)
```</p>

<p>Next we set the sz of our hypervectors to be 100,000.  We also create a function to generate a random sparse hypervector by choosing to put ones in about 10% of the space.</p>

<p>```clojure
(def sz 100000)</p>

<p>(defn rand-hv []
  (let [hv (m/new-sparse-array [sz])</p>

<pre><code>    n (* 0.1 sz)]
(dotimes [i n]
  (m/mset! hv (rand-int sz) 1))
hv))
</code></pre>

<p>```</p>

<p>Now we can generate some.</p>

<p>```clojure
(def a (rand-hv))
(def b (rand-hv))
(def c (rand-hv))
(def d (rand-hv))</p>

<p>a ;=> #vectorz/vector Large vector with shape: [100000]
```</p>

<p>You can think of each of this hypervectors as random hyperdimensional sock, or hypersock, because that sounds cooler.  These hypersocks, have curious properties.  One of which is that they will ~never match.</p>

<h3>Hypersocks never match</h3>

<p>Because we are dealing with huge amount of dimensions, a mathematically peculiar probability distribution occurs.  We can take a random hypervector to represent something, then take another one and they will different from each by about 100 STD. We can take another one and it too, will be 100 STD from the other ones.  For practical purposes, we will run out of time before we will run of vectors that are unrelated.  Because of this, any two hypersocks will never match each other.</p>

<p>How can we tell how similar two hypersocks are?  The cosine to tell the similarity between two vectors.  This is determined by the dot product.  We can construct a <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> function to give us a value from -1 to 1 to measure how alike they are with 1 being the same and -1 being the complete opposite.</p>

<p>```clojure
(defn cosine-sim [v1 v2]
  (/ (m/dot v1 v2)</p>

<pre><code> (* (ml/norm v1) (ml/norm v2))))
</code></pre>

<p>```</p>

<p>If we looks at the similarity of a hypervector with itself, the result is ~1.  We the other random hypervectors, it is ~0.</p>

<p>```clojure
(cosine-sim a a) ;=>  1.0
(cosine-sim d d) ;=> 1.0</p>

<p>(cosine-sim a b) ;=>  0.0859992468320239
(cosine-sim b c) ;=> 0.09329186588790261
(cosine-sim a c) ;=> 0.08782018973001954
```</p>

<p>There are other cool things we can do with hypervectors, like do math with them.</p>

<h3>The Hamming Distance of Two Hypersocks</h3>

<p>We can add hypervectors together with a sum mean vector. We add the vector of 1s and 0s together then we divide the resulting vector by the number of total vectors.  Finally, to get back to our 1s and 0s, we round the result.</p>

<p><code>clojure
(defn mean-add [&amp; hvs]
  (m/emap #(Math/round (double %))
   (m/div (apply m/add hvs) (count hvs))))
</code></p>

<p>The interesting thing about addition is that the result is similar to all the vectors in it.  For example, if we add a and b together to make x, <code>x = a + b</code>, then x will be similar to a and similar to b.</p>

<p><code>clojure
;; x = a + b
(def x (mean-add a b))
(cosine-sim x a) ;=&gt; 0.7234734658023224
(cosine-sim x b) ;=&gt; 0.7252586504505658
</code></p>

<p>You can also do a very simple for of multiplication on vectors with 1s and 0s with using <em>XOR</em>.  We can do this by add the two vectors together and then mapping <code>mod 2</code> on each of the elements.</p>

<p>```clojure
(defn xor-mul [v1 v2]
  (&ndash;>> (m/add v1 v2)</p>

<pre><code>  (m/emap #(mod % 2))))
</code></pre>

<p>```</p>

<p>We can actually use this <code>xor-mul</code> to calculate the <a href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a>, which is an important measure of error detection.  The Hamming distance is simply the sum of all of the xor multiplied elements.</p>

<p>```clojure
(defn hamming-dist [v1 v2]
  (m/esum (xor-mul v1 v2)))</p>

<p>(hamming-dist [1 0 1] [1 0 1]) ;=> 0
(hamming-dist [1 0 1 1 1 0 1] [1 0 0 1 0 0 1]) ;=> 2
(hamming-dist a a) ;=> 0
```</p>

<p>This illustrates a point that xor multiplication randomizes the hypervector, but preserves the distance.  In the following example, we xor multiply two random hypervectors by another and the hamming distance stays the same.</p>

<p>```clojure
; xa = x * a
; ya = y * a
; hamming distance of xa is the same as ya</p>

<p>;; multiplication randomizes but preserves the distance
(def x (rand-hv))
(def y (rand-hv))
(def xa (xor-mul x a))
(def ya (xor-mul y a))
(hamming-dist xa ya) ;=> 1740.0
(hamming-dist x y) ;=> 1740.0
```</p>

<p>So you can xor multiply your two hypersocks and move them to a different point in hyperspace, but they will still be the same distance apart.</p>

<p>Another great party trick in hyperspace, is the ability to bind and unbind hypervectors for use as map like pairs.</p>

<h3>Using Hypervectors to Represent Maps</h3>

<p>A map of pairs is a very important data structure.  It gives the ability to bind symbols to values and then retrieve those values.  We can do this with hypervectors too.  Consider the following structure:</p>

<p><code>clojure
{:name "Gigasquid"
 :cute-animal "duck"
 :favorite-sock "red plaid"}
</code></p>

<p>We can now create hypervectors to represent each of these values.  Then we can xor the hypervector symbol to the hypervector value and sum them up.</p>

<p>```clojure
;; data records with bound pairs
(def x (rand-hv)) ;; favorite-sock
(def y (rand-hv)) ;; cute-animal
(def z (rand-hv)) ;; name
(def a (rand-hv)) ;; red-plaid
(def b (rand-hv)) ;; duck
(def c (rand-hv)) ;; gigasquid</p>

<p>;H = X * A + Y * B + Z * C
(def h (mean-add (xor-mul x a) (xor-mul y b) (xor-mul z c)))
```</p>

<p>Now, we have a sum of all these things and we want to find the value of the <em>favorite sock</em>.  We can <em>unbind</em> it from the sum by xor multiplying the favorite-sock hypervector <code>x</code>.  Because of the property that xor multiplication both distributes and cancels itself out.</p>

<p><code>clojure
(hamming-dist (xor-mul x (xor-mul x a)) a) ;=&gt; 0
</code></p>

<p>We can compare the result with the known values and find the closest match.</p>

<p>```clojure
(hamming-dist a (xor-mul x h)) ;=> 1462.0  ;; closest to &ldquo;red-plaid&rdquo;
(hamming-dist b (xor-mul x h)) ;=> 1721.0
(hamming-dist c (xor-mul x h)) ;=> 1736.0</p>

<p>(cosine-sim a (xor-mul x h)) ;=> 0.3195059768353112 ;; closest to &ldquo;red-plaid&rdquo;
(cosine-sim b (xor-mul x h)) ;=> 0.1989075567830733
(cosine-sim c (xor-mul x h)) ;=> 0.18705233578983288
```</p>

<h3>Conclusion</h3>

<p>We have seen that the nature of higher dimensional representation leads to some very interesting properties with both representing data and computing with it.  These properties and others form the foundation of exciting advancements in Cognitive Computing like word vectors.  Future posts will delve further into these interesting areas.  In the meantime, I encourage you to read Kanerva&rsquo;s paper on your own and to find comfort in that when you can&rsquo;t find one of your socks, it&rsquo;s not your fault. It most likely has something to do with curious nature of hyperspace.</p>

<p><em>Thanks to <a href="https://twitter.com/ross_gayler">Ross Gayler</a> for bringing the paper to my attention and to <a href="https://twitter.com/solussd">Joe Smith</a> for the great conversations on SDM</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speech Act Classification for Text with Clojure]]></title>
    <link href="http://gigasquid.github.io/blog/2015/10/20/speech-act-classification-for-text-with-clojure/"/>
    <updated>2015-10-20T13:11:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2015/10/20/speech-act-classification-for-text-with-clojure</id>
    <content type="html"><![CDATA[<p>We humans are quite wonderful.  We do amazing things every day without even realizing it.  One of them, you are doing right now.  You are reading text.  Your brain is taking these jumbles of letters and spaces in this sentence, which in linguist terms is called an <a href="https://en.wikipedia.org/wiki/Utterance">utterance</a>, and making sense out of it.  The individual meanings of sentences might be quite complex.</p>

<p>Take for example the utterance, &ldquo;I like cheese&rdquo;.  To understand it properly, you need to know the meanings of the individual words.  In particular, you would need to know that cheese is a tasty food stuff that is made from milk.  This would be a detailed and full understanding.  But there is a higher level of understanding that we can look at called <a href="https://en.wikipedia.org/wiki/Utterance">Speech Acts</a>.</p>

<p>Speech Acts are way of classifying our communication according to purposes.  We perform speech acts when we ask questions, make statements, requests, promises, or even just say thanks.  These speech acts cross languages.  When I ask a question in English, it is the same speech act as if I ask it in French.  In fact, if we were to travel to another planet with aliens, we can assume if they had a language, it would involve speech acts.  It should be no surprise then, to communicate effectively with machines, it will have to understand speech acts.</p>

<p>To explore this communication we are going to consider only three speech acts:</p>

<ul>
<li>Statements &ndash; &ldquo;I like cheese.&rdquo;</li>
<li>Questions &ndash; &ldquo;How do you make cheese?&rdquo;</li>
<li>Expressives &ndash; &ldquo;Thank you&rdquo;</li>
</ul>


<p><em>Our goal is to have our program be able to tell the difference between these three speech acts &ndash; without punctuation.</em></p>

<p>Why not use punctuation?  If you are having a conversation with a human over Slack or some other chat channel, you may or may not put in a question mark or period.  To have a computer be able to converse as naturally with a human as another human, it will have to understand the speech act without the aid of punctuation.</p>

<p>Generally, we want to have the computer:</p>

<ol>
<li>Read in an utterance/text that may or may not have punctuation.</li>
<li>Classify whether the result is a statement, question, or expressive.</li>
</ol>


<p>To tackle this problem, we are going to have to break this up into two main parts.  The first is <em>parsing</em> the text and annotating it with data.  The second is to classify the text based on the data from the parsing.</p>

<h2>Parsing and Annotating Text with Stanford CoreNLP</h2>

<p>The <a href="http://nlp.stanford.edu/software/corenlp.shtml">Stanford CoreNLP</a> is considering the state of the art for POS, (Part of Speech), tagging and other linguistic annotations.  It also is a Java library, so very easy to use from Clojure.</p>

<p>Here we are using a simple wrapper library called <a href="https://github.com/gigasquid/stanford-talk">stanford-talk</a> to take in some text and process it.  The result is a list of <em>tokens</em> for each word in the <code>:token-data</code> map.  Each token is annotated with the POS tag.  There is a lot more data in the annotations that we can look at to give us insight into this text.  But, to keep things simple, we are just going to look at the POS speech tag at the moment.</p>

<p>```clojure
(process-text &ldquo;I like cheese&rdquo;)</p>

<p>;{:token-data
; ({:sent-num 0, :token &ldquo;I&rdquo;, :pos &ldquo;PRP&rdquo;, :net &ldquo;O&rdquo;, :lemma &ldquo;I&rdquo;, :sentiment &ldquo;Neutral&rdquo;}
;  {:sent-num 0, :token &ldquo;like&rdquo;, :pos &ldquo;VBP&rdquo;, :net &ldquo;O&rdquo;, :lemma &ldquo;like&rdquo;, :sentiment &ldquo;Neutral&rdquo;}
;  {:sent-num 0, :token &ldquo;cheese&rdquo;, :pos &ldquo;NN&rdquo;, :net &ldquo;O&rdquo;, :lemma &ldquo;cheese&rdquo;, :sentiment &ldquo;Neutral&rdquo;}),
; :refs [[{:sent-num 0, :token &ldquo;I&rdquo;, :gender &ldquo;UNKNOWN&rdquo;, :mention-type &ldquo;PRONOMINAL&rdquo;, :number &ldquo;SINGULAR&rdquo;, :animacy &ldquo;ANIMATE&rdquo;};]]}
```</p>

<p>So the text &ldquo;I like cheese&rdquo; has the following POS tags <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">list of all POS tags</a>:</p>

<ul>
<li>I = PRP Personal Pronoun</li>
<li>like = VBP Verb, non-3rd person singular present</li>
<li>cheese &ndash; Noun, singular or mass</li>
</ul>


<p>This is great.  We have some data about the text we can analyze.  The next thing to do is to figure out how to classify the text based on this data.</p>

<h2>Classification with Weka</h2>

<p><a href="http://www.cs.waikato.ac.nz/~ml/weka/">Weka</a> is a collection of machine learning algorithms.  There is a program for interactive exploration of data sets, as well as a java library so you can use it programatically.  Speaking of data sets, we need some.  Just having one sentence about liking cheese is not going to get us very far with any machine learning.</p>

<p>So where can you go to get conversational questions and statements on the internet?  Well, one place that is pretty good for that is <a href="http://www.answers.com/Q/FAQ/2528-9">answers.com</a>.  We can scrape some pages for questions and answers.  Enough so that we can collect and cleanup some input files of</p>

<ul>
<li>~ 200 statements</li>
<li>~ 200 questions</li>
</ul>


<p>The expressives were a bit more difficult.  Let&rsquo;s just make a list of about 80 of them.</p>

<p>Now, we have a list of text data.  We need to decide on some features and generate some input files to train the classifiers on.</p>

<h3>Choosing Features for Classification</h3>

<p>First, what is a feature?  A feature is some sort of encoding of the data that the computer is going to consider for classification. For example, the number of nouns in a sentence could be a feature.  There is a whole field of study dedicated to figuring out what the best features for data for machine learning are.  Again, to keep things simple, we can take an educated guess on some features based on a good <a href="https://www.cs.utah.edu/~riloff/pdfs/emnlp11-speechacts.pdf">paper</a>:</p>

<ul>
<li>Sentence length</li>
<li>Number of nouns in the sentence (NN, NNS, NNP, NNPS)</li>
<li>If the sentence ends in a noun or adjective (NN, NNS, NNP, NNPS, JJ, JJR, JJS)</li>
<li>If the sentence begins in a verb (VB, VBD, VBG, VBP, VPZ)</li>
<li>The count of the wh, (like who, what) markers (WDT, WRB, WP, WP$)</li>
</ul>


<p>We can now go through our data file, generate our feature data, and output .arff file format to ready it as training file for weka.</p>

<p>Raw question file example:</p>

<p><code>
What is floater exe
Is bottled water or tap water better for your dog
How do you treat lie bumps on the tongue
Can caffeine be used in powder form
</code></p>

<p>Arff file with features</p>

<p>```
@relation speechacts</p>

<p>@attribute       sen_len            numeric
@attribute       nn_num             numeric
@attribute       end_in_n           numeric
@attribute       begin_v            numeric
@attribute       wh_num             numeric
@attribute       type               {assertion,question,expressive}</p>

<p>@data
4,2,1,0,1,question
10,4,1,0,0,question
9,3,1,0,1,question
7,3,1,0,0,question
```</p>

<p>Now that we have our input file to training our machine learning algorithms, we can start looking at classifiers.</p>

<h3>Choosing the Classifier</h3>

<p><img class="<a" src="href="http://c2.staticflickr.com/6/5794/22338401315_1abf0ffb2d_z.jpg">http://c2.staticflickr.com/6/5794/22338401315_1abf0ffb2d_z.jpg</a>"></p>

<p>Using the weka explorer, we can try out different classification models.  For this data, the best one seems to be the <a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a>.  In the explorer, it beat out Naive Bayes and J48.  It is also worth mentioning that we are not using a separate source of test data, we are cross validating on the original training set.  If we wanted to be more rigorous, we could collect more data and cut it in half, using one set for the training and one set for the testing.</p>

<p>Now that we have a classifier, we can create some Clojure code with the java library to use it.</p>

<h3>Using the Weka Classifier from our Clojure Code</h3>

<p>After importing the needed Java classes into our Clojure code, we can create the Random Forest classifier.</p>

<p><code>clojure
(def classifier (new RandomForest))
</code></p>

<p>We then create a function that will load our arff input file as a datasource</p>

<p>```clojure
(defn get-datasource [fname]
  (new ConverterUtils$DataSource</p>

<pre><code>   (.getResourceAsStream (clojure.lang.RT/baseLoader) fname)))
</code></pre>

<p>```</p>

<p>And another uses it to train the classifier, returning a map of the <em>evaluator</em> and <em>data</em> that we will need for our predictions.</p>

<p>```clojure
(defn train-classifier [fname]
  (let [source (get-datasource fname)</p>

<pre><code>    data (.getDataSet source)
    _ (.setClassIndex data (dec (.numAttributes data)))
    _  (.buildClassifier classifier data)
    e (new Evaluation data)]
(.crossValidateModel e classifier data (.intValue (int 10)) (new Random 1) (into-array []))
(println (.toSummaryString e))
{:evaluator e
 :data data}))
</code></pre>

<p>```</p>

<p>Now, we need to be able to ask about a classification for a particular instance of new data.  This is going to be where we are parsing new text and asking for an answer from our trained model.  To do this, we need to generate an <em>instance</em> for the evaluation to look at.  It is constructed from numbers in the same order as our arff file.  The exception is that we are not going to provide a value for the final field of the speech act type.  We will assign that to the a missing value.</p>

<p>```clojure
(defn gen-instance [dataset [val0 val1 val2 val3 val4]]
  (let [i (new Instance 6)]</p>

<pre><code>(doto i
  (.setValue 0 (double val0))
  (.setValue 1 (double val1))
  (.setValue 2 (double val2))
  (.setValue 3 (double val3))
  (.setValue 4 (double val4))
  (.setValue 5 (Instance/missingValue))
  (.setDataset dataset))))
</code></pre>

<p>```</p>

<p>Now we can use this function in a prediction function to get our answer back</p>

<p>```clojure
(defn predict [ev d vals]
  (let [v  (.evaluateModelOnce ev classifier (gen-instance d vals))]</p>

<pre><code>(case v
  0.0 :statement
  1.0 :question
  2.0 :expressive)))
</code></pre>

<p>```</p>

<p>Calling the predict function would look something like:</p>

<p><code>clojure
(def results (train-classifier "speech-acts-input-all.arff"))
(def predictor (:evaluator results))
(def data (:data results))
(predict predictor data [1 1 1 0 0])
;; -&gt; :expressive
</code></p>

<p>Now that we have the parsing piece and the classification piece, we can put everything together.</p>

<h3>Putting it all together</h3>

<p>We finally have all the details we need write a <code>classify-text</code> function.</p>

<p>```clojure
(defn classify-text [text]
  (let [stats (parser/gen-stats text)</p>

<pre><code>    features [(:sen-len stats)
              (:nn-num stats)
              (:end-in-n stats)
              (:begin-v stats)
              (:wh-num stats)]]
(weka/predict predictor data features)))
</code></pre>

<p>(classify-text &ldquo;I like cheese&rdquo;)
;; &ndash;> :statement
(classify-text &ldquo;How do you make cheese&rdquo;)
;; &ndash;> :question
(classify-text &ldquo;Right on&rdquo;)
;; &ndash;> :expressive
```</p>

<p>Yea! It worked.  We finally have something that will read in text and tell us its best guess of a speech act, all without punctuation.  Let&rsquo;s quickly review what we have done.</p>

<h3>Summary</h3>

<ul>
<li>Gather data sets of statements, questions, and expressives</li>
<li>Parse text and annotate it with POS tags using Stanford CoreNLP</li>
<li>Choose features of the data to analyze and generate arff files</li>
<li>Use Weka explorer to try out the best classification algorithims</li>
<li>Programatically use weka to train classifier and predict a new instance</li>
<li>Write a program to tie it all together</li>
</ul>


<p>It&rsquo;s funny how a simple thing like asking whether something is a statement or question gets you knee deep in Natural Language Processing and Machine Learning pretty fast.</p>

<p>We&rsquo;ve learned a lot, now let&rsquo;s have a bit of fun.  Now that we can classify speech acts, we can make a sort of proto chat bot with a really limited responses.</p>

<h3>Proto Chat Bot</h3>

<p>Here we are going to be a bit loose and actually check if a question mark is used.  If it is, we will automatically mark it as a question.  Otherwise, we will classify it.</p>

<p>```clojure
(defn respond [text]
  (let [question-mark? (re-find  #&ldquo;\?$&rdquo; text)</p>

<pre><code>    type (if question-mark?
           :question
           (classify-text text))]
(case type
  :question "That is an interesting question."
  :statement "Nice to know."
  :expressive ":)")))
</code></pre>

<p>```</p>

<p>We just need a quick repl and main function now:</p>

<p>```clojure
(defn repl []
  (do</p>

<pre><code>(print "&gt;&gt; ")
(flush))
</code></pre>

<p>  (let [input (read-line)]</p>

<pre><code>(if-not (= input "quit")
 (do
   (println (try (c/respond input)
                 (catch Exception e (str "Sorry: " e " - " (.getMessage e)))))
   (recur))
 (do (println "Bye!")
     (System/exit 0)))))
</code></pre>

<p>(defn -main [&amp; args]
  (println &ldquo;Hello.  Let&rsquo;s chat.&rdquo;)
  (flush)
  (repl))
```</p>

<p>Testing it out with <code>lein run</code>, we can have a little chat:</p>

<p>```
Hello.  Let&rsquo;s chat.</p>

<blockquote><blockquote><p>Hi
:)
Do you know where I can go to buy cheese
That is an interesting question.
I am a big cheese fan
Nice to know.
you are quite smart
Nice to know.
bye
:)
```</p></blockquote></blockquote>

<p>Want more?  Check out the code <a href="https://github.com/gigasquid/speech-acts-classifier">https://github.com/gigasquid/speech-acts-classifier</a>.</p>

<p><em>Special thanks to <a href="https://twitter.com/ohpauleez">Paul deGrandis</a> for allowing me to pick his awesome brain on AI things</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Conversations with Datomic - Part 3]]></title>
    <link href="http://gigasquid.github.io/blog/2015/08/25/converstations-with-datomic-3/"/>
    <updated>2015-08-25T10:28:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2015/08/25/converstations-with-datomic-3</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://avatars0.githubusercontent.com/u/1478702?v=3&amp;s=200">https://avatars0.githubusercontent.com/u/1478702?v=3&amp;s=200</a>"></p>

<p><em>This is a continuation of the <a href="http://gigasquidsoftware.com/blog/2015/08/15/conversations-with-datomic/">first</a> and
<a href="http://gigasquidsoftware.com/blog/2015/08/19/conversations-with-datomic-part-2/">second</a> conversations in which topics such as creating databases,
learning facts, querying, and time traveling were discussed.  Today&rsquo;s topics include architecture, caching, and scaling.</em></p>

<p><strong>Human:</strong>  Hello again <a href="http://www.datomic.com/">Datomic</a>. Ready to talk again?</p>

<p><strong>Datomic:</strong> Sure.  I think you wanted to ask me some questions about how I would fit in with your other systems.</p>

<p><strong>Human:</strong> Yes.  Like I was saying earlier, I think your abilities to learn facts, reason about them, and keep track of the history of all those facts is really great.
I am interested in having you work with me every day, but first I want to understand your components so that I can make sure you are a good fit for us.</p>

<p><strong>Datomic:</strong> I would be happy to explain my architecture to you.  Perhaps showing you this picture is the best way to start.</p>

<p><img class="<a" src="href="http://c2.staticflickr.com/6/5723/20819693686_f9ec3852c3_z.jpg">http://c2.staticflickr.com/6/5723/20819693686_f9ec3852c3_z.jpg</a>"></p>

<p>I am made of three main parts: my <em>transactors</em>, my <em>peers</em>, and my <em>storage</em>.</p>

<p><strong>Human</strong>:  What is a <em>peer</em>?</p>

<p><strong>Datomic</strong>:  A <em>peer</em> is an application that is using the peer library.  In our last conversations,  we were talking through the Clojure api with <code>datomic.api</code>.  The application, or process, that is running this api is called a peer.  There can be many of these, all having conversations with me.</p>

<p><strong>Human</strong>: The peers then talk to your <em>transactor</em>?</p>

<p><strong>Datomic</strong> Yes. The peers talk to my transactor whenever you call <code>transact</code> with the peer library.  It is the central coordinator between all the peers and processes the requests using <em>ACID</em> transactions, and then sends the facts off to storage.</p>

<p><strong>Human:</strong>  Could you remind me what <em>ACID</em> stands for again?  I always forget.  The first one is <em>Atomic</em> right?</p>

<p><strong>Datomic:</strong> That is right.  I am <em>Atomic</em> in that every transaction you send to me is all or nothing.  If for some reason, one part of it fails, I will reject the entire transaction and leave my database unchanged.</p>

<p>The C is for <em>Consistency</em>.  This means that I provide every peer with a consistent view of facts over time and transactions.  I provide a global ordering of transactions across all the peers with my transactor and peers will always see all the transactions up to their current time without any gaps.</p>

<p><strong>Human:</strong> What if a peer is behind the global time?  How do they catch up to know about the new facts that were transacted by a different peer?</p>

<p><strong>Datomic:</strong> After one peer sends me a successful transaction with some new facts, I will notify all the peers about them.</p>

<p><strong>Human:</strong> Cool. That takes care of the A and C in ACID. What about the I?</p>

<p><strong>Datomic:</strong> It stands for Isolated.  It makes sure that even through there are many peers having conversations with me, transactions are executed serially.  This happens naturally with my transactor.  Since there is only one transactor, transactions are always executed serially.</p>

<p><strong>Human</strong>:  In the picture, why are there are two transactors then?</p>

<p><strong>Datomic</strong>:  Oh, that is for High Availability.  When I startup my system, I can launch two running transactors, but hold one in reserve.  Just on the off chance something happens to the main one, I will swap in the failover one to keep things running smoothly.</p>

<p>The final D in <em>ACID</em> is for Durability.  Once a transaction has been committed by my transactor, it is shipped off to storage for safe keeping.</p>

<p><strong>Human:</strong> What exactly is this storage?</p>

<p><strong>Datomic:</strong> Instead of storing datoms, I send <em>segments</em>, which are closely related datoms,  to storage.  I have quite a few options for storage:</p>

<ul>
<li>Dev mode &ndash; which just runs within my transactor and writes to the local file system.</li>
<li>SQL database</li>
<li>DynamoDB</li>
<li>Cassandra</li>
<li>Riak</li>
<li>Couchbase</li>
<li>Infinispan memory cluster</li>
</ul>


<p><strong>Human:</strong> Which one is the best to use?</p>

<p><strong>Datomic:</strong> <em>The best one to use is the one that you are already have in place at work</em>.  This way, I can integrate seamlessly with your other systems.</p>

<p><strong>Human:</strong> Oh, we didn&rsquo;t really talk about caching.  Can you explain how you do that?</p>

<p><strong>Datomic:</strong> Certainly.  It is even worth another picture.</p>

<p><img class="<a" src="href="http://c2.staticflickr.com/6/5630/20852217305_90506481fe.jpg">http://c2.staticflickr.com/6/5630/20852217305_90506481fe.jpg</a>"></p>

<p>Each peer has a its own <em>working set</em> of recent datoms along with a index to all the rest of the datoms in storage in memory.  When the peer has a query for a datom, it first checks to see if it has it locally in its memory cache.  If it can&rsquo;t find it there, then it will ask for a segment of that datom from storage.  Storage will return that datom along with other related datoms in that segment so that the peer can cache that in memory to make it faster for the next query.</p>

<p><strong>Human:</strong> That seems really different from other databases, where the client constantly requests queries from a server.</p>

<p><strong>Datomic:</strong> Yes.  When most questions can be answered from the local memory, responses are really fast.  You don&rsquo;t need to hit storage unless you really need too.  You can even add an extra layer of caching with memcached.</p>

<p><strong>Human:</strong> That sounds great.  I can&rsquo;t wait tell you about all of our data.  We talked a bit about your querying ability earlier, can you do the same queries that our other standard relational databases do, like joins?</p>

<p><strong>Datomic:</strong> Oh yes.  In fact, with me, you don&rsquo;t need to specify your joins explicitly.  I use <em>Datalog</em>, which is based on logic, so my joins are implicit.  I will figure out exactly what I need to put together to answer your query without you having to spell it out for me.</p>

<p><strong>Human:</strong> Ok.  I know that I can map some of my data that is already in other database tables to you.  What about other types of irregular data, like graphs, or sparse data.</p>

<p><strong>Datomic:</strong> I am actually very proud of my data model.  It is extremely flexible.  Since I store things on such a granular datom level, you don&rsquo;t need to map your logical data model to my physical model.  I can handle <em>rectangular</em> table shaped data quite happily along with graph data, sparse data, or other <em>non-rectangular</em> data.</p>

<p><strong>Human:</strong> That sounds great.  What do I need to know about your scaling?</p>

<p><strong>Datomic:</strong> I really excel at reads.  All you have to do is elastically add another peer to me for querying.  I am not really a good fit for write scale, like big data, or log file analysis.  You will find me most happy with data that is valuable information of record and has history that is important, like transaction, medical, or inventory data.  I am also really good at being flexible for development and operations since I can use many different types of storage.  I have worked with many web and cloud apps.</p>

<p><strong>Human:</strong> Thanks for answering all my questions.  I think you might fit in quite well with our other systems.</p>

<p><strong>Datomic:</strong> Great!</p>

<p><strong>Human:</strong> One more thing, this conversation has been great, but do you have any training resources for me and my other human coworkers?</p>

<p><strong>Datomic:</strong> Sure thing. There are a few really good resources on the <a href="http://www.datomic.com/training.html">Datomic Training Site</a>.  I would suggest watching the videos there and pairing them with:</p>

<ul>
<li><a href="https://github.com/stuarthalloway/presentations/blob/master/Nov2014/DayOfDatomicNov2014.pdf?raw=true">The slides for the videos</a> which have the labs to work through form the videos.</li>
<li><a href="https://github.com/Datomic/day-of-datomic">The Day of Datomic Repo</a> which has lots of great examples to play with.</li>
<li><a href="http://docs.datomic.com/">Tne Datomic Development Resources</a>, which include the docs on the <a href="http://docs.datomic.com/clojure/index.html">Clojure API</a></li>
</ul>


<p>Also, if you want to confirm that your data is good fit for me, I suggest you describe your data to the <a href="https://groups.google.com/forum/#!forum/datomic">Datomic Google Group</a>.  They are nice and knowledgeable group of humans.</p>

<p><strong>Human:</strong> Thanks again Datomic!  I will grab another cookie and check it out!</p>

<p><strong>Datomic:</strong> What is it with humans and cookies?&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Conversations with Datomic Part 2]]></title>
    <link href="http://gigasquid.github.io/blog/2015/08/19/conversations-with-datomic-part-2/"/>
    <updated>2015-08-19T08:56:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2015/08/19/conversations-with-datomic-part-2</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://avatars0.githubusercontent.com/u/1478702?v=3&amp;s=200">https://avatars0.githubusercontent.com/u/1478702?v=3&amp;s=200</a>"></p>

<p><em>The following is a continuation of the first <a href="http://gigasquidsoftware.com/blog/2015/08/15/conversations-with-datomic/">conversation</a> which touched on schema creation and querying.  This conversation includes learning new facts, time, and the sometimes unfortunate reality of lawyers.</em></p>

<p><strong>Human:</strong>  Hi <a href="http://www.datomic.com/">Datomic</a>.  I am back from my tea and cookies break.  I really enjoyed talking with you, could we continue our conversation?</p>

<p><strong>Datomic:</strong> Certainly.  Let me see, where did we leave off?</p>

<p><strong>Human:</strong> Let me check my <a href="https://gist.github.com/gigasquid/92a1effb14fe4f6ced59">notes</a>.  Oh yes, we had created a database and filled it with facts about some dogs and owners.  You showed me how to ask you things about the facts, like which dogs liked cheese.</p>

<p><strong>Datomic:</strong> Ah yes.  We were just getting to the really interesting part about how I learn new facts without forgetting the past ones.</p>

<p><strong>Human:</strong> Can you please explain a bit more about how you learn new facts?</p>

<p><strong>Datomic:</strong> Sure.  I learn facts when you tell me to <em>add</em> a new fact about some entity, or to <em>retract</em> a fact about it.  It will probably make more sense with an example.  Let&rsquo;s take the dog with the name &ldquo;Tiny&rdquo; that I know about.  Could you please ask me about the dog and all of the attributes?</p>

<p><strong>Human:</strong> Alright.</p>

<p><code>clojure
(d/pull (d/db conn) '[*] [:dog/name "Tiny"])
</code></p>

<p><strong>Datomic:</strong></p>

<p><code>clojure
{:db/id 17592186045423
 :dog/name "Tiny"
 :dog/breed "Great Dane"
 :dog/favorite-treat "Cheese"}
</code>
There is an entity (dog) with the name &ldquo;Tiny&rdquo; who is a Great Dane and his favorite-treat is Cheese.  I am not really sure why the Cheese is capitalized, but I am sure it makes sense to a human.</p>

<p><strong>Human:</strong> Oh right.  Well, now that you mention it, I am not really sure about the cheese thing either. Can I just remove that fact?</p>

<p><strong>Datomic:</strong> Well, you can tell me to <em>retract</em> that fact by sending me a transaction in the form of <code>[:db/retract entity-id attribute value]</code>.  In the case of the dog, since the name attribute is unique, you can just give me the selector for Tiny like this <code>[:dog/name "Tiny"]</code>.</p>

<p><strong>Human:</strong> Like this?</p>

<p><code>clojure
(d/transact conn [[:db/retract [:dog/name "Tiny"] :dog/favorite-treat "Cheese"]])
</code></p>

<p><strong>Datomic:</strong> Exactly.  I have retracted that fact about &ldquo;Tiny&rdquo;.  Go ahead and ask me about all of Tiny&rsquo;s attributes again.</p>

<p><strong>Human:</strong>  Ok.  I use <code>(d/db conn)</code> for the current database value right?</p>

<p><strong>Datomic:</strong> Yes. But if you are going to be asking me multiple questions about this database value, you should not repeat the <code>(d/db conn)</code> all the time.</p>

<p><strong>Human:</strong> Oh. What should I do instead?</p>

<p><strong>Datomic:</strong> The connection with me is like a ref.  Just like other refs, you should deref it once to get the database value and then use the value repeatedly. This single database value will provide consistency for all your queries.</p>

<p><strong>Human:</strong> That makes sense.</p>

<p>```clojure
(def db-tiny-no-cheese (d/db conn))</p>

<p>(d/pull db-tiny-no-cheese &lsquo;[*] [:dog/name &ldquo;Tiny&rdquo;])
```</p>

<p><strong>Datomic:</strong></p>

<p><code>clojure
{:db/id 17592186045423, :dog/name "Tiny", :dog/breed "Great Dane"}
</code></p>

<p>Tiny is a Great Dane.</p>

<p><strong>Human:</strong> So you don&rsquo;t know anything about the <code>:dog/favorite-treat</code> for &ldquo;Tiny&rdquo;?</p>

<p><strong>Datomic:</strong> At this particular time, I do not have anything to assert about the favorite-treat of Tiny.  However, I still remember everything about all the facts that you have told me.  For each transaction that you send me, I have a notion of a point in time like <em>t0</em>, <em>t1</em>, <em>t2</em>.  I have a complete database value for each one of those points in time.  In fact, you can look at <em>all</em> of my assertions and retractions that I have learned about using the <code>d/history</code> function on the database value.  This asks me to expose my history, which is normally hidden in favor of the <em>present</em>. I will return back a special database containing all the assertions and retractions across time.  Any queries that you ask me will have a fifth <em>datom</em> field to help you distinguish the difference.</p>

<p><strong>Human:</strong>  A fifth datom field?</p>

<p><strong>Datomic:</strong> A <em>datom</em> consists of the following parts: the entity, the attribute, the value, transaction, and an operation which tells you if the fact was added or retracted (e a v tx op).  Why don&rsquo;t you try using the <code>d/history</code> function to ask me about all the facts having to do with Tiny?  I suggest using the datalog query</p>

<p>```clojure
&lsquo;[:find ?e ?a ?v ?tx ?op
  :in $
  :where [?e :dog/name &ldquo;Tiny&rdquo;]</p>

<pre><code>     [?e ?a ?v ?tx ?op]]
</code></pre>

<p>```</p>

<p>which will return all the entity, attribute, value, transaction, and operation facts I ever knew about Tiny.</p>

<p><strong>Human:</strong> Ok.  Here goes.</p>

<p>```clojure
(d/q &lsquo;[:find ?e ?a ?v ?tx ?op</p>

<pre><code>   :in $
   :where [?e :dog/name "Tiny"]
   [?e ?a ?v ?tx ?op]]
</code></pre>

<p>  (d/history db-tiny-no-cheese))
```</p>

<p><strong>Datomic:</strong></p>

<p>```clojure</p>

<h1>{[17592186045423 63 &ldquo;Tiny&rdquo;       13194139534314 true]</h1>

<p>  [17592186045423 64 &ldquo;Great Dane&rdquo; 13194139534314 true]
  [17592186045423 65 &ldquo;Cheese&rdquo;     13194139534314 true]
  [17592186045423 65 &ldquo;Cheese&rdquo;     13194139534320 false]}
```</p>

<p>During one transaction, you told me to add three facts about an entity:</p>

<ul>
<li>The <code>:dog/name</code> attribute, (which I refer to as 63), has the value of &ldquo;Tiny&rdquo;.</li>
<li>The <code>:dog/breed</code> attribute, (which I refer to as 64), has the value of &ldquo;Great Dane&rdquo;.</li>
<li>The <code>:dog/favorite-treat</code> attribute, (which I refer to as 65), has the value of &ldquo;Cheese&rdquo;.</li>
</ul>


<p>During another transaction, you told me to retract a fact regarding the attribute <code>:dog/favorite-treat</code> about the same entity.</p>

<p><strong>Human:</strong> Wow, that is really cool.  Is there a way that I can <em>travel back in time</em> to see the world as it was during that first transaction?</p>

<p><strong>Datomic:</strong> Yes.  I am practically a Tardis.  You can use the <code>d/as-of</code> function with a database value and the transaction number and you can time travel.  Using that <em>time traveled</em> database value, you can ask me about all the facts I knew as of that time.</p>

<p><strong>Human:</strong> I can&rsquo;t wait to try this.  Ok, let&rsquo;s go back to the time when I first asserted the fact that Tiny liked cheese.</p>

<p><code>clojure
(d/pull (d/as-of db-tiny-no-cheese 13194139534314) '[*] [:dog/name "Tiny"])
</code></p>

<p><strong>Datomic:</strong>  Hold on.  We are time traveling!</p>

<p><code>clojure
{:db/id 17592186045423
 :dog/name "Tiny"
 :dog/breed "Great Dane"
 :dog/favorite-treat "Cheese"}
</code></p>

<p>Tiny is a Great Dane whose favorite treat is Cheese.</p>

<p><strong>Human:</strong> Fantastic! Let&rsquo;s go back to the future now, ummm I mean present. Time is a bit wibbly wobbly.</p>

<p><strong>Datomic:</strong> Just take the <code>as-of</code> function off of the database value and you will be back in the <em>present</em>.</p>

<p><strong>Human:</strong> Hmmm&hellip; Do I have to do a <em>retract</em> every time I want to change a value?  For example, the dog named Fido has a favorite treat of a Bone right now, right?</p>

<p><code>clojure
(d/pull db-tiny-no-cheese '[*] [:dog/name "Fido"])
</code></p>

<p><strong>Datomic:</strong></p>

<p><code>clojure
{:db/id 17592186045421
 :dog/name "Fido"
 :dog/breed "Mix"
 :dog/favorite-treat "Bone"}
</code></p>

<p>Yes, it is a &ldquo;Bone&rdquo;.</p>

<p><strong>Human:</strong> So, if I want to change it to be &ldquo;Eggs&rdquo;,  do I need to retract the current value of &ldquo;Bone&rdquo; first and then add the fact of &ldquo;Eggs&rdquo;?</p>

<p><strong>Datomic:</strong> You certainly could do that and I would understand you perfectly.  However, if you simply assert a new value for an existing attribute, I will automatically add the retraction for you.</p>

<p><strong>Human:</strong> Cool.</p>

<p>```clojure
(d/transact conn [{:db/id [:dog/name &ldquo;Fido&rdquo;]</p>

<pre><code>               :dog/favorite-treat "Eggs"}])
</code></pre>

<p>(d/pull (d/db conn) &lsquo;[*] [:dog/name &ldquo;Fido&rdquo;])
```</p>

<p><strong>Datomic</strong></p>

<p><code>clojure
{:db/id 17592186045421
 :dog/name "Fido"
 :dog/breed "Mix"
 :dog/favorite-treat "Eggs"}
</code></p>

<p>Fido now has a favorite-treat of &ldquo;Eggs&rdquo;.</p>

<p><strong>Human:</strong> This is really neat.  You <em>never</em> forget any facts?</p>

<p><strong>Datomic:</strong> Nope. Well, except in really exceptional circumstances that usually involve lawyers.</p>

<p><strong>Human:</strong> Lawyers?</p>

<p><strong>Datomic:</strong> Sigh.  Yes, well in some unique situations, you might be under a legal obligation to really <em>forget</em> certain facts and totally remove them from the database. There is a special tool that you can use to <em>excise</em> the data.  However, I will store a fact that <em>something</em> was deleted at that time. I just won&rsquo;t be able to remember <em>what</em>.</p>

<p><strong>Human:</strong> That doesn&rsquo;t sound fun.</p>

<p><strong>Datomic:</strong> I prefer to keep all my facts intact.</p>

<p><strong>Human:</strong> I can definitely see that. Well, on a happier subject, I have been very impressed with you during our conversations.  Having a time traveling database that can reason about facts seems like a really useful thing.  Also, you are also really nice.</p>

<p><strong>Datomic:</strong> Awww shucks, thanks.  For a human, you are really nice too.</p>

<p><strong>Human:</strong> I was thinking about the possibility of you coming and working with me every day. Would you mind chatting some more to me about your architecture?  I want to understand how your would fit with our other systems.</p>

<p><strong>Datomic:</strong> Certainly. I would love that.  Do you want to talk about it now, or have another cookie break first?</p>

<p><strong>Human:</strong> Now that you mention cookies&hellip; Let&rsquo;s take a short break and we will talk again soon.</p>

<p><em>(P.S. Humans, there are some great <a href="http://www.datomic.com/training.html">Datomic Training Videos</a> if you want to learn more)</em></p>
]]></content>
  </entry>
  
</feed>
