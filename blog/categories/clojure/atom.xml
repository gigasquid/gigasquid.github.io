<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Clojure | Squid's Blog]]></title>
  <link href="http://gigasquid.github.io/blog/categories/clojure/atom.xml" rel="self"/>
  <link href="http://gigasquid.github.io/"/>
  <updated>2023-05-03T09:22:53-04:00</updated>
  <id>http://gigasquid.github.io/</id>
  <author>
    <name><![CDATA[Carin Meier]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Vector Symbolic Architectures in Clojure]]></title>
    <link href="http://gigasquid.github.io/blog/2022/12/31/vector-symbolic-architectures-in-clojure/"/>
    <updated>2022-12-31T15:41:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2022/12/31/vector-symbolic-architectures-in-clojure</id>
    <content type="html"><![CDATA[<p><img src="https://live.staticflickr.com/65535/52596142860_c4cf8642b0_z.jpg" alt="" /></p>

<p><em>generated with Stable Diffusion</em></p>

<p>Before diving into the details of what Vector Symbolic Architectures are and what it means to implement Clojure data structures in them, I&rsquo;d like to start with some of my motivation in this space.</p>

<h2>Small AI for More Personal Enjoyment</h2>

<p>Over the last few years, I&rsquo;ve spent time learning, exploring, and contributing to open source deep learning. It continues to amaze me with its rapid movement and achievements at scale. However, the scale is really too big and too slow for me to enjoy it anymore.</p>

<p>Between work and family, I don&rsquo;t have a lot of free time. When I do get a few precious hours to do some coding <em>just for me</em>, I want it it to be small enough for me to fire up and play with it in a REPL on my local laptop and get a result back in under two minutes.</p>

<p>I also believe that the current state of AI is not likely to produce any more  meaningful <em>revolutionary</em> innovations in the current mainstream deep learning space. This is not to say that there won&rsquo;t be advances. Just as commercial airlines transformed the original first flight, I&rsquo;m sure we are going to continue to see the transformation of society with current big models at scale - I just think the next leap forward is going to come from somewhere else. And that somewhere else is going to be <em>small</em> AI.</p>

<h2>Vector Symbolic Architures aka Hyperdimensional Computing</h2>

<p>Although I&rsquo;m  talking about small AI,  VSA or Hyperdimensional computing is based on really big vectors - like 1,000,000 dimensions. The beauty and simplicity in it is that everything is a hypervector - symbols, maps, lists. Through the <a href="http://rctn.org/vs265/kanerva09-hyperdimensional.pdf">blessing of high dimensionality</a>, any random hypervector is mathematically guaranteed to be orthogonal to any other one. This all enables some cool things:</p>

<ul>
<li>Random hypervectors can be used to represent symbols (like numbers, strings, keywords, etc..)</li>
<li>We can use an algebra to operate on hypervectors: <em>bundling</em> and <em>binding</em> operations create new hypervectors that are compositions of each other and can store and retrieve key value pairs. These operations furthermore are <em>fuzzy</em> due to the nature of working with vectors. In the following code examples, I will be using the concrete model of <a href="https://redwood.berkeley.edu/wp-content/uploads/2021/08/Module2_VSA_models_slides.pdf">MAP (Multiply, Add, Permute)</a> by <a href="https://www.rossgayler.com/">R. Gayler</a>.</li>
<li>We can represent Clojure data structures such as maps and vectors in them and perform operations such as <code>get</code> with probabilistic outcomes.</li>
<li>Everything is a hypervector! I mean you have a keyword that is a symbol that is a hypervector, then you bundle that with other keywords to be a map. The result is a single hypervector. You then create a sequence structure and add some more in. The result is a single hypervector. The simplicity in the algebra and form of the VSA is beautiful - not unlike LISP itself. Actually, <a href="https://redwood.berkeley.edu/wp-content/uploads/2021/08/Neubert2019_Article_AnIntroductionToHyperdimension.pdf">P. Kanerva thought that a LISP could be made from it</a>. In my exploration, I only got as far as making some Clojure data structures, but I&rsquo;m sure it&rsquo;s possible.</li>
</ul>


<h2>Start with an Intro and a Paper</h2>

<p>A good place to start with Vector Symbolic Architectures is actually the paper referenced above - <a href="https://redwood.berkeley.edu/wp-content/uploads/2021/08/Neubert2019_Article_AnIntroductionToHyperdimension.pdf">An Introduction to Hyperdimensional Computing for Robots</a>. In general, I find the practice of taking a paper and then trying to implement it a great way to learn.</p>

<p>To work with VSAs in Clojure, I needed a high performing Clojure library with tensors and data types. I reached for <a href="https://github.com/techascent/tech.datatype">https://github.com/techascent/tech.datatype</a>. It could handle a million dimensions pretty easily on my laptop.</p>

<p>To create a new hypervector - simply chose random values between -1 and 1. This gives us a direction in space which is enough.</p>

<pre><code class="clojure">;; Uses Gaylor Method for HDV Operations

(def size 1e6)  ; big enough for the "Blessing of Dimensionality"

(defn binary-rand
  "Choose a random binary magnitude for the vector +1 or -1"
  []
  (if (&gt; (rand) 0.5) -1 1))


(defn hdv
  "Create a random hyperdimensional vector of default size"
  []
  (dtt/-&gt;tensor (repeatedly size #(binary-rand)) :datatype :int8))
</code></pre>

<p>The only main operations to create key value pairs is addition and matrix multiplication.</p>

<p>Adding two hyperdimensional vectors, (hdvs), together is calling bundling. Note we clip the values to 1 or -1. At high dimensions, only the direction really matters not the magnitude.</p>

<pre><code>(defn clip
  "Clips the hyperdimensional vector magnitude to 1 or -1.
   We can discard these because of the nature of the large vectors
   that the mangitudes do not matter"
  [v]
  (-&gt; v
      (dtype-fn/min 1)
      (dtype-fn/max -1)))


(defn bundle
  "Adds two hyperdimensional vectors together into a single bundle"
  [v1 v2]
  (-&gt; (bundle-op v1 v2)
      (clip)))
</code></pre>

<p>We can assign key values using <code>bind</code> which is matrix multiplication.</p>

<pre><code class="clojure">(defn bind
  "Binds two HDVs using the multiplication operator. This binding is akin to assigning a symbol to a value. "
  [v1 v2]
  (dtype-fn/* v1 v2))
</code></pre>

<p>One cool thing is that the binding of a key value pair is also the inverse of itself. So to unbind is just to bind again.</p>

<p>The final thing we need is a cleanup memory. The purpose of this is to store the hdv somewhere without any noise. As the hdv gets bundled with other operations there is noise associated with it. It helps to use the cleaned up version by comparing the result to the memory version for future operations. For Clojure, this can be a simple atom.</p>

<p>Following along the example in the paper, we reset the cleanup memory and add some symbols.</p>

<pre><code class="clojure">(vb/reset-hdv-mem!)

(vb/add-hdv! :name)
(vb/add-hdv! "Alice")
(vb/add-hdv! :yob)
(vb/add-hdv! 1980)
(vb/add-hdv! :high-score)
(vb/add-hdv! 1000)
</code></pre>

<p>Next we create the key value map with combinations of <code>bind</code> and <code>bundle</code>.</p>

<pre><code class="clojure">(def H
  (-&gt; (vb/bind (vb/get-hdv :name) (vb/get-hdv "Alice"))
      (vb/bundle
        (vb/bind (vb/get-hdv :yob) (vb/get-hdv 1980)))
      (vb/bundle
        (vb/bind (vb/get-hdv :high-score) (vb/get-hdv 1000)))))
</code></pre>

<p>So <code>H</code> is just one hypervector as a result of this. We can then query it. <code>unbind-get</code> is using the <code>bind</code> operation as inverse. So if we want to query for the <code>:name</code> value, we get the <code>:name</code> hdv from memory and do the <code>bind</code> operation on the <code>H</code> data structure which is the inverse.</p>

<pre><code class="clojure">(vb/unbind-get H :name)


;; ["Alice" #tech.v3.tensor&lt;int8&gt;[1000000]
;;  [-1 1 1 ... 1 -1 -1]]
</code></pre>

<p>We can find other values like <code>:high-score</code>.</p>

<pre><code class="clojure">(vb/unbind-get H :high-score)


;;  [1000 #tech.v3.tensor&lt;int8&gt;[1000000]
;; [-1 -1 1 ... -1 1 1]]
</code></pre>

<p>Or go the other way and look for <code>Alice</code>.</p>

<pre><code class="clojure">(vb/unbind-get H "Alice")


;; [:name #tech.v3.tensor&lt;int8&gt;[1000000]
;; [-1 1 -1 ... -1 -1 -1]]
</code></pre>

<p>Now that we have the fundamentals from the paper, we can try to implement some Clojure data structures.</p>

<h2>Clojure Data Structures in VSAs</h2>

<p>First things first, let&rsquo;s clear our cleanup memory.</p>

<pre><code class="clojure">(vb/reset-hdv-mem!)
</code></pre>

<p>Let&rsquo;s start off with a map, (keeping to non-nested versions to keep things simple).</p>

<pre><code class="clojure">(def our-first-vsa-map (vd/clj-&gt;vsa {:x 1 :y 2}))
</code></pre>

<p>The result is a 1,000,000 dimension hypervector - but remember all the parts are also hypervectors as well. Let&rsquo;s take a look at what is in the cleanup memory so far.</p>

<pre><code class="clojure">
@vb/cleanup-mem


;; {:x #tech.v3.tensor&lt;int8&gt;[1000000][1 -1 1 ... 1 -1 -1],
;;  1 #tech.v3.tensor&lt;int8&gt;[1000000][1 1 -1 ... -1 -1 -1],
;;  :y #tech.v3.tensor&lt;int8&gt;[1000000][1 1 -1 ... 1 1 1],
;;  2 #tech.v3.tensor&lt;int8&gt;[1000000][-1 -1 -1 ... -1 -1 1]}
</code></pre>

<p>We can write a <code>vsa-get</code> function that takes the composite hypervector of the map and get the value from it by finding the closest match with cosine similarity to the cleanup memory.</p>

<pre><code class="clojure">(vd/vsa-get our-first-vsa-map :x)


;; =&gt;  [1 #tech.v3.tensor&lt;int8&gt;[1000000][1 1 -1 ... -1 -1 -1]
</code></pre>

<p>In the example above, the symbolic value is the first item in the vector, in this case the number 1, and the actual hypervector is the second value.</p>

<p>We can add onto the map with a new key value pair.</p>

<pre><code class="clojure">(def our-second-vsa-map (vd/vsa-assoc our-first-vsa-map :z 3))

(vd/vsa-get our-second-vsa-map :z)


;; =&gt;  [3 #tech.v3.tensor&lt;int8&gt;[1000000][1 -1 1 ... -1 -1 1]]
</code></pre>

<p>We can represent Clojure vectors as VSA data structures as well by using the permute (or rotate) and adding them like a stack.</p>

<pre><code class="clojure">(def our-first-vsa-vector-of-maps (vd/clj-&gt;vsa [{:x 1} {:x 2 :y 3}]))


;; We can get the value of x in the 2nd map by
(vd/vsa-get our-first-vsa-vector-of-maps :x {:idx 1})


;; [2 #tech.v3.tensor&lt;int8&gt;[1000000][-1 1 1 ... 1 -1 1]]

;; Or the first map
(vd/vsa-get our-first-vsa-vector-of-maps :x {:idx 0})


;; =&gt;  [1 #tech.v3.tensor&lt;int8&gt;[1000000][-1 -1 1 ... 1 1 1]]
</code></pre>

<p>We can also add onto the Clojure vector with a conj.</p>

<pre><code class="clojure">(def our-second-vsa-vector-of-maps
  (vd/vsa-conj our-first-vsa-vector-of-maps (vd/clj-&gt;vsa {:z 5})))


(vd/vsa-get our-second-vsa-vector-of-maps :z {:idx 2})


;; =&gt;  [5 #tech.v3.tensor&lt;int8&gt;[1000000][-1 1 1 ... -1 -1 -1]]
</code></pre>

<p>What is really cool about this is that we have built in fuzziness or similarity matching. For example, with this map, we have more than one possibility of matching.</p>

<pre><code class="clojure">(def vsa-simple-map (vd/clj-&gt;vsa {:x 1 :y 1 :z 3}))
</code></pre>

<p>We can see all the possible matches and scores</p>

<pre><code class="clojure">(vd/vsa-get vsa-simple-map :x {:threshold -1 :verbose? true})


;; =&gt;  [{1 #tech.v3.tensor&lt;int8&gt;[1000000]
;;      [1 -1 1 ... -1 -1 -1], :dot 125165.0, :cos-sim 0.1582533568106879}  {:x #tech.v3.tensor&lt;int8&gt;[1000000]
;;      [1 -1 -1 ... -1 -1 1], :dot 2493.0, :cos-sim 0.0031520442498225933} {:z #tech.v3.tensor&lt;int8&gt;[1000000]
;;      [-1 -1 1 ... 1 1 -1], :dot 439.0, :cos-sim 5.550531190020531E-4}    {3 #tech.v3.tensor&lt;int8&gt;[1000000]
;;      [-1 -1 1 ... -1 -1 1], :dot -443.0, :cos-sim -5.601105506102723E-4}  {:y #tech.v3.tensor&lt;int8&gt;[1000000]
;;      [-1 -1 1 ... 1 1 1], :dot -751.0, :cos-sim -9.495327844431478E-4}]
</code></pre>

<p>This opens up the possibility of defining compound symbolic values and doing fuzzy matching. For example with colors.</p>

<pre><code class="clojure">(vb/reset-hdv-mem!)

(def primary-color-vsa-map (vd/clj-&gt;vsa {:x :red :y :yellow :z :blue}))
</code></pre>

<p>Let&rsquo;s add a new compound value to the cleanup memory that is green based on yellow and blue.</p>

<pre><code class="clojure">(vb/add-hdv! :green (vb/bundle
                      (vb/get-hdv :yellow)
                      (vb/get-hdv :blue)))
</code></pre>

<p>Now we can query the hdv color map for things that are close to green.</p>

<pre><code class="clojure">(vd/vsa-get primary-color-vsa-map :green {:threshold 0.1})


;; =&gt;  [{:z #tech.v3.tensor&lt;int8&gt;[1000000][1 -1 1 ... -1 1 1]}
;;     {:y #tech.v3.tensor&lt;int8&gt;[1000000] [-1 1 1 ... 1 1 1]}]
</code></pre>

<p>We can also define an <code>inspect</code> function for a hdv by comparing the similarity of all the values of the cleanup memory in it.</p>

<pre><code class="clojure">(vd/vsa-inspect primary-color-vsa-map)


;; Note that it includes green in it since it is a compound value
;; =&gt;  #{:y :yellow :green :z :red :blue :x}

(vd/vsa-inspect (vd/clj-&gt;vsa {:x :red}))


;; =&gt;  #{:red :x}
</code></pre>

<p>Finally, we can implement clojure <code>map</code> and <code>filter</code> functions on the vector data structures that can also include fuzziness.</p>

<pre><code class="clojure">(def color-vsa-vector-map (vd/clj-&gt;vsa [{:x :yellow} {:x :green} {:z :red}]))


(vd/vsa-map #(-&gt;&gt; (vd/vsa-get % :yellow {:threshold 0.01})
                  (mapv ffirst))
            color-vsa-vector-map)


;; =&gt;  ([:x] [:x] [])


(-&gt;&gt; color-vsa-vector-map
     (vd/vsa-filter #(vd/vsa-get % :yellow {:threshold 0.01}))
     count)


;; =&gt;  2
</code></pre>

<h2>Wrap Up</h2>

<p>VSAs and hyperdimensional computing seem like a natural fit for LISP and Clojure. I&rsquo;ve only scratched the surface here in how the two can fit together. I hope that more people are inspired to look into it and <em>small AI with big dimensions</em>.</p>

<p>Full code and examples here <a href="https://github.com/gigasquid/vsa-clj">https://github.com/gigasquid/vsa-clj</a>.</p>

<p><em>Special thanks to Ross Gayler in helping me to implement VSAs and understanding their coolness.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clojure Interop With Python NLP Libraries]]></title>
    <link href="http://gigasquid.github.io/blog/2020/01/24/clojure-interop-with-python-nlp-libraries/"/>
    <updated>2020-01-24T15:34:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2020/01/24/clojure-interop-with-python-nlp-libraries</id>
    <content type="html"><![CDATA[<p><img src="http:////live.staticflickr.com/65535/49435394578_400fdf1c7f_c.jpg" alt="clojure-python" /></p>

<p>In this edition of the blog series of Clojure/Python interop with <a href="https://github.com/cnuernber/libpython-clj">libpython-clj</a>, we&rsquo;ll be taking a look at two popular Python NLP libraries: <a href="https://www.nltk.org/">NLTK</a> and <a href="https://spacy.io/">SpaCy</a>.</p>

<h2>NLTK - Natural Language Toolkit</h2>

<p>I was taking requests for doing examples of python-clojure interop libraries on twitter the other day, and by <em>far</em> NLTK was the most requested library. After looking into it, I can see why. It&rsquo;s the most popular natural language processing library in Python and you will see it everywhere there is text someone is touching.</p>

<h3>Installation</h3>

<p>To use the NLTK toolkit you will need to install it. I use <code>sudo pip3 install nltk</code>, but libpython-clj now supports virtual environments with this <a href="https://github.com/cnuernber/libpython-clj/pull/53">PR</a>, so feel free to use whatever is best for you.</p>

<h3>Features</h3>

<p>We&rsquo;ll take a quick tour of the features of NLTK following along initially with the <a href="https://www.nltk.org/book/ch01.html">nltk official book</a> and then moving onto this more data task centered <a href="https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk">tutorial</a>.</p>

<p>First, we need to require all of our things as usual:</p>

<pre><code class="clojure">(ns gigasquid.nltk
  (:require [libpython-clj.require :refer [require-python]]
            [libpython-clj.python :as py :refer [py. py.. py.-]]))
(require-python '([nltk :as nltk]))
</code></pre>

<h4>Downloading packages</h4>

<p>There are all sorts of packages available to download from NLTK. To start out and tour the library, I would go with a small one that has basic data for the nltk book tutorial.</p>

<pre><code class="clojure"> (nltk/download "book")
  (require-python '([nltk.book :as book]))
</code></pre>

<p>There are all other sorts of downloads as well, such as <code>(nltk/download "popular")</code> for most used ones. You can also download <code>"all"</code>, but beware that it is big.</p>

<p>You can check out some of the texts it downloaded with:</p>

<pre><code class="clojure">  (book/texts)

  ;;; prints out in repl
  ;; text1: Moby Dick by Herman Melville 1851
  ;; text2: Sense and Sensibility by Jane Austen 1811
  ;; text3: The Book of Genesis
  ;; text4: Inaugural Address Corpus
  ;; text5: Chat Corpus
  ;; text6: Monty Python and the Holy Grail
  ;; text7: Wall Street Journal
  ;; text8: Personals Corpus
  ;; text9: The Man Who Was Thursday by G . K . Chesterton 1908

  book/text1 ;=&gt;  &lt;Text: Moby Dick by Herman Melville 1851&gt;
  book/text2 ;=&gt;  &lt;Text: Sense and Sensibility by Jane Austen 1811&gt;
</code></pre>

<p>  You can do fun things like see how many tokens are in a text</p>

<pre><code class="clojure">  (count (py.- book/text3 tokens))  ;=&gt; 44764
</code></pre>

<p>  Or even see the lexical diversity, which is a measure of the richness of the text by looking at the unique set of word tokens against the total tokens.</p>

<pre><code class="clojure">  (defn lexical-diversity [text]
    (let [tokens (py.- text tokens)]
      (/ (-&gt; tokens set count)
         (* 1.0 (count tokens)))))

  (lexical-diversity book/text3) ;=&gt; 0.06230453042623537
  (lexical-diversity book/text5) ;=&gt; 0.13477005109975562
</code></pre>

<p> This of course is all very interesting but I prefer to look at some more practical tasks, so we are going to look at some sentence tokenization.</p>

<h4>Sentence Tokenization</h4>

<p> Text can be broken up into individual word tokens or sentence tokens. Let&rsquo;s start off first with the token package</p>

<pre><code class="clojure">(require-python '([nltk.tokenize :as tokenize]))
(def text "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn't eat cardboard")
</code></pre>

<p>To tokenize sentences, you take the text and use <code>tokenize/sent_tokenize</code>.</p>

<pre><code class="clojure"> (def text "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn't eat cardboard")
 (def tokenized-sent (tokenize/sent_tokenize text))
 tokenized-sent
 ;;=&gt; ['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', "You shouldn't eat cardboard"]
</code></pre>

<p>Likewise, to tokenize words, you use <code>tokenize/word_tokenize</code>:</p>

<pre><code class="clojure"> (def text "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn't eat cardboard")
 (def tokenized-sent (tokenize/sent_tokenize text))
 tokenized-sent
 ;;=&gt; ['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', "You shouldn't eat cardboard"]


 (def tokenized-word (tokenize/word_tokenize text))
 tokenized-word
  ;;=&gt; ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', "n't", 'eat', 'cardboard']
</code></pre>

<h4>Frequency Distribution</h4>

<p> You can also look at the frequency distribution of the words with using the probability package.</p>

<pre><code class="clojure"> (require-python '([nltk.probability :as probability]))

 (def fdist (probability/FreqDist tokenized-word))
 fdist ;=&gt; &lt;FreqDist with 25 samples and 30 outcomes&gt;

 (py. fdist most_common)
  ;=&gt; [('is', 3), (',', 2), ('The', 2), ('.', 2), ('Hello', 1), ('Mr.', 1), ('Smith', 1), ('how', 1), ('are', 1), ('you', 1), ('doing', 1), ('today', 1), ('?', 1), ('weather', 1), ('great', 1), ('and', 1), ('city', 1), ('awesome', 1), ('sky', 1), ('pinkish-blue', 1), ('You', 1), ('should', 1), ("n't", 1), ('eat', 1), ('cardboard', 1)]
</code></pre>

<h4>Stop Words</h4>

<p>Stop words are considered noise in text and there are ways to use the library to remove them using the <code>nltk.corpus</code>.</p>

<pre><code class="clojure">(def stop-words (into #{} (py. corpus/stopwords words "english")))
 stop-words
  ;=&gt; #{"d" "itself" "more" "didn't" "ain" "won" "hers"....}
</code></pre>

<p>Now that we have a collection of the stop words, we can filter them out of our text in the normal way in Clojure.</p>

<pre><code class="clojure">(def filtered-sent (-&gt;&gt; tokenized-sent
                         (map tokenize/word_tokenize)
                         (map #(remove stop-words %))))
 filtered-sent
 ;; (("Hello" "Mr." "Smith" "," "today" "?")
 ;; ("The" "weather" "great" "," "city" "awesome" ".")
 ;; ("The" "sky" "pinkish-blue" ".")
 ;; ("You" "n't" "eat" "cardboard"))
</code></pre>

<h4>Lexion Normalization and Lemmatization</h4>

<p>Stemming and Lemmatization allow ways for the text to be reduced to base words and normalized.
For example, the word <code>flying</code> has a stemmed word of <code>fli</code> and a lemma of <code>fly</code>.</p>

<pre><code class="clojure">(require-python '([nltk.stem :as stem]))
(require-python '([nltk.stem.wordnet :as wordnet]))

(let [lem (wordnet/WordNetLemmatizer)
       stem (stem/PorterStemmer)
       word "flying"]
   {:lemmatized-word (py. lem lemmatize word "v")
    :stemmed-word (py. stem stem word)})
 ;=&gt; {:lemmatized-word "fly", :stemmed-word "fli"}
</code></pre>

<h4>POS Tagging</h4>

<p>It also has support for Part-of-Speech (POS) Tagging. A quick example of that is:</p>

<pre><code class="clojure">(let [sent "Albert Einstein was born in Ulm, Germany in 1879."
       tokens (nltk/word_tokenize sent)]
   {:tokens tokens
    :pos-tag (nltk/pos_tag tokens)})
 ;; {:tokens
 ;; ['Albert', 'Einstein', 'was', 'born', 'in', 'Ulm', ',', 'Germany', 'in', '1879', '.'],
 ;; :pos-tag
 ;; [('Albert', 'NNP'), ('Einstein', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Ulm', 'NNP'), (',', ','), ('Germany', 'NNP'), ('in', 'IN'), ('1879', 'CD'), ('.', '.')]}
</code></pre>

<p>Phew! That&rsquo;s a brief overview of what NLTK can do, now what about the other library SpaCy?</p>

<h2>SpaCy</h2>

<p><a href="https://spacy.io/usage/spacy-101#whats-spacy">SpaCy</a> is the main competitor to NLTK. It has a more opinionated library which is more object oriented than NLTK which mainly processes text. It has better performance for tokenization and POS tagging and has support for word vectors, which NLTK does not.</p>

<p>Let&rsquo;s dive in a take a look at it.</p>

<h3>Installation</h3>

<p>To install spaCy, you will need to do:</p>

<ul>
<li><code>pip3 install spacy</code></li>
<li><code>python3 -m spacy download en_core_web_sm</code> to load up the small language model</li>
</ul>


<p>We&rsquo;ll be following along this <a href="https://spacy.io/usage/spacy-101#annotat">tutorial</a></p>

<p>We will, of course, need to load up the library</p>

<pre><code class="clojure">(require-python '([spacy :as spacy]))
</code></pre>

<p>and its language model:</p>

<pre><code class="clojure">(def nlp (spacy/load "en_core_web_sm"))
</code></pre>

<h4>Linguistic Annotations</h4>

<p>There are many linguistic annotations that are available, from POS, lemmas, and more:</p>

<pre><code class="clojure">(let [doc (nlp "Apple is looking at buying U.K. startup for $1 billion")]
  (map (fn [token]
         [(py.- token text) (py.- token pos_) (py.- token dep_)])
       doc))
;; (["Apple" "PROPN" "nsubj"]
;;  ["is" "AUX" "aux"]
;;  ["looking" "VERB" "ROOT"]
;;  ["at" "ADP" "prep"]
;;  ["buying" "VERB" "pcomp"]
;;  ["U.K." "PROPN" "compound"]
;;  ["startup" "NOUN" "dobj"]
;;  ["for" "ADP" "prep"]
;;  ["$" "SYM" "quantmod"]
;;  ["1" "NUM" "compound"]
;;  ["billion" "NUM" "pobj"])
</code></pre>

<p>Here are some more:</p>

<pre><code class="clojure">(let [doc (nlp "Apple is looking at buying U.K. startup for $1 billion")]
  (map (fn [token]
         {:text (py.- token text)
          :lemma (py.- token lemma_)
          :pos (py.- token pos_)
          :tag (py.- token tag_)
          :dep (py.- token dep_)
          :shape (py.- token shape_)
          :alpha (py.- token is_alpha)
          :is_stop (py.- token is_stop)} )
       doc))

;; ({:text "Apple",
;;   :lemma "Apple",
;;   :pos "PROPN",
;;   :tag "NNP",
;;   :dep "nsubj",
;;   :shape "Xxxxx",
;;   :alpha true,
;;   :is_stop false}
;;  {:text "is",
;;   :lemma "be",
;;   :pos "AUX",
;;   :tag "VBZ",
;;   :dep "aux",
;;   :shape "xx",
;;   :alpha true,
;;   :is_stop true}
;;  ...
</code></pre>

<h3>Named Entities</h3>

<p>It also handles named entities in the same fashion.</p>

<pre><code class="clojure">(let [doc (nlp "Apple is looking at buying U.K. startup for $1 billion")]
  (map (fn [ent]
         {:text (py.- ent text)
          :start-char (py.- ent start_char)
          :end-char (py.- ent end_char)
          :label (py.- ent label_)} )
       (py.- doc ents)))

;; ({:text "Apple", :start-char 0, :end-char 5, :label "ORG"}
;;  {:text "U.K.", :start-char 27, :end-char 31, :label "GPE"}
;;  {:text "$1 billion", :start-char 44, :end-char 54, :label "MONEY"})
</code></pre>

<p>As you can see, it can handle pretty much the same things as NLTK. But let&rsquo;s take a look at what it can do that NLTK and that is with word vectors.</p>

<h4>Word Vectors</h4>

<p>In order to use word vectors, you will have to load up a medium or large size data model because the small ones don&rsquo;t ship with word vectors. You can do that at the command line with:</p>

<pre><code>python3 -m spacy download en_core_web_md
</code></pre>

<p>You will need to restart your repl and then load it with:</p>

<pre><code class="clojure">(require-python '([spacy :as spacy]))
(def nlp (spacy/load "en_core_web_md"))
</code></pre>

<p>Now you can see cool word vector stuff!</p>

<pre><code class="clojure">(let [tokens (nlp "dog cat banana afskfsd")]
  (map (fn [token]
         {:text (py.- token text)
          :has-vector (py.- token has_vector)
          :vector_norm (py.- token vector_norm)
          :is_oov (py.- token is_oov)} )
       tokens))

;; ({:text "dog",
;;   :has-vector true,
;;   :vector_norm 7.033673286437988,
;;   :is_oov false}
;;  {:text "cat",
;;   :has-vector true,
;;   :vector_norm 6.680818557739258,
;;   :is_oov false}
;;  {:text "banana",
;;   :has-vector true,
;;   :vector_norm 6.700014114379883,
;;   :is_oov false}
;;  {:text "afskfsd", :has-vector false, :vector_norm 0.0, :is_oov true})
</code></pre>

<p>And find similarity between different words.</p>

<pre><code class="clojure">(let [tokens (nlp "dog cat banana")]
  (for [token1 tokens
        token2 tokens]
    {:token1 (py.- token1 text)
     :token2 (py.- token2 text)
     :similarity (py. token1 similarity token2)}))

;; ({:token1 "dog", :token2 "dog", :similarity 1.0}
;;  {:token1 "dog", :token2 "cat", :similarity 0.8016854524612427}
;;  {:token1 "dog", :token2 "banana", :similarity 0.2432764321565628}
;;  {:token1 "cat", :token2 "dog", :similarity 0.8016854524612427}
;;  {:token1 "cat", :token2 "cat", :similarity 1.0}
;;  {:token1 "cat", :token2 "banana", :similarity 0.28154364228248596}
;;  {:token1 "banana", :token2 "dog", :similarity 0.2432764321565628}
;;  {:token1 "banana", :token2 "cat", :similarity 0.28154364228248596}
;;  {:token1 "banana", :token2 "banana", :similarity 1.0})
</code></pre>

<h2>Wrap up</h2>

<p>We&rsquo;ve seen a grand tour of the two most popular natural language python libraries that you can now use through Clojure interop!</p>

<p>I hope you&rsquo;ve enjoyed it and if you are interested in exploring yourself, the code examples are <a href="https://github.com/gigasquid/libpython-clj-examples">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Parens for Pyplot]]></title>
    <link href="http://gigasquid.github.io/blog/2020/01/18/parens-for-pyplot/"/>
    <updated>2020-01-18T15:39:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2020/01/18/parens-for-pyplot</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/cnuernber/libpython-clj">libpython-clj</a> has opened the door for Clojure to directly interop with Python libraries. That means we can take just about any Python library and directly use it in our Clojure REPL. But what about <a href="https://matplotlib.org/">matplotlib</a>?</p>

<p>Matplotlib.pyplot is a standard fixture in most tutorials and python data science code. How do we interop with a python graphics library?</p>

<h2>How do you interop?</h2>

<p>It turns out that matplotlib has a headless mode where we can export the graphics and then display it using any method that we would normally use to display a .png file. In my case, I made a quick macro for it using the shell <code>open</code>. I&rsquo;m sure that someone out that could improve upon it, (and maybe even make it a cool utility lib), but it suits what I&rsquo;m doing so far:</p>

<pre><code class="clojure">ns gigasquid.plot
(:require [libpython-clj.require :refer [require-python]]
[libpython-clj.python :as py :refer [py. py.. py.-]]
[clojure.java.shell :as sh])


;;; This uses the headless version of matplotlib to generate a graph then copy it to the JVM
;; where we can then print it

;;;; have to set the headless mode before requiring pyplot
(def mplt (py/import-module "matplotlib"))
(py. mplt "use" "Agg")

(require-python 'matplotlib.pyplot)
(require-python 'matplotlib.backends.backend_agg)
(require-python 'numpy)


(defmacro with-show
  "Takes forms with mathplotlib.pyplot to then show locally"
  [&amp; body]
  `(let [_# (matplotlib.pyplot/clf)
         fig# (matplotlib.pyplot/figure)
         agg-canvas# (matplotlib.backends.backend_agg/FigureCanvasAgg fig#)]
     ~(cons 'do body)
     (py. agg-canvas# "draw")
     (matplotlib.pyplot/savefig "temp.png")
     (sh/sh "open" "temp.png")))
</code></pre>

<h2>Parens for Pyplot!</h2>

<p>Now that we have our wrapper let&rsquo;s take it for a spin. We&rsquo;ll be following along more or less this <a href="http://cs231n.github.io/python-numpy-tutorial/#matplotlib-plotting">tutorial for numpy plotting</a></p>

<p>For setup you will need the following installed in your python environment:</p>

<ul>
<li>numpy</li>
<li>matplotlib</li>
<li>pillow</li>
</ul>


<p>We are also going to use the latest and greatest syntax from libpython-clj so you are going to need to install the snapshot version locally until the next version goes out:</p>

<ul>
<li><code>git clone git@github.com:cnuernber/libpython-clj.git</code></li>
<li><code>cd cd libpython-clj</code></li>
<li><code>lein install</code></li>
</ul>


<p>After that is all setup we can require the libs we need in clojure.</p>

<pre><code class="clojure">(ns gigasquid.numpy-plot
  (:require [libpython-clj.require :refer [require-python]]
            [libpython-clj.python :as py :refer [py. py.. py.-]]
            [gigasquid.plot :as plot]))
</code></pre>

<p>The <code>plot</code> namespace contains the macro for <code>with-show</code> above. The <code>py.</code> and others is the new and improved syntax for interop.</p>

<h3>Simple Sin and Cos</h3>

<p>Let&rsquo;s start off with a simple sine and cosine functions. This code will create a <code>x</code> numpy vector of a range from 0 to <code>3 * pi</code> in 0.1 increments and then create <code>y</code> numpy vector of the <code>sin</code> of that and plot it</p>

<pre><code class="clojure">(let [x (numpy/arange 0 (* 3 numpy/pi) 0.1)
        y (numpy/sin x)]
    (plot/with-show
      (matplotlib.pyplot/plot x y)))
</code></pre>

<p><img src="https://live.staticflickr.com/65535/49405284796_014447588d_z.jpg" alt="sin" /></p>

<p>Beautiful yes!</p>

<p>Let&rsquo;s get a bit more complicated now and and plot both the sin and cosine as well as add labels, title, and legend.</p>

<pre><code class="clojure">(let [x (numpy/arange 0 (* 3 numpy/pi) 0.1)
        y-sin (numpy/sin x)
        y-cos (numpy/cos x)]
    (plot/with-show
      (matplotlib.pyplot/plot x y-sin)
      (matplotlib.pyplot/plot x y-cos)
      (matplotlib.pyplot/xlabel "x axis label")
      (matplotlib.pyplot/ylabel "y axis label")
      (matplotlib.pyplot/title "Sine and Cosine")
      (matplotlib.pyplot/legend ["Sine" "Cosine"])))
</code></pre>

<p><img src="http:////live.staticflickr.com/65535/49405284806_1d04957bce_z.jpg" alt="sin and cos" /></p>

<p>We can also add subplots. Subplots are when you divide the plots into different portions.
It is a bit stateful and involves making one subplot <em>active</em> and making changes and then making the other subplot <em>active</em>. Again not too hard to do with Clojure.</p>

<pre><code class="clojure">(let [x (numpy/arange 0 (* 3 numpy/pi) 0.1)
        y-sin (numpy/sin x)
        y-cos (numpy/cos x)]
    (plot/with-show
      ;;; set up a subplot gird that has a height of 2 and width of 1
      ;; and set the first such subplot as active
      (matplotlib.pyplot/subplot 2 1 1)
      (matplotlib.pyplot/plot x y-sin)
      (matplotlib.pyplot/title "Sine")

      ;;; set the second subplot as active and make the second plot
      (matplotlib.pyplot/subplot 2 1 2)
      (matplotlib.pyplot/plot x y-cos)
      (matplotlib.pyplot/title "Cosine")))
</code></pre>

<p><img src="http:////live.staticflickr.com/65535/49405284836_8e49e4a6b8_z.jpg" alt="sin and cos subplots" /></p>

<h3>Plotting with Images</h3>

<p>Pyplot also has functions for working directly with images as well. Here we take a picture of my cat and create another version of it that is tinted.</p>

<pre><code class="clojure">(let [img (matplotlib.pyplot/imread "resources/cat.jpg")
        img-tinted (numpy/multiply img [1 0.95 0.9])]
    (plot/with-show
      (matplotlib.pyplot/subplot 1 2 1)
      (matplotlib.pyplot/imshow img)
      (matplotlib.pyplot/subplot 1 2 2)
      (matplotlib.pyplot/imshow (numpy/uint8 img-tinted))))
</code></pre>

<p><img src="http://live.staticflickr.com/65535/49404801993_ed398d5768_n.jpg" alt="cat tinted" /></p>

<h3>Pie charts</h3>

<p>Finally, we can show how to do a pie chart. I asked people in a <a href="https://twitter.com/gigasquid/status/1218358472049397761">twitter thread</a> what they wanted an example of in python interop and one of them was a pie chart. This is for you!</p>

<p>The original code for this example came from this <a href="https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html">tutorial</a>.</p>

<pre><code class="clojure">(let [labels ["Frogs" "Hogs" "Dogs" "Logs"]
        sizes [15 30 45 10]
        explode [0 0.1 0 0] ; only explode the 2nd slice (Hogs)
        ]
    (plot/with-show
      (let [[fig1 ax1] (matplotlib.pyplot/subplots)]
        (py. ax1 "pie" sizes :explode explode :labels labels :autopct "%1.1f%%"
                             :shadow true :startangle 90)
        (py. ax1 "axis" "equal")) ;equal aspec ration ensures that pie is drawn as circle
      ))
</code></pre>

<p><img src="http://live.staticflickr.com/65535/49404802008_7e84ceff76_z.jpg" alt="pie chart" /></p>

<h3>Onwards and Upwards!</h3>

<p>This is just the beginning. In upcoming posts, I will be showcasing examples of interop with different libraries from the python ecosystem. Part of the goal is to get people used to how to use interop but also to raise awareness of the capabilities of the python libraries out there right now since they have been historically out of our ecosystem.</p>

<p>If you have any libraries that you would like examples of, I&rsquo;m taking requests. Feel free to leave them in the comments of the blog or in the <a href="https://twitter.com/gigasquid/status/1218358472049397761">twitter thread</a>.</p>

<p>Until next time, happy interoping!</p>

<p>PS All the code examples are here <a href="https://github.com/gigasquid/libpython-clj-examples">https://github.com/gigasquid/libpython-clj-examples</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hugging Face GPT With Clojure]]></title>
    <link href="http://gigasquid.github.io/blog/2020/01/10/hugging-face-gpt-with-clojure/"/>
    <updated>2020-01-10T19:33:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2020/01/10/hugging-face-gpt-with-clojure</id>
    <content type="html"><![CDATA[<p><img src="https://live.staticflickr.com/65535/49364554561_6e4f4d0a51_w.jpg" alt="" /></p>

<p>A new age in Clojure has dawned. We now have interop access to any python library with <a href="https://github.com/cnuernber/libpython-clj">libpython-clj</a>.</p>

<p><br></p>

<p>Let me pause a minute to repeat.</p>

<p><br></p>

<p><strong> You can now interop with ANY python library. </strong></p>

<p><br></p>

<p>I know. It&rsquo;s overwhelming. It took a bit for me to come to grips with it too.</p>

<p><br></p>

<p>Let&rsquo;s take an example of something that I&rsquo;ve <em>always</em> wanted to do and have struggled with mightly finding a way to do it in Clojure:<br/>
I want to use the latest cutting edge GPT2 code out there to generate text.</p>

<p>Right now, that library is <a href="https://github.com/huggingface/transformers">Hugging Face Transformers</a>.<br/>
<br></p>

<p>Get ready. We will wrap that sweet hugging face code in Clojure parens!</p>

<h3>The setup</h3>

<p>The first thing you will need to do is to have python3 installed and the two libraries that we need:</p>

<p><br></p>

<ul>
<li>pytorch - <code>sudo pip3 install torch</code></li>
<li>hugging face transformers - <code>sudo pip3 install transformers</code></li>
</ul>


<p><br></p>

<p>Right now, some of you may not want to proceed. You might have had a bad relationship with Python in the past. It&rsquo;s ok, remember that some of us had bad relationships with Java, but still lead a happy and fulfilled life with Clojure and still can enjoy it from interop. The same is true with Python. Keep an open mind.</p>

<p><br></p>

<p>There might be some others that don&rsquo;t want to have anything to do with Python and want to keep your Clojure pure. Well, that is a valid choice. But you are missing out on what the big, vibrant, and chaotic Python Deep Learning ecosystem has to offer.</p>

<p><br></p>

<p>For those of you that are still along for the ride, let&rsquo;s dive in.</p>

<p><br></p>

<p>Your deps file should have just a single extra dependency in it:</p>

<pre><code class="clojure">:deps {org.clojure/clojure {:mvn/version "1.10.1"}
        cnuernber/libpython-clj {:mvn/version "1.30"}}
</code></pre>

<h3>Diving Into Interop</h3>

<p>The first thing that we need to do is require the libpython library.</p>

<pre><code class="clojure">(ns gigasquid.gpt2
  (:require [libpython-clj.require :refer [require-python]]
            [libpython-clj.python :as py]))
</code></pre>

<p>It has a very nice <code>require-python</code> syntax that we will use to load the python libraries so that we can use them in our Clojure code.</p>

<pre><code class="clojure">(require-python '(transformers))
(require-python '(torch))
</code></pre>

<p>Here we are going to follow along with the OpenAI GPT-2 tutorial and translate it into interop code.
The original tutorial is <a href="https://huggingface.co/transformers/quickstart.html">here</a></p>

<p><br></p>

<p>Let&rsquo;s take the python side first:</p>

<pre><code class="python">import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load pre-trained model tokenizer (vocabulary)
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
</code></pre>

<p>This is going to translate in our interop code to:</p>

<pre><code class="clojure">(def tokenizer (py/$a transformers/GPT2Tokenizer from_pretrained "gpt2"))
</code></pre>

<p>The <code>py/$a</code> function is used to call attributes on a Python object. We get the <code>transformers/GPTTokenizer</code> object that we have available to use and call <code>from_pretrained</code> on it with the string argument <code>"gpt2"</code></p>

<p><br></p>

<p>Next in the Python tutorial is:</p>

<pre><code class="python"># Encode a text inputs
text = "Who was Jim Henson ? Jim Henson was a"
indexed_tokens = tokenizer.encode(text)

# Convert indexed tokens in a PyTorch tensor
tokens_tensor = torch.tensor([indexed_tokens])
</code></pre>

<p>This is going to translate to Clojure:</p>

<pre><code class="clojure">(def text "Who was Jim Henson ? Jim Henson was a")
;; encode text input
(def indexed-tokens  (py/$a tokenizer encode text))
indexed-tokens ;=&gt;[8241, 373, 5395, 367, 19069, 5633, 5395, 367, 19069, 373, 257]

;; convert indexed tokens to pytorch tensor
(def tokens-tensor (torch/tensor [indexed-tokens]))
tokens-tensor
;; ([[ 8241,   373,  5395,   367, 19069,  5633,  5395,   367, 19069,   373,
;;    257]])
</code></pre>

<p>Here we are again using <code>py/$a</code> to call the <code>encode</code> method on the text. However, when we are just calling a function, we can do so directly with <code>(torch/tensor [indexed-tokens])</code>. We can even directly use vectors.</p>

<p><br></p>

<p>Again, you are doing this in the REPL, so you have full power for inspection and display of the python objects. It is a great interop experience - (cider even has doc information on the python functions in the minibuffer)!</p>

<p><br></p>

<p>The next part is to load the model itself. This will take a few minutes, since it has to download a big file from s3 and load it up.</p>

<p><br></p>

<p>In Python:</p>

<pre><code class="python"># Load pre-trained model (weights)
model = GPT2LMHeadModel.from_pretrained('gpt2')
</code></pre>

<p>In Clojure:</p>

<pre><code class="clojure">;;; Load pre-trained model (weights)
;;; Note: this will take a few minutes to download everything
(def model (py/$a transformers/GPT2LMHeadModel from_pretrained "gpt2"))
</code></pre>

<p>The next part is to run the model with the tokens and make the predictions.</p>

<p><br></p>

<p>Here the code starts to diverge a tiny bit.</p>

<p><br></p>

<p>Python:</p>

<pre><code class="python"># Set the model in evaluation mode to deactivate the DropOut modules
# This is IMPORTANT to have reproducible results during evaluation!
model.eval()

# If you have a GPU, put everything on cuda
tokens_tensor = tokens_tensor.to('cuda')
model.to('cuda')

# Predict all tokens
with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0]

# get the predicted next sub-word (in our case, the word 'man')
predicted_index = torch.argmax(predictions[0, -1, :]).item()
predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])
assert predicted_text == 'Who was Jim Henson? Jim Henson was a man'
</code></pre>

<p>And Clojure</p>

<pre><code class="clojure">;;; Set the model in evaluation mode to deactivate the DropOut modules
;;; This is IMPORTANT to have reproducible results during evaluation!
(py/$a model eval)


;;; Predict all tokens
(def predictions (py/with [r (torch/no_grad)]
                          (first (model tokens-tensor))))

;;; get the predicted next sub-word"
(def predicted-index (let [last-word-predictions (-&gt; predictions first last)
                           arg-max (torch/argmax last-word-predictions)]
                       (py/$a arg-max item)))

predicted-index ;=&gt;582

(py/$a tokenizer decode (-&gt; (into [] indexed-tokens)
                            (conj predicted-index)))

;=&gt; "Who was Jim Henson? Jim Henson was a man"
</code></pre>

<p>The main differences is that we are obviously not using the python array syntax in our code to manipulate the lists. For example, instead of using <code>outputs[0]</code>, we are going to use <code>(first outputs)</code>. But, other than that, it is a pretty good match, even with the <code>py/with</code>.</p>

<p>Also note that we are not making the call to configure it with GPU. This is intentionally left out to keep things simple for people to try it out. Sometimes, GPU configuration can be a bit tricky to set up depending on your system. For this example, you definitely won&rsquo;t need it since it runs fast enough on cpu. If you do want to do something more complicated later, like fine tuning, you will need to invest some time to get it set up.</p>

<h3>Doing Longer Sequences</h3>

<p>The next example in the tutorial goes on to cover generating longer text.</p>

<p><br></p>

<p>Python</p>

<pre><code class="python">tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained('gpt2')

generated = tokenizer.encode("The Manhattan bridge")
context = torch.tensor([generated])
past = None

for i in range(100):
    print(i)
    output, past = model(context, past=past)
    token = torch.argmax(output[0, :])

    generated += [token.tolist()]
    context = token.unsqueeze(0)

sequence = tokenizer.decode(generated)

print(sequence)
</code></pre>

<p>And Clojure</p>

<pre><code class="clojure">(def tokenizer (py/$a transformers/GPT2Tokenizer from_pretrained "gpt2"))
(def model (py/$a transformers/GPT2LMHeadModel from_pretrained "gpt2"))

(def generated (into [] (py/$a tokenizer encode "The Manhattan bridge")))
(def context (torch/tensor [generated]))


(defn generate-sequence-step [{:keys [generated-tokens context past]}]
  (let [[output past] (model context :past past)
        token (-&gt; (torch/argmax (first output)))
        new-generated  (conj generated-tokens (py/$a token tolist))]
    {:generated-tokens new-generated
     :context (py/$a token unsqueeze 0)
     :past past
     :token token}))

(defn decode-sequence [{:keys [generated-tokens]}]
  (py/$a tokenizer decode generated-tokens))

(loop [step {:generated-tokens generated
             :context context
             :past nil}
       i 10]
  (if (pos? i)
    (recur (generate-sequence-step step) (dec i))
    (decode-sequence step)))

;=&gt; "The Manhattan bridge\n\nThe Manhattan bridge is a major artery for"
</code></pre>

<p>The great thing is once we have it embedded in our code, there is no stopping. We can create a nice function:</p>

<pre><code class="clojure">(defn generate-text [starting-text num-of-words-to-predict]
  (let [tokens (into [] (py/$a tokenizer encode starting-text))
        context (torch/tensor [tokens])
        result (reduce
                (fn [r i]
                  (println i)
                  (generate-sequence-step r))

                {:generated-tokens tokens
                 :context context
                 :past nil}

                (range num-of-words-to-predict))]
    (decode-sequence result)))
</code></pre>

<p>And finally we can generate some fun text!</p>

<pre><code class="clojure">(generate-text "Clojure is a dynamic, general purpose programming language, combining the approachability and interactive" 20)

;=&gt; "Clojure is a dynamic, general purpose programming language, combining the approachability and interactive. It is a language that is easy to learn and use, and is easy to use for anyone"
</code></pre>

<p><strong>Clojure is a dynamic, general purpose programming language, combining the approachability and interactive. It is a language that is easy to learn and use, and is easy to use for anyone</strong></p>

<p><br></p>

<p>So true GPT2! So true!</p>

<h3>Wrap-up</h3>

<p>libpython-clj is a really powerful tool that will allow Clojurists to better explore, leverage, and integrate Python libraries into their code.</p>

<p><br></p>

<p>I&rsquo;ve been really impressed with it so far and I encourage you to check it out.</p>

<p><br></p>

<p>There is a <a href="https://github.com/gigasquid/libpython-clj-examples">repo with the examples</a> out there if you want to check them out. There is also an example of doing MXNet MNIST classification there as well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Integrating Deep Learning With clojure.spec]]></title>
    <link href="http://gigasquid.github.io/blog/2019/10/11/integrating-deep-learning-with-clojure-dot-spec/"/>
    <updated>2019-10-11T13:51:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2019/10/11/integrating-deep-learning-with-clojure-dot-spec</id>
    <content type="html"><![CDATA[<p>clojure.spec allows you to write specifications for data and use them for validation. It also provides a generative aspect that allows for robust testing as well as an additional way to understand your data through manual inspection. The dual nature of validation and generation is a natural fit for deep learning models that consist of paired discriminator/generator models.</p>

<p><br></p>

<p><strong><strong>TLDR: In this post we show that you can leverage the dual nature of clojure.spec&rsquo;s validator/generator to incorporate a deep learning model&rsquo;s classifier/generator.</strong></strong></p>

<p><br></p>

<p>A common use of clojure.spec is at the boundaries to validate that incoming data is indeed in the expected form. Again, this is boundary is a fitting place to integrate models for the deep learning paradigm and our traditional software code.</p>

<p>Before we get into the deep learning side of things, let&rsquo;s take a quick refresher on how to use clojure.spec.</p>

<h2>quick view of clojure.spec</h2>

<p>To create a simple spec for keywords that are cat sounds, we can use <code>s/def</code>.</p>

<pre><code class="clojure">(s/def ::cat-sounds #{:meow :purr :hiss})
</code></pre>

<p>To do the validation, you can use the <code>s/valid?</code> function.</p>

<pre><code class="clojure">(s/valid? ::cat-sounds :meow) ;=&gt; true
(s/valid? ::cat-sounds :bark) ;=&gt; false
</code></pre>

<p>For the generation side of things, we can turn the spec into generator and sample it.</p>

<pre><code class="clojure">(gen/sample (s/gen ::cat-sounds))
;=&gt;(:hiss :hiss :hiss :meow :meow :purr :hiss :meow :meow :meow)
</code></pre>

<p>There is the ability to compose specs by adding them together with <code>s/and</code>.</p>

<pre><code class="clojure">(s/def ::even-number (s/and int? even?))
(gen/sample (s/gen ::even-number))
;=&gt; (0 0 -2 2 0 10 -4 8 6 8)
</code></pre>

<p>We can also control the generation by creating a custom generator using <code>s/with-gen</code>.
In the following the spec is only that the data be a general string, but using the custom generator, we can restrict the output to only be a certain set of example cat names.</p>

<pre><code class="clojure">(s/def ::cat-name
  (s/with-gen
    string?
    #(s/gen #{"Suki" "Bill" "Patches" "Sunshine"})))

(s/valid? ::cat-name "Peaches") ;=&gt; true
(gen/sample (s/gen ::cat-name))
;; ("Patches" "Sunshine" "Sunshine" "Suki" "Suki" "Sunshine"
;;  "Suki" "Patches" "Sunshine" "Suki")
</code></pre>

<p>For further information on clojure.spec, I whole-heartedly recommend the <a href="https://clojure.org/guides/spec">spec Guide</a>. But, now with a basic overview of spec, we can move on to creating specs for our Deep Learning models.</p>

<h2>Creating specs for Deep Learning Models</h2>

<p>In previous posts, we covered making <a href="https://gigasquidsoftware.com/blog/2019/08/16/simple-autoencoder/">simple autoencoders for handwritten digits</a>.</p>

<p><img src="http://live.staticflickr.com/65535/48647524478_ca35bef78f_n.jpg" alt="handwritten digits" /></p>

<p>Then, we made models that would:</p>

<ul>
<li>Take an image of a digit and give you back the string value (ex: &ldquo;2&rdquo;) - <a href="https://gigasquidsoftware.com/blog/2019/08/30/focus-on-the-discriminator/">post</a></li>
<li>Take a string number value and give you back a digit image. - <a href="https://gigasquidsoftware.com/blog/2019/09/06/focus-on-the-generator/">post</a></li>
</ul>


<p>We will use both of the models to make a spec with a custom generator.</p>

<p><br></p>

<p><em>Note: For the sake of simplicity, some of the supporting code is left out. But if you want to see the whole code, it is on <a href="(https://github.com/gigasquid/clojure-mxnet-autoencoder/blob/master/src/clojure_mxnet_autoencoder/model_specs.clj">github</a>)</em></p>

<p><br></p>

<p>With the help of the trained discriminator model, we can make a function that takes in an image and returns the number string value.</p>

<pre><code class="clojure">(defn discriminate [image]
  (-&gt; (m/forward discriminator-model {:data [image]})
      (m/outputs)
      (ffirst)
      (ndarray/argmax-channel)
      (ndarray/-&gt;vec)
      (first)
      (int)))
</code></pre>

<p>Let&rsquo;s test it out with a test-image:</p>

<p><img src="http://live.staticflickr.com/65535/48881532151_251e30840e_s.jpg" alt="test-discriminator-image" /></p>

<pre><code class="clojure">(discriminate my-test-image) ;=&gt; 6
</code></pre>

<p>Likewise, with the trained generator model, we can make a function that takes a string number and returns the corresponding image.</p>

<pre><code class="clojure">(defn generate [label]
  (-&gt; (m/forward generator-model {:data [(ndarray/array [label] [batch-size])]})
      (m/outputs)
      (ffirst)))
</code></pre>

<p>Giving it a test drive as well:</p>

<pre><code class="clojure">(def generated-test-image (generate 3))
(viz/im-sav {:title "generated-image"
             :output-path "results/"
             :x (ndarray/reshape generated-test-image [batch-size 1 28 28])})
</code></pre>

<p><img src="http://live.staticflickr.com/65535/48881532451_023de68ddb_s.jpg" alt="generated-test-image" /></p>

<p>Great! Let&rsquo;s go ahead and start writing specs. First let&rsquo;s make a quick spec to describe a MNIST number - which is a single digit between 0 and 9.</p>

<pre><code class="clojure">(s/def ::mnist-number (s/and int? #(&lt;= 0 % 9)))
(s/valid? ::mnist-number 3) ;=&gt; true
(s/valid? ::mnist-number 11) ;=&gt; false
(gen/sample (s/gen ::mnist-number))
;=&gt; (0 1 0 3 5 3 7 5 0 1)
</code></pre>

<p>We now have both parts to validate and generate and can create a spec for it.</p>

<pre><code class="clojure">(s/def ::mnist-image
    (s/with-gen
      #(s/valid? ::mnist-number (discriminate %))
      #(gen/fmap (fn [n]
                   (do (ndarray/copy (generate n))))
                 (s/gen ::mnist-number))))
</code></pre>

<p>The <code>::mnist-number</code> spec is used for the validation after the <code>discriminate</code> model is used. On the generator side, we use the generator for the <code>::mnist-number</code> spec and feed that into the deep learning generator model to get sample images.</p>

<p>We have a test function that will help us test out this new spec, called <code>test-model-spec</code>. It will return a map with the following form:</p>

<pre><code>{:spec name-of-the-spec
 :valid? whether or not the `s/valid?` called on the test value is true or not
 :sample-values This calls the discriminator model on the generated values
 }
</code></pre>

<p>It will also write an image of all the sample images to a file named <code>sample-spec-name</code></p>

<p>Let&rsquo;s try it on our test image:</p>

<p><img src="http://live.staticflickr.com/65535/48881532151_251e30840e_s.jpg" alt="test-discriminator-image" /></p>

<pre><code class="clojure">(s/valid? ::mnist-image my-test-image) ;=&gt; true


(test-model-spec ::mnist-image my-test-image)
;; {:spec "mnist-image"
;;  :valid? true
;;  :sample-values [0 0 0 1 3 1 0 2 7 3]}
</code></pre>

<p><img src="http://live.staticflickr.com/65535/48882235262_1e0dd7b758_q.jpg" alt="sample-mnist-image" /></p>

<p>Pretty cool!</p>

<p>Let&rsquo;s do some more specs. But first, our spec is going to be a bit repetitive, so we&rsquo;ll make a quick macro to make things easier.</p>

<pre><code class="clojure">(defmacro def-model-spec [spec-key spec discriminate-fn generate-fn]
    `(s/def ~spec-key
       (s/with-gen
         #(s/valid? ~spec (~discriminate-fn %))
         #(gen/fmap (fn [n#]
                      (do (ndarray/copy (~generate-fn n#))))
                    (s/gen ~spec)))))
</code></pre>

<h3>More Specs - More Fun</h3>

<p>This time let&rsquo;s define an even mnist image spec</p>

<pre><code class="clojure"> (def-model-spec ::even-mnist-image
    (s/and ::mnist-number even?)
    discriminate
    generate)

  (test-model-spec ::even-mnist-image my-test-image)

  ;; {:spec "even-mnist-image"
  ;;  :valid? true
  ;;  :sample-values [0 0 2 0 8 2 2 2 0 0]}
</code></pre>

<p><img src="http://live.staticflickr.com/65535/48882253157_02e45d3132_q.jpg" alt="sample-even-mnist-image" /></p>

<p>And Odds</p>

<pre><code class="clojure">  (def-model-spec ::odd-mnist-image
    (s/and ::mnist-number odd?)
    discriminate
    generate)

  (test-model-spec ::odd-mnist-image my-test-image)

  ;; {:spec "odd-mnist-image"
  ;;  :valid? false
  ;;  :sample-values [5 1 5 1 3 3 3 1 1 1]}
</code></pre>

<p><img src="http://live.staticflickr.com/65535/48881548138_c18850f806_q.jpg" alt="sample-odd-mnist-image" /></p>

<p>Finally, let&rsquo;s do Odds that are over 2!</p>

<pre><code class="clojure">  (def-model-spec ::odd-over-2-mnist-image
    (s/and ::mnist-number odd? #(&gt; % 2))
    discriminate
    generate)

  (test-model-spec ::odd-over-2-mnist-image my-test-image)

  ;; {:spec "odd-over-2-mnist-image"
  ;;  :valid? false
  ;;  :sample-values [3 3 3 5 3 5 7 7 7 3]}
</code></pre>

<p><img src="http://live.staticflickr.com/65535/48882089776_6f55416418_q.jpg" alt="sample-odd-over-2-mnist-image" /></p>

<h2>Conclusion</h2>

<p>We have shown some of the potential of integrating deep learning models with Clojure. clojure.spec is a powerful tool and it can be leveraged in new and interesting ways for both deep learning and AI more generally.</p>

<p>I hope that more people are intrigued to experiment and take a further look into what we can do in this area.</p>
]]></content>
  </entry>
  
</feed>
