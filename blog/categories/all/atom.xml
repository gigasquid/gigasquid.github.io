<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: All | Squid's Blog]]></title>
  <link href="http://gigasquid.github.io/blog/categories/all/atom.xml" rel="self"/>
  <link href="http://gigasquid.github.io/"/>
  <updated>2018-07-05T19:53:27-04:00</updated>
  <id>http://gigasquid.github.io/</id>
  <author>
    <name><![CDATA[Carin Meier]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Clojure MXNet - The Module API]]></title>
    <link href="http://gigasquid.github.io/blog/2018/07/05/clojure-mxnet-the-module-api/"/>
    <updated>2018-07-05T19:39:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2018/07/05/clojure-mxnet-the-module-api</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg">https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg</a>"></p>

<p>This is an introduction to the high level Clojure API for deep learning library <a href="http://mxnet.incubator.apache.org/">MXNet</a>.</p>

<p>The module API provides an intermediate and high-level interface for performing computation with neural networks in MXNet.</p>

<p>To follow along with this documentation, you can use this namespace to with the needed requires:</p>

<p>```clojure
(ns docs.module
  (:require [clojure.java.io :as io]</p>

<pre><code>        [clojure.java.shell :refer [sh]]
        [org.apache.clojure-mxnet.eval-metric :as eval-metric]
        [org.apache.clojure-mxnet.io :as mx-io]
        [org.apache.clojure-mxnet.module :as m]
        [org.apache.clojure-mxnet.symbol :as sym]
        [org.apache.clojure-mxnet.ndarray :as ndarray]))
</code></pre>

<p>```</p>

<h2>Prepare the Data</h2>

<p>In this example, we are going to use the MNIST data set. If you have cloned the MXNet repo and <code>cd contrib/clojure-package</code>, we can run some helper scripts to download the data for us.</p>

<p>```clojure
(def data-dir &ldquo;data/&rdquo;)</p>

<p>(when-not (.exists (io/file (str data-dir &ldquo;train-images-idx3-ubyte&rdquo;)))
  (sh &ldquo;../../scripts/get_mnist_data.sh&rdquo;))
```</p>

<p>MXNet provides function in the <code>io</code> namespace to load the MNIST datasets into training and test data iterators that we can use with our module.</p>

<p>```clojure
(def train-data (mx-io/mnist-iter {:image (str data-dir &ldquo;train-images-idx3-ubyte&rdquo;)</p>

<pre><code>                               :label (str data-dir "train-labels-idx1-ubyte")
                               :label-name "softmax_label"
                               :input-shape [784]
                               :batch-size 10
                               :shuffle true
                               :flat true
                               :silent false
                               :seed 10}))
</code></pre>

<p>(def test-data (mx-io/mnist-iter {:image (str data-dir &ldquo;t10k-images-idx3-ubyte&rdquo;)</p>

<pre><code>                              :label (str data-dir "t10k-labels-idx1-ubyte")
                              :input-shape [784]
                              :batch-size 10
                              :flat true
                              :silent false}))
</code></pre>

<p>```</p>

<h2>Preparing a Module for Computation</h2>

<p>To construct a module, we need to have a symbol as input. This symbol takes input data in the first layer and then has subsequent layers of fully connected and relu activation layers, ending up in a softmax layer for output.</p>

<p>```clojure
(let [data (sym/variable &ldquo;data&rdquo;)</p>

<pre><code>  fc1 (sym/fully-connected "fc1" {:data data :num-hidden 128})
  act1 (sym/activation "relu1" {:data fc1 :act-type "relu"})
  fc2 (sym/fully-connected "fc2" {:data act1 :num-hidden 64})
  act2 (sym/activation "relu2" {:data fc2 :act-type "relu"})
  fc3 (sym/fully-connected "fc3" {:data act2 :num-hidden 10})
  out (sym/softmax-output "softmax" {:data fc3})]
</code></pre>

<p>  out)
  ;=>#object[org.apache.mxnet.Symbol 0x1f43a406 &ldquo;org.apache.mxnet.Symbol@1f43a406&rdquo;]
```</p>

<p>You can also write this with the <code>as-&gt;</code> threading macro.</p>

<p>```clojure
(def out (as-> (sym/variable &ldquo;data&rdquo;) data</p>

<pre><code>       (sym/fully-connected "fc1" {:data data :num-hidden 128})
       (sym/activation "relu1" {:data data :act-type "relu"})
       (sym/fully-connected "fc2" {:data data :num-hidden 64})
       (sym/activation "relu2" {:data data :act-type "relu"})
       (sym/fully-connected "fc3" {:data data :num-hidden 10})
       (sym/softmax-output "softmax" {:data data})))
</code></pre>

<p>;=> #&lsquo;tutorial.module/out
```</p>

<p>By default, <code>context</code> is the CPU. If you need data parallelization, you can specify a GPU context or an array of GPU contexts like this <code>(m/module out {:contexts [(context/gpu)]})</code></p>

<p>Before you can compute with a module, you need to call <code>bind</code> to allocate the device memory and <code>init-params</code> or <code>set-params</code> to initialize the parameters. If you simply want to fit a module, you don’t need to call <code>bind</code> and <code>init-params</code> explicitly, because the <code>fit</code> function automatically calls them if they are needed.</p>

<p>```clojure
(let [mod (m/module out)]
  (&ndash;> mod</p>

<pre><code>  (m/bind {:data-shapes (mx-io/provide-data train-data)
           :label-shapes (mx-io/provide-label train-data)})
  (m/init-params)))
</code></pre>

<p>```</p>

<p>Now you can compute with the module using functions like <code>forward</code>, <code>backward</code>, etc.</p>

<h2>Training and Predicting</h2>

<p>Modules provide high-level APIs for training, predicting, and evaluating. To fit a module, call the <code>fit</code> function with some data iterators:</p>

<p><code>clojure
(def mod (m/fit (m/module out) {:train-data train-data :eval-data test-data :num-epoch 1}))
;; Epoch  0  Train- [accuracy 0.12521666]
;; Epoch  0  Time cost- 8392
;; Epoch  0  Validation-  [accuracy 0.2227]
</code></p>

<p>You can pass in batch-end callbacks using batch-end-callback and epoch-end callbacks using epoch-end-callback in the <code>fit-params</code>. You can also set parameters using functions like in the fit-params like optimizer and eval-metric. To learn more about the fit-params, see the fit-param function options. To predict with a module, call <code>predict</code> with a DataIter:</p>

<p>```clojure
(def results (m/predict mod {:eval-data test-data}))
(first results) ;=>#object[org.apache.mxnet.NDArray 0x3540b6d3 &ldquo;org.apache.mxnet.NDArray@a48686ec&rdquo;]</p>

<p>(first (ndarray/&ndash;>vec (first results))) ;=>0.08261358
```</p>

<p>The module collects and returns all of the prediction results. For more details about the format of the return values, see the documentation for the <code>predict</code> function.</p>

<p>When prediction results might be too large to fit in memory, use the <code>predict-every-batch</code> API.</p>

<p>```clojure
(let [preds (m/predict-every-batch mod {:eval-data test-data})]
  (mx-io/reduce-batches test-data</p>

<pre><code>                    (fn [i batch]
                      (println (str "pred is " (first (get preds i))))
                      (println (str "label is " (mx-io/batch-label batch)))
                      ;;; do something
                      (inc i))))
</code></pre>

<p>```</p>

<p>If you need to evaluate on a test set and don’t need the prediction output, call the <code>score</code> function with a data iterator and an eval metric:</p>

<p><code>clojure
(m/score mod {:eval-data test-data :eval-metric (eval-metric/accuracy)}) ;=&gt;["accuracy" 0.2227]
</code></p>

<p>This runs predictions on each batch in the provided data iterator and computes the evaluation score using the provided eval metric. The evaluation results are stored in metric so that you can query later.</p>

<h2>Saving and Loading</h2>

<p>To save the module parameters in each training epoch, use a <code>checkpoint</code> function:</p>

<p>```clojure
(let [save-prefix &ldquo;my-model&rdquo;]
  (doseq [epoch-num (range 3)]</p>

<pre><code>(mx-io/do-batches train-data (fn [batch
                                      ;; do something
</code></pre>

<p>]))</p>

<pre><code>(m/save-checkpoint mod {:prefix save-prefix :epoch epoch-num :save-opt-states true})))
</code></pre>

<p>;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0000.params
;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0000.states
;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0001.params
;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0001.states
;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0002.params
;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0002.states</p>

<p>```</p>

<p>To load the saved module parameters, call the <code>load-checkpoint</code> function:</p>

<p>```clojure
(def new-mod (m/load-checkpoint {:prefix &ldquo;my-model&rdquo; :epoch 1 :load-optimizer-states true}))</p>

<p>new-mod ;=> #object[org.apache.mxnet.module.Module 0x5304d0f4 &ldquo;org.apache.mxnet.module.Module@5304d0f4&rdquo;]
```</p>

<p>To initialize parameters, Bind the symbols to construct executors first with bind function. Then, initialize the parameters and auxiliary states by calling <code>init-params</code> function.</p>

<p>```clojure
(&ndash;> new-mod</p>

<pre><code>(m/bind {:data-shapes (mx-io/provide-data train-data) :label-shapes (mx-io/provide-label train-data)})
(m/init-params))
</code></pre>

<p>```</p>

<p>To get current parameters, use <code>params</code></p>

<p>```clojure</p>

<p>(let [[arg-params aux-params] (m/params new-mod)]
  {:arg-params arg-params
   :aux-params aux-params})</p>

<p>;; {:arg-params
;;  {&ldquo;fc3_bias&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x39adc3b0 &ldquo;org.apache.mxnet.NDArray@49caf426&rdquo;],
;;   &ldquo;fc2_weight&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x25baf623 &ldquo;org.apache.mxnet.NDArray@a6c8f9ac&rdquo;],
;;   &ldquo;fc1_bias&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x6e089973 &ldquo;org.apache.mxnet.NDArray@9f91d6eb&rdquo;],
;;   &ldquo;fc3_weight&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x756fd109 &ldquo;org.apache.mxnet.NDArray@2dd0fe3c&rdquo;],
;;   &ldquo;fc2_bias&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x1dc69c8b &ldquo;org.apache.mxnet.NDArray@d128f73d&rdquo;],
;;   &ldquo;fc1_weight&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x20abc769 &ldquo;org.apache.mxnet.NDArray@b8e1c5e8&rdquo;]},
;;  :aux-params {}}</p>

<p>```</p>

<p>To assign parameter and aux state values, use <code>set-params</code> function.</p>

<p><code>clojure
(m/set-params new-mod {:arg-params (m/arg-params new-mod) :aux-params (m/aux-params new-mod)})
;=&gt; #object[org.apache.mxnet.module.Module 0x5304d0f4 "org.apache.mxnet.module.Module@5304d0f4"]
</code></p>

<p>To resume training from a saved checkpoint, instead of calling <code>set-params</code>, directly call <code>fit</code>, passing the loaded parameters, so that <code>fit</code> knows to start from those parameters instead of initializing randomly</p>

<p>Create fit-params, and then use it to set <code>begin-epoch</code> so that <code>fit</code> knows to resume from a saved epoch.</p>

<p>```clojure
;; reset the training data before calling fit or you will get an error
(mx-io/reset train-data)
(mx-io/reset test-data)</p>

<p>(m/fit new-mod {:train-data train-data :eval-data test-data :num-epoch 2</p>

<pre><code>            :fit-params (-&gt; (m/fit-params {:begin-epoch 1}))})
</code></pre>

<p>```</p>

<p>If you are interested in checking out MXNet and exploring on your own, check out the main page <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">here</a> with instructions on how to install and other information.</p>

<h3>See other blog posts about MXNet</h3>

<ul>
<li><a href="http://gigasquidsoftware.com/blog/2018/06/03/meet-clojure-mxnet-ndarray/">Clojure MXNet &ndash; NDArray</a></li>
<li><a href="http://gigasquidsoftware.com/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/">Clojure MXNet Joins Apache MXNet</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clojure MXNet Joins the Apache MXNet Project]]></title>
    <link href="http://gigasquid.github.io/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/"/>
    <updated>2018-07-01T10:44:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg">https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg</a>"></p>

<p>I&rsquo;m delighted to share the news that the Clojure package for <a href="https://mxnet.apache.org/">MXNet</a> has now joined the main Apache MXNet project. A big thank you to the efforts of everyone involved to make this possible. Having it as part of the main project is a great place for growth and collaboration that will benefit both MXNet and the Clojure community.</p>

<h2>Invitation to Join and Contribute</h2>

<p>The Clojure package has been brought in as a <em>contrib</em> <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">clojure-package</a>. It is still very new and will go through a period of feedback, stabilization, and improvement before it graduates out of contrib.</p>

<p>We welcome contributors and people getting involved to make it better.</p>

<p>Are you interested in Deep Learning and Clojure? Great &ndash; Join us!</p>

<p>There are a few ways to get involved.</p>

<ul>
<li>Check out the current state of the Clojure package some contribution needs here <a href="https://cwiki.apache.org/confluence/display/MXNET/Clojure+Package+Contribution+Needs">https://cwiki.apache.org/confluence/display/MXNET/Clojure+Package+Contribution+Needs</a></li>
<li>Join the Clojurian Slack #mxnet channel</li>
<li>Join the <a href="https://lists.apache.org/list.html?dev@mxnet.apache.org">MXNet dev mailing list</a> by sending an email to <code>dev-subscribe@mxnet.apache.org.</code>.</li>
<li>Join the MXNET Slack channel &ndash; You have to join the MXnet dev mailing list first, but after that says you would like to join the slack and someone will add you.</li>
<li>Join the <a href="https://discuss.mxnet.io/">MXNet Discussion Forum</a></li>
</ul>


<h3>Want to Learn More?</h3>

<p>There are lots of examples in the package to check out, but a good place to start are the tutorials here <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial">https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial</a></p>

<p>There is a blog walkthough here as well &ndash; <a href="http://gigasquidsoftware.com/blog/2018/06/03/meet-clojure-mxnet-ndarray/">Meet Clojure MXNet NDArray</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Meet Clojure MXNet - NDArray]]></title>
    <link href="http://gigasquid.github.io/blog/2018/06/03/meet-clojure-mxnet-ndarray/"/>
    <updated>2018-06-03T16:13:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2018/06/03/meet-clojure-mxnet-ndarray</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg">https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg</a>"></p>

<p>This is the beginning of a series of blog posts to get to know the <a href="https://mxnet.apache.org/">Apache MXNet</a> Deep Learning project and the new Clojure language binding <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">clojure-package</a></p>

<p>MXNet is a first class, modern deep learning library that AWS has officially picked as its chosen library. It supports multiple languages on a first class basis and is incubating as an Apache project.</p>

<p>The motivation for creating a Clojure package is to be able to open the deep learning library to the Clojure ecosystem and build bridges for future development and innovation for the community. It provides all the needed tools including low level and high level apis, dynamic graphs, and things like GAN and natural language support.</p>

<p>So let&rsquo;s get on with our introduction with one of the basic building blocks of MXNet, the <code>NDArray</code>.</p>

<h2>Meet NDArray</h2>

<p>The <code>NDArray</code> is the tensor data structure in MXNet. Let&rsquo;s start of by creating one. First we need to require the <code>ndarray</code> namespace:</p>

<p><code>clojure
(ns tutorial.ndarray
  (:require [org.apache.clojure-mxnet.ndarray :as ndarray]))
</code></p>

<p>Now let&rsquo;s create an all zero array of dimension 100 x 50</p>

<p><code>clojure
(ndarray/zeros [100 50])
;=&gt; #object[org.apache.mxnet.NDArray 0x3e396d0 "org.apache.mxnet.NDArray@aeea40b6"]
</code></p>

<p>We can check the shape of this by using <code>shape-vec</code></p>

<p><code>clojure
(ndarray/shape-vec (ndarray/zeros [100 50]))
;=&gt; [100 50]
</code></p>

<p>There is also a quick way to create an ndarray of ones with the <code>ones</code> function:</p>

<p><code>clojure
(ndarray/ones [256 32 128 1])
</code></p>

<p>Ones and zeros are nice, but what an array with specific contents? There is an <code>array</code> function for that. Specific the contents of the array first and the shape second:</p>

<p><code>clojure
(def c (ndarray/array [1 2 3 4 5 6] [2 3]))
(ndarray/shape-vec c)  ;=&gt; [2 3]
</code></p>

<p>To convert it back to a vector format, we can use the <code>-&gt;vec</code> function.</p>

<p><code>clojure
(ndarray/-&gt;vec c)
;=&gt; [1.0 2.0 3.0 4.0 5.0 6.0]
</code></p>

<p>Now that we know how to create NDArrays, we can get to do something interesting like operations on them.</p>

<h3>Operations</h3>

<p>There are all the standard arithmetic operations:</p>

<p><code>clojure
(def a (ndarray/ones [1 5]))
(def b (ndarray/ones [1 5]))
(-&gt; (ndarray/+ a b) (ndarray/-&gt;vec))
;=&gt;  [2.0 2.0 2.0 2.0 2.0]
</code></p>

<p>Note that the original ndarrays are unchanged.</p>

<p><code>clojure
(ndarray/-&gt;vec a) ;=&gt; [1.0 1.0 1.0 1.0 1.0]
(ndarray/-&gt;vec b) ;=&gt; [1.0 1.0 1.0 1.0 1.0]
</code></p>

<p>But, we can change that if we use the inplace operators:</p>

<p><code>clojure
(ndarray/+= a b)
(ndarray/-&gt;vec a) ;=&gt;  [2.0 2.0 2.0 2.0 2.0]
</code></p>

<p>There are many more operations, but just to give you a taste, we&rsquo;ll take a look a the <code>dot</code> product operation:</p>

<p><code>clojure
(def arr1 (ndarray/array [1 2] [1 2]))
(def arr2 (ndarray/array [3 4] [2 1]))
(def res (ndarray/dot arr1 arr2))
(ndarray/shape-vec res) ;=&gt; [1 1]
(ndarray/-&gt;vec res) ;=&gt; [11.0]
</code></p>

<p>If you are curious about the other operators available in NDArray API check out the <a href="https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html">MXNet project documentation page</a></p>

<p>Now that we have ndarrays and can do calculations on them, we might want to save and load them.</p>

<h3>Saving and Loading</h3>

<p>You can save ndarrays with a name as a map like:</p>

<p><code>clojure
(ndarray/save "filename" {"arr1" arr1 "arr2" arr2})
</code></p>

<p>To load them, you just specify the filename and the map is returned.</p>

<p><code>clojure
(ndarray/load "filename")
;=&gt; {"arr1" #object[org.apache.mxnet.NDArray 0x1b629ff4 "org.apache.mxnet.NDArray@63da08cb"]
;=&gt;  "arr2" #object[org.apache.mxnet.NDArray 0x25d994e3 "org.apache.mxnet.NDArray@5bbaf2c3"]}
</code></p>

<p>One more cool thing, we can even due our operations on the cpu or gpu.</p>

<h3>Multi-Device Support</h3>

<p>When creating an <code>ndarray</code> you can use a context argument to specify the device. To do this, we will need the help of the <code>context</code> namespace.</p>

<p><code>clojure
(require '[org.apache.clojure-mxnet.context :as context])
</code></p>

<p>By default, the <code>ndarray</code> is created on the cpu context.</p>

<p><code>clojure
(def cpu-a (ndarray/zeros [100 200]))
(ndarray/context cpu-a)
;=&gt; #object[ml.dmlc.mxnet.Context 0x3f376123 "cpu(0)"]
</code></p>

<p>But we can specify the gpu instead, (if we have a gpu enabled build).</p>

<p><code>clojure
(def gpu-b (ndarray/zeros [100 200] {:ctx (context/gpu 0)}))
</code></p>

<p><em>Note: Operations among different contexts are currently not allowed, but there is a <code>copy-to</code> function that can help copy the content from one device to another and then continue on with the computation.</em></p>

<h2>Wrap up</h2>

<p>I hope you&rsquo;ve enjoyed the brief introduction to the MXNet library, there is much more to explore in future posts. If you are interested in giving it a try, there are native jars for OSX cpu and Linux cpu/gpu available and the code for the ndarray tutorial can be found <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial">here</a></p>

<p><em>Please remember that the library is in a experimential state, so if you encounter any problems or have any other feedback, please log an issue so bugs and rough edges can be fixed :).</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cats and Dogs with Cortex Redux]]></title>
    <link href="http://gigasquid.github.io/blog/2017/11/07/cats-and-dogs-with-cortex-redux/"/>
    <updated>2017-11-07T18:51:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2017/11/07/cats-and-dogs-with-cortex-redux</id>
    <content type="html"><![CDATA[<p>I wrote a <a href="http://gigasquidsoftware.com/blog/2016/12/27/deep-learning-in-clojure-with-cortex/">blog post</a> a while back about using a Clojure machine learning library called <a href="https://github.com/thinktopic/cortex">Cortex</a> to do the Kaggle Cats and Dogs classification challenge.</p>

<p>I wanted to revisit it for a few reasons. The first one is that the Cortex library has progressed and improved considerably over the last year. It&rsquo;s still not at version 1.0, but it my eyes, it&rsquo;s really starting to shine. The second reason is that they recently published an <a href="https://github.com/thinktopic/cortex/tree/master/examples/resnet-retrain">example</a> of using the RESNET50 model, (I&rsquo;ll explain later on), to do fine-tuning or transfer learning. The third reason, is that there is a great new plugin for leiningen the supports using <a href="https://github.com/didiercrunch/lein-jupyter">Jupyter notebooks with Clojure projects</a>. These notebooks are a great way of doing walkthroughs and tutorials.</p>

<p>Putting all these things together, I felt like I was finally at a stage where I could somewhat replicate the first lesson in the <a href="https://github.com/fastai/courses/blob/master/deeplearning1/nbs/dogs_cats_redux.ipynb">Practical Deep Learning Course for Coders</a> with Cats and Dogs &ndash; although this time all in Clojure!</p>

<h2>Where to Start?</h2>

<p><img class="<a" src="href="http://kaggle2.blob.core.windows.net/competitions/kaggle/3362/media/woof_meow.jpg">http://kaggle2.blob.core.windows.net/competitions/kaggle/3362/media/woof_meow.jpg</a>"></p>

<p>In the last blog post, we created our deep learning network and trained the data on scaled down images (like 50x50) from scratch. This time we are much smarter.</p>

<p>We are still of course going to have to get a hold of all the training data from <a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data">Kaggle Cats vs Dogs Challenge</a>. The big difference is this time, we are just going to have to train our model for <em>1 epoch</em>. What&rsquo;s more, the results will be way better than before.</p>

<p>How is this possible? We are going to use an already trained model, RESNET50. This model has already been painstakingly trained with a gigantic network that is 50 layers deep on the ImageNet challenge. That&rsquo;s a challenge that has models try to classify a 1000 different categories. The theory is that the inner layers of the network have already learned about the features that make up cats and dogs, all we would need to do is peel off the final layer of the network and graft on a new layers that just learns the final classification for our 2 categories of cats and dogs. This is called <em>transfer learning</em> or <em>retraining</em>.</p>

<h2>Plan of Action</h2>

<ul>
<li>Get all the cats and dogs pictures in the right directory format for training</li>
<li>Train the model with all but the last layer in the RESNET model. The last layer we are going to replace with our own layer that will finetune it to classify only cats and dogs</li>
<li>Run the test data and come up with a spreadsheet of results to submit to Kaggle.</li>
</ul>


<h3>Getting all the data pictures in the right format</h3>

<p>This is the generally the most time consuming step of most deep learning. I&rsquo;ll spare you the gritty details but we want to get all the pictures from the <code>train.zip</code> into the format</p>

<p>```
-data
  -cats-dogs-training</p>

<pre><code>  -cat
      1110.png
      ...
  -dog
      12416.png
      ...
</code></pre>

<p>  -cats-dogs-testing</p>

<pre><code>  -cat
      11.png
      ...
  -dog
      12.png
      ...
</code></pre>

<p>```</p>

<p>The image sizes must also all be resized to match the input of the RESNET50. That means they all have to be 224x224.</p>

<h3>Train the model</h3>

<p>The cortex functions allow you to load the resnet50 model, remove the last layer, freeze all the other layers so that they will not be retrained, and add new layers.</p>

<p>I was surprised that I could actually train the model with all the images at 224x244 with the huge RESNET50 model. I built the uberjar and ran it which helped the performance.</p>

<p><code>lein uberjar</code></p>

<p><code>java -jar target/cats-dogs-cortex-redux.jar</code></p>

<p>Training one epoch took me approximately 6 minutes. Not bad, especially considering that&rsquo;s all the training I really needed to do.</p>

<p><code>
Loss for epoch 1: (current) 0.05875186542016347 (best) null
Saving network to trained-network.nippy
</code></p>

<p>The key point is that it saved the fine tuned network to trained-network.nippy</p>

<h3>Run the Kaggle test results and submit the results</h3>

<p>You will need to do a bit more setup for this. First, you need to get the Kaggle test images for classification. There are 12500 of these in the test.zip file from the site. Under the data directory, create a new directory called kaggle-test. Now unzip the contents of test.zip inside that folder. The full directory with all the test images should now be:</p>

<p><code>data/kaggle-test/test</code></p>

<p>This step takes a long time and you might have to tweak the batch size again depending on your memory. There are 12500 predications to be made. The main logic for this is in function called <code>(kaggle-results batch-size)</code>. It will take a long time to run. It will print the results as it goes along to the kaggle-results.csv file. If you want to check progress you can do wc -l kaggle-results.csv</p>

<p>For me locally, with <code>(cats-dogs/kaggle-results 100)</code> it took me 28 minutes locally.</p>

<h3>Compare the results</h3>

<p><img class="<a" src="href="http://c1.staticflickr.com/5/4518/26477015609_1af781b8da_b.jpg">http://c1.staticflickr.com/5/4518/26477015609_1af781b8da_b.jpg</a>"></p>

<p>My one epoch of fine tuning beat my best results of going through the Practical Deep Learning exercise with the fine tuning the VGG16 model. Not bad at all.</p>

<h2>Summary</h2>

<p>For those of you that are interested in checking out the code, it&rsquo;s out there on <a href="https://github.com/gigasquid/cats-dogs-cortex-redux">github</a></p>

<p>Even more exciting, there is a <a href="https://github.com/gigasquid/cats-dogs-cortex-redux/blob/master/Cats%20and%20Dogs%20in%20Cortex%20(Redux).ipynb">walkthrough in a jupyter notebook</a> with a lein-jupyter plugin.</p>

<p>The Deep Learning world in Clojure is an exciting place to be and gaining tools and traction more and more.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Embedded Interop between Clojure, R, and Python with GraalVM]]></title>
    <link href="http://gigasquid.github.io/blog/2017/10/22/embedded-interop-between-clojure-r-and-python-with-graalvm/"/>
    <updated>2017-10-22T16:02:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2017/10/22/embedded-interop-between-clojure-r-and-python-with-graalvm</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://images-na.ssl-images-amazon.com/images/M/MV5BOTViY2Y0ZGItMTg2OC00YzEzLWJhYjYtZjg4OTMyOWE4YzM1XkEyXkFqcGdeQXVyNTQ1NzU4Njk@._V1_.jpg">https://images-na.ssl-images-amazon.com/images/M/MV5BOTViY2Y0ZGItMTg2OC00YzEzLWJhYjYtZjg4OTMyOWE4YzM1XkEyXkFqcGdeQXVyNTQ1NzU4Njk@._V1_.jpg</a>" title="" ></p>

<p>In my talk at <a href="https://www.youtube.com/watch?v=eLl6_k_fZn4">Clojure Conj</a> I mentioned how a project from Oracle Labs named GraalVM might have to potential for Clojure to interop with Python on the same VM. At the time of the talk, I had just learned about it so I didn&rsquo;t have time to take a look at it. Over the last week, I&rsquo;ve managed to take it for a test drive and I wanted to share what I found.</p>

<h3>Are you ready?</h3>

<p>In this example, we will be using an ordinary Leinengen project and using the REPL we will interop with both R and python.</p>

<p>But first will need a bit of setup.</p>

<p>We will download the <a href="http://www.oracle.com/technetwork/oracle-labs/program-languages/downloads/index.html">Graal project</a> so we can use its <code>java</code> instead of our own.</p>

<p>Once we have it downloaded we will configure our PATH to use Graal&rsquo;s java instead of our own.</p>

<p>```</p>

<h1>export PATH=/path/to/graalAndTruffle/bin:$PATH</h1>

<p>```</p>

<p>Now, we can create a new lein project and run <code>lein repl</code> and begin the fun.</p>

<h3>The Polyglot Context</h3>

<p>In our new namespace, we just need to import the <a href="http://graalvm.github.io/graal/truffle/javadoc/org/graalvm/polyglot/Context.html">Polyglot Context</a> to get started:</p>

<p>```clojure
(ns graal-test.core
  (:import (org.graalvm.polyglot Context)))</p>

<p>;; note that is also supports Ruby, LLVM, and JS
(def context (Context/create (into-array [&ldquo;python&rdquo; &ldquo;R&rdquo;])))
```</p>

<p>Now, we are ready to actually try to run some R and Python code right in our REPL. Let&rsquo;s start first with R.</p>

<h3>Interoping with R</h3>

<p>The main function we are going to use is the <code>eval</code> function in the context. Let&rsquo;s start small with some basic math.</p>

<p><code>clojure
(.eval context "R" "
3^2 + 2^2
")
;=&gt; #object[org.graalvm.polyglot.Value 0x7ff40e4d "13.0"]
</code></p>

<p>Wow! It actually did something. It returned something called a <a href="https://github.com/graalvm/graal/blob/master/sdk/src/org.graalvm.polyglot/src/org/graalvm/polyglot/Value.java">Polyglot Value</a> with what looks like the right answer in it.</p>

<p>Emboldened by our early success, let&rsquo;s try something a little more complicated like calling a function.</p>

<p><code>clojure
(def result1 (.eval context "R" "
sum.of.squares &lt;- function(x,y) {
  x^2 + y^2
}
sum.of.squares(3,4)
"))
;=&gt; #object[org.graalvm.polyglot.Value 0xc3edd92 "25.0"]
</code></p>

<p>Again, it looks like it worked. Let&rsquo;s try to get the result back into Clojure as a value we can work with. We could ask the result what sort of type it is with</p>

<p><code>clojure
(.isNumber result1) ;=&gt; true
</code></p>

<p>but let&rsquo;s just use <code>clojure.edn</code> to read the string and save some time.</p>

<p>```
(defn &ndash;>clojure [polyglot-value]
  (&ndash;> polyglot-value</p>

<pre><code>  (.toString)
  (clojure.edn/read-string)))
</code></pre>

<p>(&ndash;>clojure result1) ;=> 25
```</p>

<p>It would be nice to have a easier way to export symbols and import symbols to and from the guest and host language. In fact, Graal provides a way to do this but to do this in Clojure, we would need something else called <a href="https://github.com/graalvm/graal/tree/master/truffle">Truffle</a>.</p>

<p>Truffle is part of the Graal project and is a framework for implementing languages with the Graal compliler.
There are quite a few languages implemented with the Truffle framework. R is one of them.</p>

<p><img class="<a" src="href="https://image.slidesharecdn.com/polyglotonthejvmwithgraalenglish-170521104613/95/polyglot-on-the-jvm-with-graal-english-14-638.jpg?cb=1495364615">https://image.slidesharecdn.com/polyglotonthejvmwithgraalenglish-170521104613/95/polyglot-on-the-jvm-with-graal-english-14-638.jpg?cb=1495364615</a>"></p>

<p>My understanding is that if Clojure was implemented as a truffle lang, then interop could be much more seamless like this example in Ruby</p>

<p><img class="<a" src="href="https://image.slidesharecdn.com/polyglotonthejvmwithgraalenglish-170521104613/95/polyglot-on-the-jvm-with-graal-english-37-638.jpg?cb=1495364615">https://image.slidesharecdn.com/polyglotonthejvmwithgraalenglish-170521104613/95/polyglot-on-the-jvm-with-graal-english-37-638.jpg?cb=1495364615</a>"></p>

<p>But let&rsquo;s continue in our exploration. What about doing something more interesting, like importing a useful R library and using it. How about the <a href="https://www.rdocumentation.org/packages/numDeriv/versions/2016.8-1">numDeriv</a> package that supports Accurate Numerical Derivatives?</p>

<p>First we import the package using cran.</p>

<p><code>clojure
(.eval context "R" "
install.packages(\"numDeriv\", repos = \"http://cran.case.edu/\")
")
</code></p>

<p>If you are doing this at your REPL, you can will see lots of text going on in your <code>lein repl</code> process at this point. It&rsquo;s going out and figuring out what deps you need and installing them in your <code>/graalvm-0.28.2/jre/languages/R</code> directory structure.</p>

<p>After it is done, we can actually use it!</p>

<p>```clojure
(def result2 (.eval context &ldquo;R&rdquo; &ldquo;
library(numDeriv)
grad(sin, (0:10)<em>2</em>pi/10)
&rdquo;))
result2 ;=> #object[org.graalvm.polyglot.Value 0x76765898 &ldquo;c(1,</p>

<pre><code>    ;0.809016994367249, 0.309016994372158, -0.309016994373567,
    ;-0.809016994368844, -0.999999999993381, -0.809016994370298,
    ;-0.309016994373312, 0.309016994372042, 0.809016994369185,
    ;0.999999999993381)"]
</code></pre>

<p>```</p>

<p>This has a bit more interesting result as an array. But the Context has ways of dealing with it.</p>

<p>```clojure
(.hasArrayElements result2) ;=> true
(.getArraySize result2) ;=> 11</p>

<p>(for [i (range 10)]
  (&ndash;> (.getArrayElement result2 i) (&ndash;>clojure)))
;=> (1.0 0.8090169943672489 0.3090169943721585 -0.3090169943735675
;-0.8090169943688436 -0.9999999999933814
; -0.8090169943702977 -0.3090169943733122 0.30901699437204233
; 0.8090169943691851)
```</p>

<p>So, we&rsquo;ve showed basic interop with R &ndash; which is pretty neat. What about Python?</p>

<h3>Interoping with Python</h3>

<p>Truffle is scheduled to fully support Python in 2018, but there is already an early alpha version in the Graal download that we can play with.</p>

<p><code>clojure
(.eval context "python" "
import time;
time.clock()
")
 ;=&gt; #object[org.graalvm.polyglot.Value 0x4a6b3b70 "1.508202803249E9"]
</code></p>

<p>Neat!</p>

<p>It is still a long way for <code>import numpy</code> or <code>import tensorflow</code> but cPython compatibility is the goal. Although the c-extensions are the really tricky part.</p>

<p>So keep an eye on Graal and Truffle for the future and wish the Oracle Labs team the best on their mission to make the JVM Polyglot.</p>

<h3>Footnotes</h3>

<p>If you are interested in playing with the code. I have a github repo here <a href="https://github.com/gigasquid/graal-test">graal-test</a>. If you are interested in watching a video, I really liked <a href="https://www.youtube.com/watch?v=TQMKPRc6cbE">this one</a>. There are also some really nice examples of running in polyglot mode with R and Java and JS here <a href="https://github.com/graalvm/examples">https://github.com/graalvm/examples</a>.</p>
]]></content>
  </entry>
  
</feed>
