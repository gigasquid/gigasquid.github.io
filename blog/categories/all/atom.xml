<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: All | Squid's Blog]]></title>
  <link href="http://gigasquid.github.io/blog/categories/all/atom.xml" rel="self"/>
  <link href="http://gigasquid.github.io/"/>
  <updated>2020-01-18T16:32:59-05:00</updated>
  <id>http://gigasquid.github.io/</id>
  <author>
    <name><![CDATA[Carin Meier]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Parens for Pyplot]]></title>
    <link href="http://gigasquid.github.io/blog/2020/01/18/parens-for-pyplot/"/>
    <updated>2020-01-18T15:39:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2020/01/18/parens-for-pyplot</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/cnuernber/libpython-clj">libpython-clj</a> has opened the door for Clojure to directly interop with Python libraries. That means we can take just about any Python library and directly use it in our Clojure REPL. But what about <a href="https://matplotlib.org/">matplotlib</a>?</p>

<p>Matplotlib.pyplot is a standard fixture in most tutorials and python data science code. How do we interop with a python graphics library?</p>

<h2>How do you interop?</h2>

<p>It turns out that matplotlib has a headless mode where we can export the graphics and then display it using any method that we would normally use to display a .png file. In my case, I made a quick macro for it using the shell <code>open</code>. I&rsquo;m sure that someone out that could improve upon it, (and maybe even make it a cool utility lib), but it suits what I&rsquo;m doing so far:</p>

<p>```clojure
ns gigasquid.plot
(:require [libpython-clj.require :refer [require-python]]
[libpython-clj.python :as py :refer [py. py.. py.&ndash;]]
[clojure.java.shell :as sh])</p>

<p>;;; This uses the headless version of matplotlib to generate a graph then copy it to the JVM
;; where we can then print it</p>

<p>;;;; have to set the headless mode before requiring pyplot
(def mplt (py/import-module &ldquo;matplotlib&rdquo;))
(py. mplt &ldquo;use&rdquo; &ldquo;Agg&rdquo;)</p>

<p>(require-python &lsquo;matplotlib.pyplot)
(require-python 'matplotlib.backends.backend_agg)
(require-python 'numpy)</p>

<p>(defmacro with-show
  &ldquo;Takes forms with mathplotlib.pyplot to then show locally&rdquo;
  [&amp; body]
  `(let [_# (matplotlib.pyplot/clf)</p>

<pre><code>     fig# (matplotlib.pyplot/figure)
     agg-canvas# (matplotlib.backends.backend_agg/FigureCanvasAgg fig#)]
 ~(cons 'do body)
 (py. agg-canvas# "draw")
 (matplotlib.pyplot/savefig "temp.png")
 (sh/sh "open" "temp.png")))
</code></pre>

<p>```</p>

<h2>Parens for Pyplot!</h2>

<p>Now that we have our wrapper let&rsquo;s take it for a spin. We&rsquo;ll be following along more or less this <a href="http://cs231n.github.io/python-numpy-tutorial/#matplotlib-plotting">tutorial for numpy plotting</a></p>

<p>For setup you will need the following installed in your python environment:</p>

<ul>
<li>numpy</li>
<li>matplotlib</li>
<li>pillow</li>
</ul>


<p>We are also going to use the latest and greatest syntax from libpython-clj so you are going to need to install the snapshot version locally until the next version goes out:</p>

<ul>
<li><code>git clone git@github.com:cnuernber/libpython-clj.git</code></li>
<li><code>cd cd libpython-clj</code></li>
<li><code>lein install</code></li>
</ul>


<p>After that is all setup we can require the libs we need in clojure.</p>

<p>```clojure
(ns gigasquid.numpy-plot
  (:require [libpython-clj.require :refer [require-python]]</p>

<pre><code>        [libpython-clj.python :as py :refer [py. py.. py.-]]
        [gigasquid.plot :as plot]))
</code></pre>

<p>```</p>

<p>The <code>plot</code> namespace contains the macro for <code>with-show</code> above. The <code>py.</code> and others is the new and improved syntax for interop.</p>

<h3>Simple Sin and Cos</h3>

<p>Let&rsquo;s start off with a simple sine and cosine functions. This code will create a <code>x</code> numpy vector of a range from 0 to <code>3 * pi</code> in 0.1 increments and then create <code>y</code> numpy vector of the <code>sin</code> of that and plot it</p>

<p>```clojure
(let [x (numpy/arange 0 (* 3 numpy/pi) 0.1)</p>

<pre><code>    y (numpy/sin x)]
(plot/with-show
  (matplotlib.pyplot/plot x y)))
</code></pre>

<p>```</p>

<p><img src="https://live.staticflickr.com/65535/49405284796_014447588d_z.jpg" alt="sin" /></p>

<p>Beautiful yes!</p>

<p>Let&rsquo;s get a bit more complicated now and and plot both the sin and cosine as well as add labels, title, and legend.</p>

<p>```clojure
(let [x (numpy/arange 0 (* 3 numpy/pi) 0.1)</p>

<pre><code>    y-sin (numpy/sin x)
    y-cos (numpy/cos x)]
(plot/with-show
  (matplotlib.pyplot/plot x y-sin)
  (matplotlib.pyplot/plot x y-cos)
  (matplotlib.pyplot/xlabel "x axis label")
  (matplotlib.pyplot/ylabel "y axis label")
  (matplotlib.pyplot/title "Sine and Cosine")
  (matplotlib.pyplot/legend ["Sine" "Cosine"])))
</code></pre>

<p>```</p>

<p><img src="http:////live.staticflickr.com/65535/49405284806_1d04957bce_z.jpg" alt="sin and cos" /></p>

<p>We can also add subplots. Subplots are when you divide the plots into different portions.
It is a bit stateful and involves making one subplot <em>active</em> and making changes and then making the other subplot <em>active</em>. Again not too hard to do with Clojure.</p>

<p>```clojure
(let [x (numpy/arange 0 (* 3 numpy/pi) 0.1)</p>

<pre><code>    y-sin (numpy/sin x)
    y-cos (numpy/cos x)]
(plot/with-show
  ;;; set up a subplot gird that has a height of 2 and width of 1
  ;; and set the first such subplot as active
  (matplotlib.pyplot/subplot 2 1 1)
  (matplotlib.pyplot/plot x y-sin)
  (matplotlib.pyplot/title "Sine")

  ;;; set the second subplot as active and make the second plot
  (matplotlib.pyplot/subplot 2 1 2)
  (matplotlib.pyplot/plot x y-cos)
  (matplotlib.pyplot/title "Cosine")))
</code></pre>

<p>```</p>

<p><img src="http:////live.staticflickr.com/65535/49405284836_8e49e4a6b8_z.jpg" alt="sin and cos subplots" /></p>

<h3>Plotting with Images</h3>

<p>Pyplot also has functions for working directly with images as well. Here we take a picture of my cat and create another version of it that is tinted.</p>

<p>```clojure
(let [img (matplotlib.pyplot/imread &ldquo;resources/cat.jpg&rdquo;)</p>

<pre><code>    img-tinted (numpy/multiply img [1 0.95 0.9])]
(plot/with-show
  (matplotlib.pyplot/subplot 1 2 1)
  (matplotlib.pyplot/imshow img)
  (matplotlib.pyplot/subplot 1 2 2)
  (matplotlib.pyplot/imshow (numpy/uint8 img-tinted))))
</code></pre>

<p>```</p>

<p><img src="http://live.staticflickr.com/65535/49404801993_ed398d5768_n.jpg" alt="cat tinted" /></p>

<h3>Pie charts</h3>

<p>Finally, we can show how to do a pie chart. I asked people in a <a href="https://twitter.com/gigasquid/status/1218358472049397761">twitter thread</a> what they wanted an example of in python interop and one of them was a pie chart. This is for you!</p>

<p>The original code for this example came from this <a href="https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html">tutorial</a>.</p>

<p>```clojure
(let [labels [&ldquo;Frogs&rdquo; &ldquo;Hogs&rdquo; &ldquo;Dogs&rdquo; &ldquo;Logs&rdquo;]</p>

<pre><code>    sizes [15 30 45 10]
    explode [0 0.1 0 0] ; only explode the 2nd slice (Hogs)
    ]
(plot/with-show
  (let [[fig1 ax1] (matplotlib.pyplot/subplots)]
    (py. ax1 "pie" sizes :explode explode :labels labels :autopct "%1.1f%%"
                         :shadow true :startangle 90)
    (py. ax1 "axis" "equal")) ;equal aspec ration ensures that pie is drawn as circle
  ))
</code></pre>

<p>```</p>

<p><img src="http://live.staticflickr.com/65535/49404802008_7e84ceff76_z.jpg" alt="pie chart" /></p>

<h3>Onwards and Upwards!</h3>

<p>This is just the beginning. In upcoming posts, I will be showcasing examples of interop with different libraries from the python ecosystem. Part of the goal is to get people used to how to use interop but also to raise awareness of the capabilities of the python libraries out there right now since they have been historically out of our ecosystem.</p>

<p>If you have any libraries that you would like examples of, I&rsquo;m taking requests. Feel free to leave them in the comments of the blog or in the <a href="https://twitter.com/gigasquid/status/1218358472049397761">twitter thread</a>.</p>

<p>Until next time, happy interoping!</p>

<p>PS All the code examples are here <a href="https://github.com/gigasquid/libpython-clj-examples">https://github.com/gigasquid/libpython-clj-examples</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hugging Face GPT With Clojure]]></title>
    <link href="http://gigasquid.github.io/blog/2020/01/10/hugging-face-gpt-with-clojure/"/>
    <updated>2020-01-10T19:33:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2020/01/10/hugging-face-gpt-with-clojure</id>
    <content type="html"><![CDATA[<p><img src="https://live.staticflickr.com/65535/49364554561_6e4f4d0a51_w.jpg" alt="" /></p>

<p>A new age in Clojure has dawned. We now have interop access to any python library with <a href="https://github.com/cnuernber/libpython-clj">libpython-clj</a>.</p>

<br>


<p>Let me pause a minute to repeat.</p>

<br>


<p><strong> You can now interop with ANY python library. </strong></p>

<br>


<p>I know. It&rsquo;s overwhelming. It took a bit for me to come to grips with it too.</p>

<br>


<p>Let&rsquo;s take an example of something that I&rsquo;ve <em>always</em> wanted to do and have struggled with mightly finding a way to do it in Clojure:<br/>
I want to use the latest cutting edge GPT2 code out there to generate text.</p>

<p>Right now, that library is <a href="https://github.com/huggingface/transformers">Hugging Face Transformers</a>.</p>

<br>


<p>Get ready. We will wrap that sweet hugging face code in Clojure parens!</p>

<h3>The setup</h3>

<p>The first thing you will need to do is to have python3 installed and the two libraries that we need:</p>

<br>


<ul>
<li>pytorch &ndash; <code>sudo pip3 install torch</code></li>
<li>hugging face transformers &ndash; <code>sudo pip3 install transformers</code></li>
</ul>


<br>


<p>Right now, some of you may not want to proceed. You might have had a bad relationship with Python in the past. It&rsquo;s ok, remember that some of us had bad relationships with Java, but still lead a happy and fulfilled life with Clojure and still can enjoy it from interop. The same is true with Python. Keep an open mind.</p>

<br>


<p>There might be some others that don&rsquo;t want to have anything to do with Python and want to keep your Clojure pure. Well, that is a valid choice. But you are missing out on what the big, vibrant, and chaotic Python Deep Learning ecosystem has to offer.</p>

<br>


<p>For those of you that are still along for the ride, let&rsquo;s dive in.</p>

<br>


<p>Your deps file should have just a single extra dependency in it:</p>

<p>```clojure
:deps {org.clojure/clojure {:mvn/version &ldquo;1.10.1&rdquo;}</p>

<pre><code>    cnuernber/libpython-clj {:mvn/version "1.30"}}
</code></pre>

<p>```</p>

<h3>Diving Into Interop</h3>

<p>The first thing that we need to do is require the libpython library.</p>

<p>```clojure
(ns gigasquid.gpt2
  (:require [libpython-clj.require :refer [require-python]]</p>

<pre><code>        [libpython-clj.python :as py]))
</code></pre>

<p>```</p>

<p>It has a very nice <code>require-python</code> syntax that we will use to load the python libraries so that we can use them in our Clojure code.</p>

<p><code>clojure
(require-python '(transformers))
(require-python '(torch))
</code></p>

<p>Here we are going to follow along with the OpenAI GPT-2 tutorial and translate it into interop code.
The original tutorial is <a href="https://huggingface.co/transformers/quickstart.html">here</a></p>

<br>


<p>Let&rsquo;s take the python side first:</p>

<p>```python
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel</p>

<h1>Load pre-trained model tokenizer (vocabulary)</h1>

<p>tokenizer = GPT2Tokenizer.from_pretrained(&lsquo;gpt2&rsquo;)
```</p>

<p>This is going to translate in our interop code to:</p>

<p><code>clojure
(def tokenizer (py/$a transformers/GPT2Tokenizer from_pretrained "gpt2"))
</code></p>

<p>The <code>py/$a</code> function is used to call attributes on a Python object. We get the <code>transformers/GPTTokenizer</code> object that we have available to use and call <code>from_pretrained</code> on it with the string argument <code>"gpt2"</code></p>

<br>


<p>Next in the Python tutorial is:</p>

<p>```python</p>

<h1>Encode a text inputs</h1>

<p>text = &ldquo;Who was Jim Henson ? Jim Henson was a&rdquo;
indexed_tokens = tokenizer.encode(text)</p>

<h1>Convert indexed tokens in a PyTorch tensor</h1>

<p>tokens_tensor = torch.tensor([indexed_tokens])
```</p>

<p>This is going to translate to Clojure:</p>

<p>```clojure
(def text &ldquo;Who was Jim Henson ? Jim Henson was a&rdquo;)
;; encode text input
(def indexed-tokens  (py/$a tokenizer encode text))
indexed-tokens ;=>[8241, 373, 5395, 367, 19069, 5633, 5395, 367, 19069, 373, 257]</p>

<p>;; convert indexed tokens to pytorch tensor
(def tokens-tensor (torch/tensor [indexed-tokens]))
tokens-tensor
;; ([[ 8241,   373,  5395,   367, 19069,  5633,  5395,   367, 19069,   373,
;;    257]])
```</p>

<p>Here we are again using <code>py/$a</code> to call the <code>encode</code> method on the text. However, when we are just calling a function, we can do so directly with <code>(torch/tensor [indexed-tokens])</code>. We can even directly use vectors.</p>

<br>


<p>Again, you are doing this in the REPL, so you have full power for inspection and display of the python objects. It is a great interop experience &ndash; (cider even has doc information on the python functions in the minibuffer)!</p>

<br>


<p>The next part is to load the model itself. This will take a few minutes, since it has to download a big file from s3 and load it up.</p>

<br>


<p>In Python:</p>

<p>```python</p>

<h1>Load pre-trained model (weights)</h1>

<p>model = GPT2LMHeadModel.from_pretrained(&lsquo;gpt2&rsquo;)
```</p>

<p>In Clojure:</p>

<p><code>clojure
;;; Load pre-trained model (weights)
;;; Note: this will take a few minutes to download everything
(def model (py/$a transformers/GPT2LMHeadModel from_pretrained "gpt2"))
</code></p>

<p>The next part is to run the model with the tokens and make the predictions.</p>

<br>


<p>Here the code starts to diverge a tiny bit.</p>

<br>


<p>Python:</p>

<p>```python</p>

<h1>Set the model in evaluation mode to deactivate the DropOut modules</h1>

<h1>This is IMPORTANT to have reproducible results during evaluation!</h1>

<p>model.eval()</p>

<h1>If you have a GPU, put everything on cuda</h1>

<p>tokens_tensor = tokens_tensor.to(&lsquo;cuda&rsquo;)
model.to(&lsquo;cuda&rsquo;)</p>

<h1>Predict all tokens</h1>

<p>with torch.no_grad():</p>

<pre><code>outputs = model(tokens_tensor)
predictions = outputs[0]
</code></pre>

<h1>get the predicted next sub-word (in our case, the word &lsquo;man&rsquo;)</h1>

<p>predicted_index = torch.argmax(predictions[0, -1, :]).item()
predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])
assert predicted_text == &lsquo;Who was Jim Henson? Jim Henson was a man&rsquo;
```</p>

<p>And Clojure</p>

<p>```clojure
;;; Set the model in evaluation mode to deactivate the DropOut modules
;;; This is IMPORTANT to have reproducible results during evaluation!
(py/$a model eval)</p>

<p>;;; Predict all tokens
(def predictions (py/with [r (torch/no_grad)]</p>

<pre><code>                      (first (model tokens-tensor))))
</code></pre>

<p>;;; get the predicted next sub-word"
(def predicted-index (let [last-word-predictions (&ndash;> predictions first last)</p>

<pre><code>                       arg-max (torch/argmax last-word-predictions)]
                   (py/$a arg-max item)))
</code></pre>

<p>predicted-index ;=>582</p>

<p>(py/$a tokenizer decode (&ndash;> (into [] indexed-tokens)</p>

<pre><code>                        (conj predicted-index)))
</code></pre>

<p>;=> &ldquo;Who was Jim Henson? Jim Henson was a man&rdquo;
```</p>

<p>The main differences is that we are obviously not using the python array syntax in our code to manipulate the lists. For example, instead of using <code>outputs[0]</code>, we are going to use <code>(first outputs)</code>. But, other than that, it is a pretty good match, even with the <code>py/with</code>.</p>

<p>Also note that we are not making the call to configure it with GPU. This is intentionally left out to keep things simple for people to try it out. Sometimes, GPU configuration can be a bit tricky to set up depending on your system. For this example, you definitely won&rsquo;t need it since it runs fast enough on cpu. If you do want to do something more complicated later, like fine tuning, you will need to invest some time to get it set up.</p>

<h3>Doing Longer Sequences</h3>

<p>The next example in the tutorial goes on to cover generating longer text.</p>

<br>


<p>Python</p>

<p>```python
tokenizer = GPT2Tokenizer.from_pretrained(&ldquo;gpt2&rdquo;)
model = GPT2LMHeadModel.from_pretrained(&lsquo;gpt2&rsquo;)</p>

<p>generated = tokenizer.encode(&ldquo;The Manhattan bridge&rdquo;)
context = torch.tensor([generated])
past = None</p>

<p>for i in range(100):</p>

<pre><code>print(i)
output, past = model(context, past=past)
token = torch.argmax(output[0, :])

generated += [token.tolist()]
context = token.unsqueeze(0)
</code></pre>

<p>sequence = tokenizer.decode(generated)</p>

<p>print(sequence)</p>

<p>```</p>

<p>And Clojure</p>

<p>```clojure
(def tokenizer (py/$a transformers/GPT2Tokenizer from_pretrained &ldquo;gpt2&rdquo;))
(def model (py/$a transformers/GPT2LMHeadModel from_pretrained &ldquo;gpt2&rdquo;))</p>

<p>(def generated (into [] (py/$a tokenizer encode &ldquo;The Manhattan bridge&rdquo;)))
(def context (torch/tensor [generated]))</p>

<p>(defn generate-sequence-step [{:keys [generated-tokens context past]}]
  (let [[output past] (model context :past past)</p>

<pre><code>    token (-&gt; (torch/argmax (first output)))
    new-generated  (conj generated-tokens (py/$a token tolist))]
{:generated-tokens new-generated
 :context (py/$a token unsqueeze 0)
 :past past
 :token token}))
</code></pre>

<p>(defn decode-sequence [{:keys [generated-tokens]}]
  (py/$a tokenizer decode generated-tokens))</p>

<p>(loop [step {:generated-tokens generated</p>

<pre><code>         :context context
         :past nil}
   i 10]
</code></pre>

<p>  (if (pos? i)</p>

<pre><code>(recur (generate-sequence-step step) (dec i))
(decode-sequence step)))
</code></pre>

<p>;=> &ldquo;The Manhattan bridge\n\nThe Manhattan bridge is a major artery for&rdquo;
```</p>

<p>The great thing is once we have it embedded in our code, there is no stopping. We can create a nice function:</p>

<p>```clojure
(defn generate-text [starting-text num-of-words-to-predict]
  (let [tokens (into [] (py/$a tokenizer encode starting-text))</p>

<pre><code>    context (torch/tensor [tokens])
    result (reduce
            (fn [r i]
              (println i)
              (generate-sequence-step r))

            {:generated-tokens tokens
             :context context
             :past nil}

            (range num-of-words-to-predict))]
(decode-sequence result)))
</code></pre>

<p>```</p>

<p>And finally we can generate some fun text!</p>

<p>```clojure
(generate-text &ldquo;Clojure is a dynamic, general purpose programming language, combining the approachability and interactive&rdquo; 20)</p>

<p>;=> &ldquo;Clojure is a dynamic, general purpose programming language, combining the approachability and interactive. It is a language that is easy to learn and use, and is easy to use for anyone&rdquo;
```</p>

<p><strong>Clojure is a dynamic, general purpose programming language, combining the approachability and interactive. It is a language that is easy to learn and use, and is easy to use for anyone</strong></p>

<br>


<p>So true GPT2! So true!</p>

<h3>Wrap-up</h3>

<p>libpython-clj is a really powerful tool that will allow Clojurists to better explore, leverage, and integrate Python libraries into their code.</p>

<br>


<p>I&rsquo;ve been really impressed with it so far and I encourage you to check it out.</p>

<br>


<p>There is a <a href="https://github.com/gigasquid/libpython-clj-examples">repo with the examples</a> out there if you want to check them out. There is also an example of doing MXNet MNIST classification there as well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Integrating Deep Learning With clojure.spec]]></title>
    <link href="http://gigasquid.github.io/blog/2019/10/11/integrating-deep-learning-with-clojure-dot-spec/"/>
    <updated>2019-10-11T13:51:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2019/10/11/integrating-deep-learning-with-clojure-dot-spec</id>
    <content type="html"><![CDATA[<p>clojure.spec allows you to write specifications for data and use them for validation. It also provides a generative aspect that allows for robust testing as well as an additional way to understand your data through manual inspection. The dual nature of validation and generation is a natural fit for deep learning models that consist of paired discriminator/generator models.</p>

<br>


<p><strong><strong>TLDR: In this post we show that you can leverage the dual nature of clojure.spec&rsquo;s validator/generator to incorporate a deep learning model&rsquo;s classifier/generator.</strong></strong></p>

<br>


<p>A common use of clojure.spec is at the boundaries to validate that incoming data is indeed in the expected form. Again, this is boundary is a fitting place to integrate models for the deep learning paradigm and our traditional software code.</p>

<p>Before we get into the deep learning side of things, let&rsquo;s take a quick refresher on how to use clojure.spec.</p>

<h2>quick view of clojure.spec</h2>

<p>To create a simple spec for keywords that are cat sounds, we can use <code>s/def</code>.</p>

<p><code>clojure
(s/def ::cat-sounds #{:meow :purr :hiss})
</code></p>

<p>To do the validation, you can use the <code>s/valid?</code> function.</p>

<p><code>clojure
(s/valid? ::cat-sounds :meow) ;=&gt; true
(s/valid? ::cat-sounds :bark) ;=&gt; false
</code></p>

<p>For the generation side of things, we can turn the spec into generator and sample it.</p>

<p><code>clojure
(gen/sample (s/gen ::cat-sounds))
;=&gt;(:hiss :hiss :hiss :meow :meow :purr :hiss :meow :meow :meow)
</code></p>

<p>There is the ability to compose specs by adding them together with <code>s/and</code>.</p>

<p><code>clojure
(s/def ::even-number (s/and int? even?))
(gen/sample (s/gen ::even-number))
;=&gt; (0 0 -2 2 0 10 -4 8 6 8)
</code></p>

<p>We can also control the generation by creating a custom generator using <code>s/with-gen</code>.
In the following the spec is only that the data be a general string, but using the custom generator, we can restrict the output to only be a certain set of example cat names.</p>

<p>```clojure
(s/def ::cat-name
  (s/with-gen</p>

<pre><code>string?
#(s/gen #{"Suki" "Bill" "Patches" "Sunshine"})))
</code></pre>

<p>(s/valid? ::cat-name &ldquo;Peaches&rdquo;) ;=> true
(gen/sample (s/gen ::cat-name))
;; (&ldquo;Patches&rdquo; &ldquo;Sunshine&rdquo; &ldquo;Sunshine&rdquo; &ldquo;Suki&rdquo; &ldquo;Suki&rdquo; &ldquo;Sunshine&rdquo;
;;  &ldquo;Suki&rdquo; &ldquo;Patches&rdquo; &ldquo;Sunshine&rdquo; &ldquo;Suki&rdquo;)
```</p>

<p>For further information on clojure.spec, I whole-heartedly recommend the <a href="https://clojure.org/guides/spec">spec Guide</a>. But, now with a basic overview of spec, we can move on to creating specs for our Deep Learning models.</p>

<h2>Creating specs for Deep Learning Models</h2>

<p>In previous posts, we covered making <a href="https://gigasquidsoftware.com/blog/2019/08/16/simple-autoencoder/">simple autoencoders for handwritten digits</a>.</p>

<p><img src="http://live.staticflickr.com/65535/48647524478_ca35bef78f_n.jpg" alt="handwritten digits" /></p>

<p>Then, we made models that would:</p>

<ul>
<li>Take an image of a digit and give you back the string value (ex: &ldquo;2&rdquo;) &ndash; <a href="https://gigasquidsoftware.com/blog/2019/08/30/focus-on-the-discriminator/">post</a></li>
<li>Take a string number value and give you back a digit image. &ndash; <a href="https://gigasquidsoftware.com/blog/2019/09/06/focus-on-the-generator/">post</a></li>
</ul>


<p>We will use both of the models to make a spec with a custom generator.</p>

<br>


<p><em>Note: For the sake of simplicity, some of the supporting code is left out. But if you want to see the whole code, it is on <a href="(https://github.com/gigasquid/clojure-mxnet-autoencoder/blob/master/src/clojure_mxnet_autoencoder/model_specs.clj">github</a>)</em></p>

<br>


<p>With the help of the trained discriminator model, we can make a function that takes in an image and returns the number string value.</p>

<p>```clojure
(defn discriminate [image]
  (&ndash;> (m/forward discriminator-model {:data [image]})</p>

<pre><code>  (m/outputs)
  (ffirst)
  (ndarray/argmax-channel)
  (ndarray/-&gt;vec)
  (first)
  (int)))
</code></pre>

<p>```</p>

<p>Let&rsquo;s test it out with a test-image:</p>

<p><img src="http://live.staticflickr.com/65535/48881532151_251e30840e_s.jpg" alt="test-discriminator-image" /></p>

<p><code>clojure
(discriminate my-test-image) ;=&gt; 6
</code></p>

<p>Likewise, with the trained generator model, we can make a function that takes a string number and returns the corresponding image.</p>

<p>```clojure
(defn generate [label]
  (&ndash;> (m/forward generator-model {:data [(ndarray/array [label] [batch-size])]})</p>

<pre><code>  (m/outputs)
  (ffirst)))
</code></pre>

<p>```</p>

<p>Giving it a test drive as well:</p>

<p>```clojure
(def generated-test-image (generate 3))
(viz/im-sav {:title &ldquo;generated-image&rdquo;</p>

<pre><code>         :output-path "results/"
         :x (ndarray/reshape generated-test-image [batch-size 1 28 28])})
</code></pre>

<p>```</p>

<p><img src="http://live.staticflickr.com/65535/48881532451_023de68ddb_s.jpg" alt="generated-test-image" /></p>

<p>Great! Let&rsquo;s go ahead and start writing specs. First let&rsquo;s make a quick spec to describe a MNIST number &ndash; which is a single digit between 0 and 9.</p>

<p><code>clojure
(s/def ::mnist-number (s/and int? #(&lt;= 0 % 9)))
(s/valid? ::mnist-number 3) ;=&gt; true
(s/valid? ::mnist-number 11) ;=&gt; false
(gen/sample (s/gen ::mnist-number))
;=&gt; (0 1 0 3 5 3 7 5 0 1)
</code></p>

<p>We now have both parts to validate and generate and can create a spec for it.</p>

<p>```clojure
(s/def ::mnist-image</p>

<pre><code>(s/with-gen
  #(s/valid? ::mnist-number (discriminate %))
  #(gen/fmap (fn [n]
               (do (ndarray/copy (generate n))))
             (s/gen ::mnist-number))))
</code></pre>

<p>```</p>

<p>The <code>::mnist-number</code> spec is used for the validation after the <code>discriminate</code> model is used. On the generator side, we use the generator for the <code>::mnist-number</code> spec and feed that into the deep learning generator model to get sample images.</p>

<p>We have a test function that will help us test out this new spec, called <code>test-model-spec</code>. It will return a map with the following form:</p>

<p><code>
{:spec name-of-the-spec
 :valid? whether or not the `s/valid?` called on the test value is true or not
 :sample-values This calls the discriminator model on the generated values
 }
</code>
It will also write an image of all the sample images to a file named <code>sample-spec-name</code></p>

<p>Let&rsquo;s try it on our test image:</p>

<p><img src="http://live.staticflickr.com/65535/48881532151_251e30840e_s.jpg" alt="test-discriminator-image" /></p>

<p>```clojure
(s/valid? ::mnist-image my-test-image) ;=> true</p>

<p>(test-model-spec ::mnist-image my-test-image)
;; {:spec &ldquo;mnist-image&rdquo;
;;  :valid? true
;;  :sample-values [0 0 0 1 3 1 0 2 7 3]}
```</p>

<p><img src="http://live.staticflickr.com/65535/48882235262_1e0dd7b758_q.jpg" alt="sample-mnist-image" /></p>

<p>Pretty cool!</p>

<p>Let&rsquo;s do some more specs. But first, our spec is going to be a bit repetitive, so we&rsquo;ll make a quick macro to make things easier.</p>

<p>```clojure
(defmacro def-model-spec [spec-key spec discriminate-fn generate-fn]</p>

<pre><code>`(s/def ~spec-key
   (s/with-gen
     #(s/valid? ~spec (~discriminate-fn %))
     #(gen/fmap (fn [n#]
                  (do (ndarray/copy (~generate-fn n#))))
                (s/gen ~spec)))))
</code></pre>

<p>```</p>

<h3>More Specs &ndash; More Fun</h3>

<p>This time let&rsquo;s define an even mnist image spec</p>

<p>```clojure
 (def-model-spec ::even-mnist-image</p>

<pre><code>(s/and ::mnist-number even?)
discriminate
generate)
</code></pre>

<p>  (test-model-spec ::even-mnist-image my-test-image)</p>

<p>  ;; {:spec &ldquo;even-mnist-image&rdquo;
  ;;  :valid? true
  ;;  :sample-values [0 0 2 0 8 2 2 2 0 0]}
```</p>

<p><img src="http://live.staticflickr.com/65535/48882253157_02e45d3132_q.jpg" alt="sample-even-mnist-image" /></p>

<p>And Odds</p>

<p>```clojure
  (def-model-spec ::odd-mnist-image</p>

<pre><code>(s/and ::mnist-number odd?)
discriminate
generate)
</code></pre>

<p>  (test-model-spec ::odd-mnist-image my-test-image)</p>

<p>  ;; {:spec &ldquo;odd-mnist-image&rdquo;
  ;;  :valid? false
  ;;  :sample-values [5 1 5 1 3 3 3 1 1 1]}
```
<img src="http://live.staticflickr.com/65535/48881548138_c18850f806_q.jpg" alt="sample-odd-mnist-image" /></p>

<p>Finally, let&rsquo;s do Odds that are over 2!</p>

<p>```clojure
  (def-model-spec ::odd-over-2-mnist-image</p>

<pre><code>(s/and ::mnist-number odd? #(&gt; % 2))
discriminate
generate)
</code></pre>

<p>  (test-model-spec ::odd-over-2-mnist-image my-test-image)</p>

<p>  ;; {:spec &ldquo;odd-over-2-mnist-image&rdquo;
  ;;  :valid? false
  ;;  :sample-values [3 3 3 5 3 5 7 7 7 3]}
```</p>

<p><img src="http://live.staticflickr.com/65535/48882089776_6f55416418_q.jpg" alt="sample-odd-over-2-mnist-image" /></p>

<h2>Conclusion</h2>

<p>We have shown some of the potential of integrating deep learning models with Clojure. clojure.spec is a powerful tool and it can be leveraged in new and interesting ways for both deep learning and AI more generally.</p>

<p>I hope that more people are intrigued to experiment and take a further look into what we can do in this area.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Focus on the Generator]]></title>
    <link href="http://gigasquid.github.io/blog/2019/09/06/focus-on-the-generator/"/>
    <updated>2019-09-06T18:07:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2019/09/06/focus-on-the-generator</id>
    <content type="html"><![CDATA[<p><a data-flickr-embed="true"  href="https://www.flickr.com/photos/smigla-bobinski/19705409981/in/album-72157647756733695/" title="SIMULACRA by Karina Smigla-Bobinski"><img src="https://live.staticflickr.com/330/19705409981_4e0ae93572.jpg" width="500" height="267" alt="SIMULACRA by Karina Smigla-Bobinski"></a><script async src="http://gigasquid.github.io//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script></p>

<p>In this first post of this series, we took a look at a <a href="https://gigasquidsoftware.com/blog/2019/08/16/simple-autoencoder/">simple autoencoder</a>. It took and image and transformed it back to an image. Then, we <a href="https://gigasquidsoftware.com/blog/2019/08/30/focus-on-the-discriminator/">focused in on the disciminator</a> portion of the model, where we took an image and transformed it to a label. Now, we focus in on the generator portion of the model do the inverse operation: we transform a label to an image. In recap:</p>

<ul>
<li>Autoencoder: image &ndash;> image</li>
<li>Discriminator: image &ndash;> label</li>
<li>Generator: label &ndash;> image (This is what we are doing now!)</li>
</ul>


<p><img src="https://live.staticflickr.com/65535/48689260086_11fe4b089b_b.jpg" alt="generator" /></p>

<h2>Still Need Data of Course</h2>

<p>Nothing changes here. We are still using the MNIST handwritten digit set and have an input and out to our model.</p>

<p>```clojure
(def
  train-data
  (mx-io/mnist-iter {:image (str data-dir &ldquo;train-images-idx3-ubyte&rdquo;)</p>

<pre><code>                 :label (str data-dir "train-labels-idx1-ubyte")
                 :input-shape [784]
                 :flat true
                 :batch-size batch-size
                 :shuffle true}))
</code></pre>

<p>(def
  test-data (mx-io/mnist-iter</p>

<pre><code>         {:image (str data-dir "t10k-images-idx3-ubyte")
          :label (str data-dir "t10k-labels-idx1-ubyte")
          :input-shape [784]
          :batch-size batch-size
          :flat true
          :shuffle true}))
</code></pre>

<p>(def input (sym/variable &ldquo;input&rdquo;))
(def output (sym/variable &ldquo;input_&rdquo;))
```</p>

<h2>The Generator Model</h2>

<p>The model does change to one hot encode the label for the number. Other than that, it&rsquo;s pretty much the exact same second half of the autoencoder model.</p>

<p>```clojure
(defn get-symbol []
  (as-> input data</p>

<pre><code>(sym/one-hot "onehot" {:indices data :depth 10})
;; decode
(sym/fully-connected "decode1" {:data data :num-hidden 50})
(sym/activation "sigmoid3" {:data data :act-type "sigmoid"})

;; decode
(sym/fully-connected "decode2" {:data data :num-hidden 100})
(sym/activation "sigmoid4" {:data data :act-type "sigmoid"})

;;output
(sym/fully-connected "result" {:data data :num-hidden 784})
(sym/activation "sigmoid5" {:data data :act-type "sigmoid"})

(sym/linear-regression-output {:data data :label output})))
</code></pre>

<p>(def data-desc
  (first
   (mx-io/provide-data-desc train-data)))
(def label-desc
  (first
   (mx-io/provide-label-desc train-data)))
```</p>

<p>When binding the shapes to the model, we now need to specify that the input data shapes is the label instead of the image and the output of the model is going to be the image.</p>

<p>```clojure
(def
  model
  ;;; change data shapes to label shapes
  (&ndash;> (m/module (get-symbol) {:data-names [&ldquo;input&rdquo;] :label-names [&ldquo;input_&rdquo;]})</p>

<pre><code>  (m/bind {:data-shapes [(assoc label-desc :name "input")]
           :label-shapes [(assoc data-desc :name "input_")]})
  (m/init-params {:initializer  (initializer/uniform 1)})
  (m/init-optimizer {:optimizer (optimizer/adam {:learning-rage 0.001})})))
</code></pre>

<p>(def my-metric (eval-metric/mse))
```</p>

<h2>Training</h2>

<p>The training of the model is pretty straight forward. Just being mindful that we are using hte batch-label, (number label),  as the input and and validating with the batch-data, (image).</p>

<p>```clojure
(defn train [num-epochs]
  (doseq [epoch-num (range 0 num-epochs)]</p>

<pre><code>(println "starting epoch " epoch-num)
(mx-io/do-batches
 train-data
 (fn [batch]
   ;;; change input to be the label
   (-&gt; model
       (m/forward {:data (mx-io/batch-label batch)
                   :label (mx-io/batch-data batch)})
       (m/update-metric my-metric (mx-io/batch-data batch))
       (m/backward)
       (m/update))))
(println "result for epoch " epoch-num " is "
         (eval-metric/get-and-reset my-metric))))
</code></pre>

<p>```</p>

<h2>Results Before Training</h2>

<p>```clojure
(def my-test-batch (mx-io/next test-data))
  ;;; change to input labels
  (def test-labels (mx-io/batch-label my-test-batch))
  (def preds (m/predict-batch model {:data test-labels} ))
  (viz/im-sav {:title &ldquo;before-training-preds&rdquo;</p>

<pre><code>           :output-path "results/"
           :x (ndarray/reshape (first preds) [100 1 28 28])})
</code></pre>

<p>  (&ndash;>> test-labels first ndarray/&ndash;>vec (take 10))
  ;=> (6.0 1.0 0.0 0.0 3.0 1.0 4.0 8.0 0.0 9.0)
```</p>

<p><img src="http://live.staticflickr.com/65535/48689304281_a41bf39353.jpg" alt="before training" /></p>

<p>Not very impressive&hellip; Let&rsquo;s train</p>

<p>```clojure
(train 3)</p>

<p>starting epoch  0
result for epoch  0  is<br/>
[mse 0.0723091]
starting epoch  1
result for epoch  1  is  [mse 0.053891845]
starting epoch  2
result for epoch  2  is  [mse 0.05337505]
```</p>

<h2>Results After Training</h2>

<p>```clojure
 (def my-test-batch (mx-io/next test-data))
  (def test-labels (mx-io/batch-label my-test-batch))
  (def preds (m/predict-batch model {:data test-labels}))
  (viz/im-sav {:title &ldquo;after-training-preds&rdquo;</p>

<pre><code>           :output-path "results/"
           :x (ndarray/reshape (first preds) [100 1 28 28])})
</code></pre>

<p>  (&ndash;>> test-labels first ndarray/&ndash;>vec (take 10))</p>

<p>  ;=>   (9.0 5.0 7.0 1.0 8.0 6.0 6.0 0.0 8.0 1.0)
```</p>

<p><img src="https://live.staticflickr.com/65535/48689328481_338416ba7c.jpg" alt="after training" /></p>

<p>Cool! The first row is indeed</p>

<p><code>(9.0 5.0 7.0 1.0 8.0 6.0 6.0 0.0 8.0 1.0)</code></p>

<h2>Save Your Model</h2>

<p>Don&rsquo;t forget to save the generator model off &ndash; we are going to use it next time.</p>

<p><code>clojure
(m/save-checkpoint model {:prefix "model/generator" :epoch 2})
</code></p>

<p>Happy Deep Learning until next time &hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Focus on the Discriminator]]></title>
    <link href="http://gigasquid.github.io/blog/2019/08/30/focus-on-the-discriminator/"/>
    <updated>2019-08-30T10:16:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2019/08/30/focus-on-the-discriminator</id>
    <content type="html"><![CDATA[<p><a data-flickr-embed="true"  href="https://www.flickr.com/photos/marcomagrini/698692268/in/photolist-24JYSq-hTTAJN-4gjQW9-9GRKCW-4gfNhz-x2yZ-6Nnwy1-6Lm68p-66BVjW-8hawRk-4sE2Jz-5Z6uvQ-6B4iH3-qzDvGU-aNpvLT-9UFZLh-egKvNt-bMh6PR-ceG9AL-gDqtze-96JhRW-7EWMH6-3MTfDt-9rUJ4W-dFPssj-8LLrys-aDAda3-9rUJ45-7xLAFR-prSHik-7yDFHC-7erqEc-6YJx8e-39SyR4-dkQnGi-7hy6zT-4UokrH-hkMoBr-9tBN3K-jq8Bpu-aDMSk2-pwQdmt-9tFrUD-6TzF6G-WDAsCC-8Mm4tD-8M8hyS-4yzkGK-67MPUw-crfg" title="sunflowers"><img src="https://live.staticflickr.com/1007/698692268_b31d429272.jpg" width="500" height="325" alt="sunflowers"></a><script async src="http://gigasquid.github.io//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script></p>

<p>In the <a href="https://gigasquidsoftware.com/blog/2019/08/16/simple-autoencoder/">last post</a>, we took a look at a simple autoencoder. The autoencoder is a deep learning model that takes in an image and, (through an encoder and decoder), works to produce the same image. In short:</p>

<ul>
<li>Autoencoder: image &ndash;> image</li>
</ul>


<p>For a discriminator, we are going to focus on only the first half on the autoencoder.</p>

<p><img src="https://live.staticflickr.com/65535/48647347383_9577b7b672_b.jpg" alt="discriminator" /></p>

<p>Why only half? We want a different transformation. We are going to want to take an image as input and then do some <em>discrimination</em> of the image and classify what type of image it is. In our case, the model is going to input an image of a handwritten digit and attempt to decide which number it is.</p>

<ul>
<li>Discriminator: image &ndash;> label</li>
</ul>


<p>As always, with deep learning. To do anything, we need data.</p>

<h3>MNIST Data</h3>

<p>Nothing changes here from the autoencoder code. We are still using the MNIST dataset for handwritten digits.</p>

<p>```clojure
;;; Load the MNIST datasets
(def train-data
  (mx-io/mnist-iter
   {:image (str data-dir &ldquo;train-images-idx3-ubyte&rdquo;)</p>

<pre><code>:label (str data-dir "train-labels-idx1-ubyte")
:input-shape [784]
:flat true
:batch-size batch-size
:shuffle true}))
</code></pre>

<p>(def test-data
  (mx-io/mnist-iter
   {:image (str data-dir &ldquo;t10k-images-idx3-ubyte&rdquo;)</p>

<pre><code>:label (str data-dir "t10k-labels-idx1-ubyte")
:input-shape [784]
:batch-size batch-size
:flat true
:shuffle true}))
</code></pre>

<p>```</p>

<p>The model will change since we want a different output.</p>

<h3>The Model</h3>

<p>We are still taking in the image as input, and using the same encoder layers from the autoencoder model. However, at the end, we use a fully connected layer that has 10 hidden nodes &ndash; one for each label of the digits 0-9. Then we use a softmax for the classification output.</p>

<p>```clojure
(def input (sym/variable &ldquo;input&rdquo;))
(def output (sym/variable &ldquo;input_&rdquo;))</p>

<p>(defn get-symbol []
  (as-> input data</p>

<pre><code>;; encode
(sym/fully-connected "encode1" {:data data :num-hidden 100})
(sym/activation "sigmoid1" {:data data :act-type "sigmoid"})

;; encode
(sym/fully-connected "encode2" {:data data :num-hidden 50})
(sym/activation "sigmoid2" {:data data :act-type "sigmoid"})

;;; this last bit changed from autoencoder
;;output
(sym/fully-connected "result" {:data data :num-hidden 10})
(sym/softmax-output {:data data :label output})))
</code></pre>

<p>```</p>

<p>In the autoencoder, we were never actually using the label, but we will certainly need to use it this time. It is reflected in the model&rsquo;s bindings with the data and label shapes.</p>

<p>```clojure
(def model (&ndash;> (m/module (get-symbol) {:data-names [&ldquo;input&rdquo;] :label-names [&ldquo;input_&rdquo;]})</p>

<pre><code>           (m/bind {:data-shapes [(assoc data-desc :name "input")]
                    :label-shapes [(assoc label-desc :name "input_")]})
           (m/init-params {:initializer (initializer/uniform 1)})
           (m/init-optimizer {:optimizer (optimizer/adam {:learning-rage 0.001})})))
</code></pre>

<p>```</p>

<p>For the evaluation metric, we are also going to use an accuracy metric vs a mean squared error (mse) metric</p>

<p><code>clojure
(def my-metric (eval-metric/accuracy))
</code></p>

<p>With these items in place, we are ready to train the model.</p>

<h3>Training</h3>

<p>The training from the autoencoder needs to changes to use the real label for the the forward pass and updating the metric.</p>

<p>```clojure
(defn train [num-epochs]
  (doseq [epoch-num (range 0 num-epochs)]</p>

<pre><code>(println "starting epoch " epoch-num)
(mx-io/do-batches
 train-data
 (fn [batch]
   ;;; here we make sure to use the label
   ;;; now for forward and update-metric
   (-&gt; model
       (m/forward {:data (mx-io/batch-data batch)
                   :label (mx-io/batch-label batch)})
       (m/update-metric my-metric (mx-io/batch-label batch))
       (m/backward)
       (m/update))))
(println {:epoch epoch-num
          :metric (eval-metric/get-and-reset my-metric)})))
</code></pre>

<p>```</p>

<h3>Let&rsquo;s Run Things</h3>

<p>It&rsquo;s always a good idea to take a look at things before you start training.</p>

<p>The first batch of the training data looks like:</p>

<p>```clojure
  (def my-batch (mx-io/next train-data))
  (def images (mx-io/batch-data my-batch))
  (viz/im-sav {:title &ldquo;originals&rdquo;</p>

<pre><code>           :output-path "results/"
           :x (-&gt; images
                  first
                  (ndarray/reshape [100 1 28 28]))})
</code></pre>

<p>```</p>

<p><img src="https://live.staticflickr.com/65535/48648000857_fb17f0de66.jpg" alt="training-batch" /></p>

<p>Before training, if we take the first batch from the test data and predict what the labels are:</p>

<p>```clojure
  (def my-test-batch (mx-io/next test-data))
  (def test-images (mx-io/batch-data my-test-batch))
  (viz/im-sav {:title &ldquo;test-images&rdquo;</p>

<pre><code>           :output-path "results/"
           :x (-&gt; test-images
                  first
                  (ndarray/reshape [100 1 28 28]))})
</code></pre>

<p>```</p>

<p><img src="https://live.staticflickr.com/65535/48647524478_ca35bef78f.jpg" alt="test-batch" /></p>

<p>```clojure
  (def preds (m/predict-batch model {:data test-images} ))
  (&ndash;>> preds</p>

<pre><code>   first
   (ndarray/argmax-channel)
   (ndarray/-&gt;vec)
   (take 10))
</code></pre>

<p> ;=> (1.0 8.0 8.0 8.0 8.0 8.0 2.0 8.0 8.0 1.0)
```</p>

<p>Yeah, not even close. The real first line of the images is <code>6 1 0 0 3 1 4 8 0 9</code></p>

<p>Let&rsquo;s Train!</p>

<p>```clojure
  (train 3)</p>

<p>;; starting epoch  0
;; {:epoch 0, :metric [accuracy 0.83295]}
;; starting epoch  1
;; {:epoch 1, :metric [accuracy 0.9371333]}
;; starting epoch  2
;; {:epoch 2, :metric [accuracy 0.9547667]}</p>

<p>```</p>

<p>After the training, let&rsquo;s have another look at the predicted labels.</p>

<p>```clojure
  (def preds (m/predict-batch model {:data test-images} ))
  (&ndash;>> preds</p>

<pre><code>   first
   (ndarray/argmax-channel)
   (ndarray/-&gt;vec)
   (take 10))
</code></pre>

<p> ;=> (6.0 1.0 0.0 0.0 3.0 1.0 4.0 8.0 0.0 9.0)
```</p>

<ul>
<li>Predicted = <code>(6.0 1.0 0.0 0.0 3.0 1.0 4.0 8.0 0.0 9.0)</code></li>
<li>Actual = <code>6 1 0 0 3 1 4 8 0 9</code></li>
</ul>


<p>Rock on!</p>

<h3>Closing</h3>

<p>In this post, we focused on the first half of the autoencoder and made a discriminator model that took in an image and gave us a label.</p>

<p>Don&rsquo;t forget to save the trained model for later, we&rsquo;ll be using it.</p>

<p>```clojure
  (m/save-checkpoint model {:prefix &ldquo;model/discriminator&rdquo;</p>

<pre><code>                        :epoch 2})
</code></pre>

<p>```</p>

<p>Until then, here is a picture of Otto the cat in a basket to keep you going.</p>

<p><img src="https://live.staticflickr.com/65535/48647579433_ce703809fa_z.jpg" alt="Otto in basket" /></p>

<p><em>P.S. If you want to run all the code for yourself. It is <a href="https://github.com/gigasquid/clojure-mxnet-autoencoder/blob/master/src/clojure_mxnet_autoencoder/discriminator.clj">here</a></em></p>
]]></content>
  </entry>
  
</feed>
