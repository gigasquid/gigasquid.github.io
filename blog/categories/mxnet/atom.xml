<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MXNet | Squid's Blog]]></title>
  <link href="http://gigasquid.github.io/blog/categories/mxnet/atom.xml" rel="self"/>
  <link href="http://gigasquid.github.io/"/>
  <updated>2019-01-19T16:41:42-05:00</updated>
  <id>http://gigasquid.github.io/</id>
  <author>
    <name><![CDATA[Carin Meier]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Object Detection with Clojure MXNet]]></title>
    <link href="http://gigasquid.github.io/blog/2019/01/19/object-detection-with-clojure-mxnet/"/>
    <updated>2019-01-19T13:34:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2019/01/19/object-detection-with-clojure-mxnet</id>
    <content type="html"><![CDATA[<p><img src="https://c1.staticflickr.com/8/7837/32928474208_4960caafb3.jpg" alt="" /></p>

<p>Object detection just landed in MXNet thanks to the work of contributors <a href="https://github.com/kedarbellare">Kedar Bellare</a> and <a href="https://github.com/hellonico/">Nicolas Modrzyk</a>. Kedar ported over the <code>infer</code> package to Clojure, making inference and prediction much easier for users and Nicolas integrated in his <a href="https://github.com/hellonico/origami">Origami</a> OpenCV library into the the examples to make the visualizations happen.</p>

<p>We&rsquo;ll walk through the main steps to use the <code>infer</code> object detection which include creating the detector with a model and then loading the image and running the inference on it.</p>

<h3>Creating the Detector</h3>

<p>To create the detector you need to define a couple of things:</p>

<ul>
<li>How big is your image?</li>
<li>What model are you going to be using for object detection?</li>
</ul>


<p>In the code below, we are going to be giving it an color image of size 512 x 512.</p>

<p>```clojure
(defn create-detector []
  (let [descriptors [{:name &ldquo;data&rdquo;</p>

<pre><code>                  :shape [1 3 512 512]
                  :layout layout/NCHW
                  :dtype dtype/FLOAT32}]
    factory (infer/model-factory model-path-prefix descriptors)]
(infer/create-object-detector factory)))
</code></pre>

<p>```</p>

<ul>
<li>The shape is going to be <code>[1 3 512 512]</code>.

<ul>
<li>The <code>1</code> is for the batch size which in our case is a single image.</li>
<li>The <code>3</code> is for the channels in the image which for a RGB image is <code>3</code></li>
<li>The <code>512</code> is for the image height and width.</li>
</ul>
</li>
<li>The <code>layout</code> specifies that the shape given is in terms of <code>NCHW</code> which is batch size, channel size, height, and width.</li>
<li>The <code>dtype</code> is the image data type which will be the standard <code>FLOAT32</code></li>
<li>The <code>model-path-prefix</code> points to the place where the trained model we are using for object detection lives.</li>
</ul>


<p>The model we are going to use is the <a href="https://arxiv.org/abs/1512.02325">Single Shot Multiple Box Object Detector (SSD)</a>. You can download the model yourself using this <a href="https://github.com/apache/incubator-mxnet/blob/master/contrib/clojure-package/examples/infer/objectdetector/scripts/get_ssd_data.sh">script</a>.</p>

<h3>How to Load an Image and Run the Detector</h3>

<p>Now that we have a model and a detector, we can load an image up and run the object detection.</p>

<p>To load the image use <code>load-image</code> which will load the image from the path.</p>

<p><code>clojure
(infer/load-image-from-file input-image)
</code></p>

<p>Then run the detection using <code>infer/detect-objects</code> which will give you the top five predictions by default.</p>

<p><code>clojure
(infer/detect-objects detector image)
</code></p>

<p>It will give an output something like this:</p>

<p><code>clojure
[[{:class "person",
   :prob 0.9657765,
   :x-min 0.021868259,
   :y-min 0.049295247,
   :x-max 0.9975169,
   :y-max 0.9734151}
  {:class "dog",
   :prob 0.17513266,
   :x-min 0.16772352,
   :y-min 0.45792937,
   :x-max 0.55409217,
   :y-max 0.72507095}
   ...
]]
</code></p>

<p>which you can then use to draw bounding boxes on the image.</p>

<h3>Try Running the Example</h3>

<p><img src="https://c1.staticflickr.com/8/7804/31862638207_61be3a6e3c_b.jpg" alt="" /></p>

<p>One of the best ways to explore using it is with the <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/infer/objectdetector">object detection example</a> in the MXNet repo. It will be coming out officially in the <code>1.5.0</code> release, but you can get an early peek at it by building the project and running the example with the nightly snapshot.</p>

<p>You can do this by cloning the <a href="https://github.com/apache/incubator-mxnet">MXNet Repo</a> and changing directory to <code>contrib/clojure-package</code>.</p>

<p>Next, edit the <code>project.clj</code> to look like this:</p>

<p>```clojure
(defproject org.apache.mxnet.contrib.clojure/clojure-mxnet &ldquo;1.5.0-SNAPSHOT&rdquo;
  :description &ldquo;Clojure package for MXNet&rdquo;
  :url &ldquo;<a href="https://github.com/apache/incubator-mxnet">https://github.com/apache/incubator-mxnet</a>&rdquo;
  :license {:name &ldquo;Apache License&rdquo;</p>

<pre><code>        :url "http://www.apache.org/licenses/LICENSE-2.0"}
</code></pre>

<p>  :dependencies [[org.clojure/clojure &ldquo;1.9.0&rdquo;]</p>

<pre><code>             [t6/from-scala "0.3.0"]

             ;; To use with nightly snapshot
             ;[org.apache.mxnet/mxnet-full_2.11-osx-x86_64-cpu "&lt;insert-snapshot-version&gt;"]
             ;[org.apache.mxnet/mxnet-full_2.11-linux-x86_64-cpu "&lt;insert-snapshot-version&gt;"]
             ;[org.apache.mxnet/mxnet-full_2.11-linux-x86_64-gpu "&lt;insert-snapshot-version"]

             [org.apache.mxnet/mxnet-full_2.11-osx-x86_64-cpu "1.5.0-SNAPSHOT"]

             ;;; CI
             #_[org.apache.mxnet/mxnet-full_2.11 "INTERNAL"]

             [org.clojure/tools.logging "0.4.0"]
             [org.apache.logging.log4j/log4j-core "2.8.1"]
             [org.apache.logging.log4j/log4j-api "2.8.1"]
             [org.slf4j/slf4j-log4j12 "1.7.25" :exclusions [org.slf4j/slf4j-api]]]
</code></pre>

<p>  :pedantic? :skip
  :plugins [[lein-codox &ldquo;0.10.3&rdquo; :exclusions [org.clojure/clojure]]</p>

<pre><code>        [lein-cloverage "1.0.10" :exclusions [org.clojure/clojure]]
        [lein-cljfmt "0.5.7"]]
</code></pre>

<p>  :codox {:namespaces [#&ldquo;^org.apache.clojure-mxnet.(?!gen).*&rdquo;]}
  :aot [dev.generator]
  :repositories [[&ldquo;staging&rdquo; {:url &ldquo;<a href="https://repository.apache.org/content/repositories/staging">https://repository.apache.org/content/repositories/staging</a>&rdquo;                  :snapshots true</p>

<pre><code>                         :update :always}]
             ["snapshots" {:url "https://repository.apache.org/content/repositories/snapshots"               :snapshots true
                          :update :always}]])
</code></pre>

<p><code>``
If you are running on linux, you should change the</code>mxnet-full_2.11-osx-x86_64-cpu<code>to</code>mxnet-full_2.11-linux-x86_64-cpu`.</p>

<p>Next, go ahead and do <code>lein test</code> to make sure that everything builds ok. If you run into any trouble please refer to <a href="https://github.com/apache/incubator-mxnet/blob/master/contrib/clojure-package/README.md">README</a> for any missing dependencies.</p>

<p>After that do a <code>lein install</code> to install the <code>clojure-mxnet</code> jar to your local maven. Now you are ready to <code>cd examples/infer/object-detection</code> to try it out. Refer to the README for more details.</p>

<p>If you run into any problems getting started, feel free to reach out in the Clojurian #mxnet slack room or open an issue at the MXNet project. We are a friendly group and happy to help out.</p>

<p>Thanks again to the community for the contributions to make this possible. It&rsquo;s great seeing new things coming to life.</p>

<p>Happy Object Detecting!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to GAN a Flan]]></title>
    <link href="http://gigasquid.github.io/blog/2018/12/18/how-to-gan-a-flan/"/>
    <updated>2018-12-18T16:34:00-05:00</updated>
    <id>http://gigasquid.github.io/blog/2018/12/18/how-to-gan-a-flan</id>
    <content type="html"><![CDATA[<p>It&rsquo;s holiday time and that means parties and getting together with friends. Bringing a baked good or dessert to a gathering is a time honored tradition. But what if this year, you could take it to the next level? Everyone brings actual food. But with the help of Deep Learning, you can bring something completely different &ndash;  you can bring the <em>image</em> of baked good! I&rsquo;m not talking about just any old image that someone captured with a camera or created with a pen and paper. I&rsquo;m talking about the computer itself <strong>creating</strong>. This image would be never before seen, totally unique, and crafted by the creative process of the machine.</p>

<p>That is exactly what we are going to do. We are going to create a <em>flan</em></p>

<p><img src="https://c1.staticflickr.com/5/4065/4339500429_aa9c55f246_n.jpg" alt="Photo by Lucia Sanchez on Flickr" /></p>

<p>If you&rsquo;ve never had a flan before, it&rsquo;s a yummy dessert made of a baked custard with caramel sauce on it.</p>

<p>&ldquo;Why a flan?&rdquo;, you may ask. There are quite a few reasons:</p>

<ul>
<li>It&rsquo;s tasty in real life.</li>
<li>Flan rhymes with GAN, <em>(unless you pronounce it &ldquo;Gaaahn&rdquo;)</em>.</li>
<li>Why not?</li>
</ul>


<p>Onto the recipe. How are we actually going to make this work? We need some ingredients:</p>

<ul>
<li><a href="https://clojure.org/">Clojure</a> &ndash; the most advanced programming language to create generative desserts.</li>
<li><a href="https://mxnet.apache.org">Apache MXNet</a> &ndash; a flexible and efficient deep learning library that has a Clojure package.</li>
<li>1000-5000 pictures of flans &ndash; for Deep Learning you need data!</li>
</ul>


<h2>Gather Flan Pictures</h2>

<p>The first thing you want to do is gather your 1000 or more images with a <a href="https://github.com/montoyamoraga/scrapers">scraper</a>. The scraper will crawl google, bing, or instagram and download pictures of <em>mostly</em> flans to your computer. You may have to eyeball and remove any clearly wrong ones from your stash.</p>

<p>Next, you need to gather all these images in a directory and run a tool called <a href="https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py">im2rec.py</a> on them to turn them into an <a href="https://mxnet.incubator.apache.org/tutorials/basic/data.html#loading-data-using-image-iterators">image record iterator</a> for use with MXNet. This will produce an optimized format that will allow our deep learning program to efficiently cycle through them.</p>

<p>Run:</p>

<pre><code>python3 im2rec.py --resize 28 root flan
</code></pre>

<p>to produce a <code>flan.rec</code> file with images resized to 28x28 that we can use next.</p>

<h2>Load Flan Pictures into MXNet</h2>

<p>The next step is to import the image record iterator into the MXNet with the <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">Clojure API</a>. We can do this with the <code>io</code> namespace.</p>

<p>Add this to your require:</p>

<pre><code>[org.apache.clojure-mxnet.io :as mx-io]
</code></pre>

<p>Now, we can load our images:</p>

<p>```clojure
(def flan-iter (mx-io/image-record-iter {:path-imgrec &ldquo;flan.rec&rdquo;</p>

<pre><code>                                     :data-shape [3 28 28]
                                     :batch-size batch-size}))
</code></pre>

<p>```</p>

<p>Now, that we have the images, we need to create our <code>model</code>. This is what is actually going to do the learning and creating of images.</p>

<h2>Creating a GAN model.</h2>

<p>GAN stands for <em>Generative Adversarial Network</em>. This is a incredibly cool deep learning technique that has two different models pitted against each, yet both learning and getting better at the same time. The two models are a generator and a discriminator. The generator model creates a new image from a random noise vector. The discriminator then tries to tell whether the image is a real image or a fake image. We need to create both of these models for our network.</p>

<p>First, the discriminator model. We are going to use the <code>symbol</code> namespace for the clojure package:</p>

<p>```clojure
(defn discriminator []
  (as-> (sym/variable &ldquo;data&rdquo;) data</p>

<pre><code>(sym/convolution "d1" {:data data
                       :kernel [4 4]
                       :pad [3 3]
                       :stride [2 2]
                       :num-filter ndf
                       :no-bias true})
(sym/batch-norm "dbn1" {:data data :fix-gamma true :eps eps})
(sym/leaky-re-lu "dact1" {:data data :act-type "leaky" :slope 0.2})

...
</code></pre>

<p>```</p>

<p>There is a variable for the <code>data</code> coming in, (which is the picture of the flan), it then flows through the other layers which consist of convolutions, normalization, and activation layers. The last three layers actually repeat another two times before ending in the output, which tells whether it thinks the image was a fake or not.</p>

<p>The generator model looks similar:</p>

<p>```clojure
(defn generator []
  (as-> (sym/variable &ldquo;rand&rdquo;) data</p>

<pre><code>(sym/deconvolution "g1" {:data data
                         :kernel [4 4]
                         :pad [0 0]
                         :stride [1 1]
                         :num-filter
                         (* 4 ndf) :no-bias true})
(sym/batch-norm "gbn1" {:data data :fix-gamma true :eps eps})
(sym/activation "gact1" {:data data :act-type "relu"})

...
</code></pre>

<p>```</p>

<p>There is a variable for the <code>data</code> coming in, but this time it is a random noise vector. Another interesting point that is is using a <code>deconvolution</code> layer instead of a <code>convolution</code> layer. The generator is basically the inverse of the discriminator. It starts with a random noise vector, but that is translated up through the layers until it is expanded to a image output.</p>

<p>Next, we iterate through all of our training images in our <code>flan-iter</code> with <code>reduce-batches</code>. Here is just an excerpt where we get a random noise vector and have the generator run the data through and produce the output image:</p>

<p>```clojure
(mx-io/reduce-batches</p>

<pre><code>   flan-iter
   (fn [n batch]
     (let [rbatch (mx-io/next rand-noise-iter)
           dbatch (mapv normalize-rgb-ndarray (mx-io/batch-data batch))
           out-g (-&gt; mod-g
                     (m/forward rbatch)
                     (m/outputs))
</code></pre>

<p>```</p>

<p>The whole code is <a href="https://github.com/gigasquid/mxnet-gan-flan">here</a> for reference, but let&rsquo;s skip forward and run it and see what happens.</p>

<p><img src="/images/gout-96-0.jpg" alt="" /></p>

<p>FLANS!! Well, they could be flans if you squint a bit.</p>

<p>Now that we have them kinda working for a small image size 28x28, let&rsquo;s biggerize it.</p>

<h2>Turn on the Oven and Bake</h2>

<p>Turning up the size to 128x128 requires some alterations in the layers' parameters to make sure that it processes and generates the correct size, but other than that we are good to go.</p>

<p>Here comes the fun part, watching it train and learn:</p>

<h3>Epoch 0</h3>

<p><img src="/images/flan-random-128-0-0.jpg" alt="" /></p>

<p>In the beginning there was nothing but random noise.</p>

<h3>Epoch 10</h3>

<p><img src="/images/flan-random-128-10-0.jpg" alt="" /></p>

<p>It&rsquo;s beginning to learn colors! Red, yellow, brown seem to be important to flans.</p>

<h3>Epoch 23</h3>

<p><img src="/images/flan-random-128-23-0.jpg" alt="" /></p>

<p>It&rsquo;s learning shapes! It has learned that flans seem to be blob shaped.</p>

<h3>Epoch 33</h3>

<p><img src="/images/flan-random-128-33-0.jpg" alt="" /></p>

<p>It is moving into its surreal phase. Salvidor Dali would be proud of these flans.</p>

<h3>Epoch 45</h3>

<p><img src="/images/flan-random-128-45.jpg" alt="" /></p>

<p>Things take a weird turn. Does that flan have eyes?</p>

<h3>Epoch 68</h3>

<p><img src="/images/flan-random-128-68-0.jpg" alt="" /></p>

<p>Even worse. Are those demonic flans? Should we even continue down this path?</p>

<p>Answer: Yes &ndash; <strong>the training must go on..</strong></p>

<h3>Epoch 161</h3>

<p><img src="/images/flan-random-161-0.jpg" alt="" /></p>

<p>Big moment here. It looks like something that could possibly be edible.</p>

<h3>Epoch 170</h3>

<p><img src="/images/flan-random-170-0.jpg" alt="" /></p>

<p>Ick! Green Flans! No one is going to want that.</p>

<h3>Epoch 195</h3>

<p><img src="/images/explore-195.jpg" alt="" /></p>

<p>We&rsquo;ve achieved maximum flan, (for the time being).</p>

<h2>Explore</h2>

<p>If you are interested in playing around with the pretrained model, you can check it out <a href="https://github.com/gigasquid/mxnet-gan-flan/blob/master/src/mxnet_gan_flan/gan.clj#L355">here with the pretrained function</a>.
It will load up the trained model and generate flans for you to explore and bring to your dinner parties.</p>

<p>Wrapping up, training GANs is a <em>lot</em> of fun. With MXNet, you can bring the fun with you to Clojure.</p>

<p>Want more, check out this Clojure Conj video &ndash;  <a href="https://www.youtube.com/watch?v=yzfnlcHtwiY">Can You GAN?</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clojure MXNet - The Module API]]></title>
    <link href="http://gigasquid.github.io/blog/2018/07/05/clojure-mxnet-the-module-api/"/>
    <updated>2018-07-05T19:39:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2018/07/05/clojure-mxnet-the-module-api</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg">https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg</a>"></p>

<p>This is an introduction to the high level Clojure API for deep learning library <a href="http://mxnet.incubator.apache.org/">MXNet</a>.</p>

<p>The module API provides an intermediate and high-level interface for performing computation with neural networks in MXNet.</p>

<p>To follow along with this documentation, you can use this namespace to with the needed requires:</p>

<p>```clojure
(ns docs.module
  (:require [clojure.java.io :as io]</p>

<pre><code>        [clojure.java.shell :refer [sh]]
        [org.apache.clojure-mxnet.eval-metric :as eval-metric]
        [org.apache.clojure-mxnet.io :as mx-io]
        [org.apache.clojure-mxnet.module :as m]
        [org.apache.clojure-mxnet.symbol :as sym]
        [org.apache.clojure-mxnet.ndarray :as ndarray]))
</code></pre>

<p>```</p>

<h2>Prepare the Data</h2>

<p>In this example, we are going to use the MNIST data set. If you have cloned the MXNet repo and <code>cd contrib/clojure-package</code>, we can run some helper scripts to download the data for us.</p>

<p>```clojure
(def data-dir &ldquo;data/&rdquo;)</p>

<p>(when-not (.exists (io/file (str data-dir &ldquo;train-images-idx3-ubyte&rdquo;)))
  (sh &ldquo;../../scripts/get_mnist_data.sh&rdquo;))
```</p>

<p>MXNet provides function in the <code>io</code> namespace to load the MNIST datasets into training and test data iterators that we can use with our module.</p>

<p>```clojure
(def train-data (mx-io/mnist-iter {:image (str data-dir &ldquo;train-images-idx3-ubyte&rdquo;)</p>

<pre><code>                               :label (str data-dir "train-labels-idx1-ubyte")
                               :label-name "softmax_label"
                               :input-shape [784]
                               :batch-size 10
                               :shuffle true
                               :flat true
                               :silent false
                               :seed 10}))
</code></pre>

<p>(def test-data (mx-io/mnist-iter {:image (str data-dir &ldquo;t10k-images-idx3-ubyte&rdquo;)</p>

<pre><code>                              :label (str data-dir "t10k-labels-idx1-ubyte")
                              :input-shape [784]
                              :batch-size 10
                              :flat true
                              :silent false}))
</code></pre>

<p>```</p>

<h2>Preparing a Module for Computation</h2>

<p>To construct a module, we need to have a symbol as input. This symbol takes input data in the first layer and then has subsequent layers of fully connected and relu activation layers, ending up in a softmax layer for output.</p>

<p>```clojure
(let [data (sym/variable &ldquo;data&rdquo;)</p>

<pre><code>  fc1 (sym/fully-connected "fc1" {:data data :num-hidden 128})
  act1 (sym/activation "relu1" {:data fc1 :act-type "relu"})
  fc2 (sym/fully-connected "fc2" {:data act1 :num-hidden 64})
  act2 (sym/activation "relu2" {:data fc2 :act-type "relu"})
  fc3 (sym/fully-connected "fc3" {:data act2 :num-hidden 10})
  out (sym/softmax-output "softmax" {:data fc3})]
</code></pre>

<p>  out)
  ;=>#object[org.apache.mxnet.Symbol 0x1f43a406 &ldquo;org.apache.mxnet.Symbol@1f43a406&rdquo;]
```</p>

<p>You can also write this with the <code>as-&gt;</code> threading macro.</p>

<p>```clojure
(def out (as-> (sym/variable &ldquo;data&rdquo;) data</p>

<pre><code>       (sym/fully-connected "fc1" {:data data :num-hidden 128})
       (sym/activation "relu1" {:data data :act-type "relu"})
       (sym/fully-connected "fc2" {:data data :num-hidden 64})
       (sym/activation "relu2" {:data data :act-type "relu"})
       (sym/fully-connected "fc3" {:data data :num-hidden 10})
       (sym/softmax-output "softmax" {:data data})))
</code></pre>

<p>;=> #&lsquo;tutorial.module/out
```</p>

<p>By default, <code>context</code> is the CPU. If you need data parallelization, you can specify a GPU context or an array of GPU contexts like this <code>(m/module out {:contexts [(context/gpu)]})</code></p>

<p>Before you can compute with a module, you need to call <code>bind</code> to allocate the device memory and <code>init-params</code> or <code>set-params</code> to initialize the parameters. If you simply want to fit a module, you don’t need to call <code>bind</code> and <code>init-params</code> explicitly, because the <code>fit</code> function automatically calls them if they are needed.</p>

<p>```clojure
(let [mod (m/module out)]
  (&ndash;> mod</p>

<pre><code>  (m/bind {:data-shapes (mx-io/provide-data train-data)
           :label-shapes (mx-io/provide-label train-data)})
  (m/init-params)))
</code></pre>

<p>```</p>

<p>Now you can compute with the module using functions like <code>forward</code>, <code>backward</code>, etc.</p>

<h2>Training and Predicting</h2>

<p>Modules provide high-level APIs for training, predicting, and evaluating. To fit a module, call the <code>fit</code> function with some data iterators:</p>

<p><code>clojure
(def mod (m/fit (m/module out) {:train-data train-data :eval-data test-data :num-epoch 1}))
;; Epoch  0  Train- [accuracy 0.12521666]
;; Epoch  0  Time cost- 8392
;; Epoch  0  Validation-  [accuracy 0.2227]
</code></p>

<p>You can pass in batch-end callbacks using batch-end-callback and epoch-end callbacks using epoch-end-callback in the <code>fit-params</code>. You can also set parameters using functions like in the fit-params like optimizer and eval-metric. To learn more about the fit-params, see the fit-param function options. To predict with a module, call <code>predict</code> with a DataIter:</p>

<p>```clojure
(def results (m/predict mod {:eval-data test-data}))
(first results) ;=>#object[org.apache.mxnet.NDArray 0x3540b6d3 &ldquo;org.apache.mxnet.NDArray@a48686ec&rdquo;]</p>

<p>(first (ndarray/&ndash;>vec (first results))) ;=>0.08261358
```</p>

<p>The module collects and returns all of the prediction results. For more details about the format of the return values, see the documentation for the <code>predict</code> function.</p>

<p>When prediction results might be too large to fit in memory, use the <code>predict-every-batch</code> API.</p>

<p>```clojure
(let [preds (m/predict-every-batch mod {:eval-data test-data})]
  (mx-io/reduce-batches test-data</p>

<pre><code>                    (fn [i batch]
                      (println (str "pred is " (first (get preds i))))
                      (println (str "label is " (mx-io/batch-label batch)))
                      ;;; do something
                      (inc i))))
</code></pre>

<p>```</p>

<p>If you need to evaluate on a test set and don’t need the prediction output, call the <code>score</code> function with a data iterator and an eval metric:</p>

<p><code>clojure
(m/score mod {:eval-data test-data :eval-metric (eval-metric/accuracy)}) ;=&gt;["accuracy" 0.2227]
</code></p>

<p>This runs predictions on each batch in the provided data iterator and computes the evaluation score using the provided eval metric. The evaluation results are stored in metric so that you can query later.</p>

<h2>Saving and Loading</h2>

<p>To save the module parameters in each training epoch, use a <code>checkpoint</code> function:</p>

<p>```clojure
(let [save-prefix &ldquo;my-model&rdquo;]
  (doseq [epoch-num (range 3)]</p>

<pre><code>(mx-io/do-batches train-data (fn [batch
                                      ;; do something
</code></pre>

<p>]))</p>

<pre><code>(m/save-checkpoint mod {:prefix save-prefix :epoch epoch-num :save-opt-states true})))
</code></pre>

<p>;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0000.params
;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0000.states
;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0001.params
;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0001.states
;; INFO  org.apache.mxnet.module.Module: Saved checkpoint to my-model-0002.params
;; INFO  org.apache.mxnet.module.Module: Saved optimizer state to my-model-0002.states</p>

<p>```</p>

<p>To load the saved module parameters, call the <code>load-checkpoint</code> function:</p>

<p>```clojure
(def new-mod (m/load-checkpoint {:prefix &ldquo;my-model&rdquo; :epoch 1 :load-optimizer-states true}))</p>

<p>new-mod ;=> #object[org.apache.mxnet.module.Module 0x5304d0f4 &ldquo;org.apache.mxnet.module.Module@5304d0f4&rdquo;]
```</p>

<p>To initialize parameters, Bind the symbols to construct executors first with bind function. Then, initialize the parameters and auxiliary states by calling <code>init-params</code> function.</p>

<p>```clojure
(&ndash;> new-mod</p>

<pre><code>(m/bind {:data-shapes (mx-io/provide-data train-data) :label-shapes (mx-io/provide-label train-data)})
(m/init-params))
</code></pre>

<p>```</p>

<p>To get current parameters, use <code>params</code></p>

<p>```clojure</p>

<p>(let [[arg-params aux-params] (m/params new-mod)]
  {:arg-params arg-params
   :aux-params aux-params})</p>

<p>;; {:arg-params
;;  {&ldquo;fc3_bias&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x39adc3b0 &ldquo;org.apache.mxnet.NDArray@49caf426&rdquo;],
;;   &ldquo;fc2_weight&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x25baf623 &ldquo;org.apache.mxnet.NDArray@a6c8f9ac&rdquo;],
;;   &ldquo;fc1_bias&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x6e089973 &ldquo;org.apache.mxnet.NDArray@9f91d6eb&rdquo;],
;;   &ldquo;fc3_weight&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x756fd109 &ldquo;org.apache.mxnet.NDArray@2dd0fe3c&rdquo;],
;;   &ldquo;fc2_bias&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x1dc69c8b &ldquo;org.apache.mxnet.NDArray@d128f73d&rdquo;],
;;   &ldquo;fc1_weight&rdquo;
;;   #object[org.apache.mxnet.NDArray 0x20abc769 &ldquo;org.apache.mxnet.NDArray@b8e1c5e8&rdquo;]},
;;  :aux-params {}}</p>

<p>```</p>

<p>To assign parameter and aux state values, use <code>set-params</code> function.</p>

<p><code>clojure
(m/set-params new-mod {:arg-params (m/arg-params new-mod) :aux-params (m/aux-params new-mod)})
;=&gt; #object[org.apache.mxnet.module.Module 0x5304d0f4 "org.apache.mxnet.module.Module@5304d0f4"]
</code></p>

<p>To resume training from a saved checkpoint, instead of calling <code>set-params</code>, directly call <code>fit</code>, passing the loaded parameters, so that <code>fit</code> knows to start from those parameters instead of initializing randomly</p>

<p>Create fit-params, and then use it to set <code>begin-epoch</code> so that <code>fit</code> knows to resume from a saved epoch.</p>

<p>```clojure
;; reset the training data before calling fit or you will get an error
(mx-io/reset train-data)
(mx-io/reset test-data)</p>

<p>(m/fit new-mod {:train-data train-data :eval-data test-data :num-epoch 2</p>

<pre><code>            :fit-params (-&gt; (m/fit-params {:begin-epoch 1}))})
</code></pre>

<p>```</p>

<p>If you are interested in checking out MXNet and exploring on your own, check out the main page <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">here</a> with instructions on how to install and other information.</p>

<h3>See other blog posts about MXNet</h3>

<ul>
<li><a href="http://gigasquidsoftware.com/blog/2018/06/03/meet-clojure-mxnet-ndarray/">Clojure MXNet &ndash; NDArray</a></li>
<li><a href="http://gigasquidsoftware.com/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/">Clojure MXNet Joins Apache MXNet</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clojure MXNet Joins the Apache MXNet Project]]></title>
    <link href="http://gigasquid.github.io/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project/"/>
    <updated>2018-07-01T10:44:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2018/07/01/clojure-mxnet-joins-the-apache-mxnet-project</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg">https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg</a>"></p>

<p>I&rsquo;m delighted to share the news that the Clojure package for <a href="https://mxnet.apache.org/">MXNet</a> has now joined the main Apache MXNet project. A big thank you to the efforts of everyone involved to make this possible. Having it as part of the main project is a great place for growth and collaboration that will benefit both MXNet and the Clojure community.</p>

<h2>Invitation to Join and Contribute</h2>

<p>The Clojure package has been brought in as a <em>contrib</em> <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">clojure-package</a>. It is still very new and will go through a period of feedback, stabilization, and improvement before it graduates out of contrib.</p>

<p>We welcome contributors and people getting involved to make it better.</p>

<p>Are you interested in Deep Learning and Clojure? Great &ndash; Join us!</p>

<p>There are a few ways to get involved.</p>

<ul>
<li>Check out the current state of the Clojure package some contribution needs here <a href="https://cwiki.apache.org/confluence/display/MXNET/Clojure+Package+Contribution+Needs">https://cwiki.apache.org/confluence/display/MXNET/Clojure+Package+Contribution+Needs</a></li>
<li>Join the Clojurian Slack #mxnet channel</li>
<li>Join the <a href="https://lists.apache.org/list.html?dev@mxnet.apache.org">MXNet dev mailing list</a> by sending an email to <code>dev-subscribe@mxnet.apache.org.</code>.</li>
<li>Join the MXNET Slack channel &ndash; You have to join the MXnet dev mailing list first, but after that says you would like to join the slack and someone will add you.</li>
<li>Join the <a href="https://discuss.mxnet.io/">MXNet Discussion Forum</a></li>
</ul>


<h3>Want to Learn More?</h3>

<p>There are lots of examples in the package to check out, but a good place to start are the tutorials here <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial">https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial</a></p>

<p>There is a blog walkthough here as well &ndash; <a href="http://gigasquidsoftware.com/blog/2018/07/05/clojure-mxnet-the-module-api/">Clojure MXNet Module API</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Meet Clojure MXNet - NDArray]]></title>
    <link href="http://gigasquid.github.io/blog/2018/06/03/meet-clojure-mxnet-ndarray/"/>
    <updated>2018-06-03T16:13:00-04:00</updated>
    <id>http://gigasquid.github.io/blog/2018/06/03/meet-clojure-mxnet-ndarray</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg">https://cdn-images-1.medium.com/max/800/1*OoqsrMD7JzXAvRUGx_8_fg.jpeg</a>"></p>

<p>This is the beginning of a series of blog posts to get to know the <a href="https://mxnet.apache.org/">Apache MXNet</a> Deep Learning project and the new Clojure language binding <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package">clojure-package</a></p>

<p>MXNet is a first class, modern deep learning library that AWS has officially picked as its chosen library. It supports multiple languages on a first class basis and is incubating as an Apache project.</p>

<p>The motivation for creating a Clojure package is to be able to open the deep learning library to the Clojure ecosystem and build bridges for future development and innovation for the community. It provides all the needed tools including low level and high level apis, dynamic graphs, and things like GAN and natural language support.</p>

<p>So let&rsquo;s get on with our introduction with one of the basic building blocks of MXNet, the <code>NDArray</code>.</p>

<h2>Meet NDArray</h2>

<p>The <code>NDArray</code> is the tensor data structure in MXNet. Let&rsquo;s start of by creating one. First we need to require the <code>ndarray</code> namespace:</p>

<p><code>clojure
(ns tutorial.ndarray
  (:require [org.apache.clojure-mxnet.ndarray :as ndarray]))
</code></p>

<p>Now let&rsquo;s create an all zero array of dimension 100 x 50</p>

<p><code>clojure
(ndarray/zeros [100 50])
;=&gt; #object[org.apache.mxnet.NDArray 0x3e396d0 "org.apache.mxnet.NDArray@aeea40b6"]
</code></p>

<p>We can check the shape of this by using <code>shape-vec</code></p>

<p><code>clojure
(ndarray/shape-vec (ndarray/zeros [100 50]))
;=&gt; [100 50]
</code></p>

<p>There is also a quick way to create an ndarray of ones with the <code>ones</code> function:</p>

<p><code>clojure
(ndarray/ones [256 32 128 1])
</code></p>

<p>Ones and zeros are nice, but what an array with specific contents? There is an <code>array</code> function for that. Specific the contents of the array first and the shape second:</p>

<p><code>clojure
(def c (ndarray/array [1 2 3 4 5 6] [2 3]))
(ndarray/shape-vec c)  ;=&gt; [2 3]
</code></p>

<p>To convert it back to a vector format, we can use the <code>-&gt;vec</code> function.</p>

<p><code>clojure
(ndarray/-&gt;vec c)
;=&gt; [1.0 2.0 3.0 4.0 5.0 6.0]
</code></p>

<p>Now that we know how to create NDArrays, we can get to do something interesting like operations on them.</p>

<h3>Operations</h3>

<p>There are all the standard arithmetic operations:</p>

<p><code>clojure
(def a (ndarray/ones [1 5]))
(def b (ndarray/ones [1 5]))
(-&gt; (ndarray/+ a b) (ndarray/-&gt;vec))
;=&gt;  [2.0 2.0 2.0 2.0 2.0]
</code></p>

<p>Note that the original ndarrays are unchanged.</p>

<p><code>clojure
(ndarray/-&gt;vec a) ;=&gt; [1.0 1.0 1.0 1.0 1.0]
(ndarray/-&gt;vec b) ;=&gt; [1.0 1.0 1.0 1.0 1.0]
</code></p>

<p>But, we can change that if we use the inplace operators:</p>

<p><code>clojure
(ndarray/+= a b)
(ndarray/-&gt;vec a) ;=&gt;  [2.0 2.0 2.0 2.0 2.0]
</code></p>

<p>There are many more operations, but just to give you a taste, we&rsquo;ll take a look a the <code>dot</code> product operation:</p>

<p><code>clojure
(def arr1 (ndarray/array [1 2] [1 2]))
(def arr2 (ndarray/array [3 4] [2 1]))
(def res (ndarray/dot arr1 arr2))
(ndarray/shape-vec res) ;=&gt; [1 1]
(ndarray/-&gt;vec res) ;=&gt; [11.0]
</code></p>

<p>If you are curious about the other operators available in NDArray API check out the <a href="https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html">MXNet project documentation page</a></p>

<p>Now that we have ndarrays and can do calculations on them, we might want to save and load them.</p>

<h3>Saving and Loading</h3>

<p>You can save ndarrays with a name as a map like:</p>

<p><code>clojure
(ndarray/save "filename" {"arr1" arr1 "arr2" arr2})
</code></p>

<p>To load them, you just specify the filename and the map is returned.</p>

<p><code>clojure
(ndarray/load "filename")
;=&gt; {"arr1" #object[org.apache.mxnet.NDArray 0x1b629ff4 "org.apache.mxnet.NDArray@63da08cb"]
;=&gt;  "arr2" #object[org.apache.mxnet.NDArray 0x25d994e3 "org.apache.mxnet.NDArray@5bbaf2c3"]}
</code></p>

<p>One more cool thing, we can even due our operations on the cpu or gpu.</p>

<h3>Multi-Device Support</h3>

<p>When creating an <code>ndarray</code> you can use a context argument to specify the device. To do this, we will need the help of the <code>context</code> namespace.</p>

<p><code>clojure
(require '[org.apache.clojure-mxnet.context :as context])
</code></p>

<p>By default, the <code>ndarray</code> is created on the cpu context.</p>

<p><code>clojure
(def cpu-a (ndarray/zeros [100 200]))
(ndarray/context cpu-a)
;=&gt; #object[ml.dmlc.mxnet.Context 0x3f376123 "cpu(0)"]
</code></p>

<p>But we can specify the gpu instead, (if we have a gpu enabled build).</p>

<p><code>clojure
(def gpu-b (ndarray/zeros [100 200] {:ctx (context/gpu 0)}))
</code></p>

<p><em>Note: Operations among different contexts are currently not allowed, but there is a <code>copy-to</code> function that can help copy the content from one device to another and then continue on with the computation.</em></p>

<h2>Wrap up</h2>

<p>I hope you&rsquo;ve enjoyed the brief introduction to the MXNet library, there is much more to explore in future posts. If you are interested in giving it a try, there are native jars for OSX cpu and Linux cpu/gpu available and the code for the ndarray tutorial can be found <a href="https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package/examples/tutorial">here</a></p>

<p><em>Please remember that the library is in a experimential state, so if you encounter any problems or have any other feedback, please log an issue so bugs and rough edges can be fixed :).</em></p>
]]></content>
  </entry>
  
</feed>
