<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
  <meta charset="utf-8">
  <title>Neural Networks in Clojure With core.matrix - Squid's Blog</title>
  <meta name="author" content="Carin Meier">

  
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://gigasquid.github.io/blog/2013/12/02/neural-networks-in-clojure-with-core-dot-matrix/">
  <link href="/favicon.png" type="image/png" rel="icon">
  <link href="/atom.xml" rel="alternate" title="Squid's Blog" type="application/atom+xml">

  <!-- http://opengraphprotocol.org/ -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://gigasquid.github.io/blog/2013/12/02/neural-networks-in-clojure-with-core-dot-matrix/">
  <meta property="og:title" content="Neural Networks in Clojure With core.matrix - Squid's Blog">
  

  <script src="/javascripts/libs/jquery/jquery-2.1.3.min.js"></script>

<link href="/assets/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/assets/bootstrap/dist/css/bootstrap-theme.min.css" rel="stylesheet" type="text/css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">


  
  <link href="/stylesheets/screen.css" media="screen, projection, print" rel="stylesheet" type="text/css">

  

</head>

  <body   >
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="wrap">
      
        <header role="banner">
          <nav class="navbar navbar-default" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" title="toggle navbar" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Squid's Blog</a>
        </div>

        <div class="navbar-collapse collapse">
            <ul class="nav navbar-nav">
                <li ><a href="/blog/archives">Archives</a></li>
<li ><a href="/about">About</a></li>
<li ><a href="/books">Books</a></li>
<li ><a href="/speaking">Speaking</a></li>

            </ul>
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a class="subscribe-rss" href="/atom.xml" title="subscribe via RSS">
                        <span class="visible-xs">RSS</span>
                        <img class="hidden-xs" src="/images/rss.png" alt="RSS">
                    </a>
                </li>
                
            </ul>
            
                <form class="navbar-form navbar-right" action="http://google.com/search" method="GET">
                    <input type="hidden" name="sitesearch" value="gigasquid.github.io">
                    <div class="form-group">
                        <input class="form-control" type="text" name="q" placeholder="Search">
                    </div>
                </form>
            
        </div>
    </div>
</nav>


        </header>
      
      <div id="main" role="main" class="container">
        <div id="content">
          <div class="row">
  <div class="page-content" itemscope itemtype="http://schema.org/Blog">
    <meta itemprop="name" content="Squid's Blog" />
    
    <meta itemprop="url" content="http://gigasquid.github.io" />
    <article class="hentry" role="article" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
      
  <header class="page-header">
    
      <p class="meta text-muted text-uppercase">
        












<span class="glyphicon glyphicon-calendar"></span> <time datetime="2013-12-02T19:28:00-05:00"  data-updated="true" itemprop="datePublished dateCreated">Mon  2 Dec 2013,  7:28 PM</time>
        
           | <a href="#disqus_thread" itemprop="discussionUrl"
             data-disqus-identifier="http://gigasquid.github.io">Comments</a>
        
      </p>
    
    
    <h1 class="entry-title" itemprop="name headline">
        Neural Networks in Clojure With core.matrix
        
    </h1>
    
  </header>


<div class="entry-content clearfix" itemprop="articleBody description"><p>After having spent some time recently looking at top-down AI, I
thought I would spend some time looking at bottom&rsquo;s up AI, machine
learning and neural networks.</p>

<p>I was pleasantly introduced to <a href="https://twitter.com/mikera">@mikea&rsquo;s</a> <a href="https://github.com/mikera/core.matrix">core.matrix</a> at Clojure Conj
this year and wanted to try making my own neural network using the
library. The purpose of this blog is to share my learnings along the
way.</p>

<h2>What is a neural network?</h2>

<p>A neural network is an approach to machine learning that involves
simulating, (in an idealized way), the way our brains work on a
biological level.  There are three layers to neural network: the input
layer, the hidden layers, and the output layer.  Each layer consists
of neurons that have a value.  In each layer, each neuron is connected to
the neuron in the next layer by a connection strength. To get data
into the neural network, you assign values to the input layer, (values
between 0 and 1). These values are then &ldquo;fed forward&rdquo; to the hidden layer neurons though an algorithm that
relies on the input values and the connection strengths. The values
are finally &ldquo;fed forward&rdquo; in a similar fashion to the output layer.
The &ldquo;learning&rdquo; portion of the neural network comes from &ldquo;training&rdquo; the
network data.  The training data consists of a collection of
associated input values and target values. The training process at a
high level looks like this:</p>

<ul>
<li>Feed forward input values to get the output values</li>
<li>How far off are the output values from the target values?</li>
<li>Calculate the error values and adjust the strengths of the network</li>
<li>Repeat until you think it has &ldquo;learned&rdquo; enough, that is when you
feed the input values in the result of the output values are close
enough to the target you are looking for</li>
</ul>


<p>The beauty of this system is that the neural network, (given the right
configuration and the right training), can approximate any function &ndash;
just by exposing it to data.</p>

<h2>Start Small</h2>

<p>I wanted to start with a very small network so that I could understand
the algorithms and actually do the maths for the tests along the way.
The network configuration I chose is one with 1 hidden layer.  The
input layer has 2 neurons, the hidden layer has 3 neurons and the
output layer has 2 neurons.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="c1">;;Neurons</span>
</span><span class='line'><span class="c1">;;  Input Hidden  Output</span>
</span><span class='line'><span class="c1">;;  A     1       C</span>
</span><span class='line'><span class="c1">;;  B     2       D</span>
</span><span class='line'><span class="c1">;;        3</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c1">;; Connection Strengths</span>
</span><span class='line'><span class="c1">;; Input to Hidden =&gt; [[A1 A2 A3] [B1 B2 B3]]</span>
</span><span class='line'><span class="c1">;; Hidden to Output =&gt; [[1C 1D] [2C 2D] [3C 3D]]</span>
</span></code></pre></td></tr></table></div></figure>


<p>In this example we have:</p>

<ul>
<li>Input Neurons: neuronA neuronB</li>
<li>Hidden Neurons: neuron1 neuron2 neuron3</li>
<li>Output Neurons: neuronC neuronD</li>
<li>Connections between the Input and Hidden Layers

<ul>
<li>neuronA-neuron1</li>
<li>neuronA-neuron2</li>
<li>neuronA-neuron3</li>
<li>neuronB-neuron1</li>
<li>neuronB-neuron2</li>
<li>neuronB-neuron3</li>
</ul>
</li>
<li>Connections betwen the Hidden and Output Layers

<ul>
<li>neuron1-nerounC</li>
<li>neuron1-nerounD</li>
<li>neuron2-nerounC</li>
<li>neuron2-nerounD</li>
<li>neuron3-nerounC</li>
<li>neuron3-nerounD</li>
</ul>
</li>
</ul>


<p>To give us a concrete example to work with, let&rsquo;s actually assign all
our neurons and connection strengths to some real values.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">input-neurons</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">])</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">input-hidden-strengths</span> <span class="p">[</span> <span class="p">[</span><span class="mf">0.12</span> <span class="mf">0.2</span> <span class="mf">0.13</span><span class="p">]</span>
</span><span class='line'>                              <span class="p">[</span><span class="mf">0.01</span> <span class="mf">0.02</span> <span class="mf">0.03</span><span class="p">]])</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">hidden-neurons</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">])</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">hidden-output-strengths</span> <span class="p">[[</span><span class="mf">0.15</span> <span class="mf">0.16</span><span class="p">]</span>
</span><span class='line'>                              <span class="p">[</span><span class="mf">0.02</span> <span class="mf">0.03</span><span class="p">]</span>
</span><span class='line'>                              <span class="p">[</span><span class="mf">0.01</span> <span class="mf">0.02</span><span class="p">]])</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Feed Forward</h2>

<p>Alright, we have values in the input neuron layer, let&rsquo;s feed them
forward through the network. The new value of neuron in the hidden
layer is the sum of all the inputs of its connections multiplied by
the connection strength.  The neuron can also have its own threshold,
(meaning you would subtract the threshold from the sum of inputs), but
to keep things a simple as possible in this example, the threshold is
0 &ndash; so we will ignore it.  The sum is then feed into an activation
function, that has an output in the range of -1 to 1.  The activation
function is the tanh function.  We will also need the derivative of
the tanh function a little later when we are calculating errors, so we
will define both here.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">activation-fn</span> <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">x</span><span class="p">]</span> <span class="p">(</span><span class="nf">Math/tanh</span> <span class="nv">x</span><span class="p">)))</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">dactivation-fn</span> <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">y</span><span class="p">]</span> <span class="p">(</span><span class="nb">- </span><span class="mf">1.0</span> <span class="p">(</span><span class="nb">* </span><span class="nv">y</span> <span class="nv">y</span><span class="p">))))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">layer-activation</span> <span class="p">[</span><span class="nv">inputs</span> <span class="nv">strengths</span><span class="p">]</span>
</span><span class='line'>  <span class="s">&quot;forward propagate the input of a layer&quot;</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">mapv</span> <span class="nv">activation-fn</span>
</span><span class='line'>      <span class="p">(</span><span class="nf">mapv</span> <span class="o">#</span><span class="p">(</span><span class="nb">reduce + </span><span class="nv">%</span><span class="p">)</span>
</span><span class='line'>       <span class="p">(</span><span class="nb">* </span><span class="nv">inputs</span> <span class="p">(</span><span class="nf">transpose</span> <span class="nv">strengths</span><span class="p">)))))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note how nice core.matrix works on multipling vectors &lt;3.</p>

<p>So now if we calculate the hidden neuron values from the input [1 0],
we get:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="nf">layer-activation</span> <span class="nv">input-neurons</span> <span class="nv">input-hidden-strengths</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt;  [0.11942729853438588 0.197375320224904 0.12927258360605834]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s just remember those hidden neuron values for our next step</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">new-hidden-neurons</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">layer-activation</span> <span class="nv">input-neurons</span> <span class="nv">input-hidden-strengths</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now we do the same thing to calculate the output values</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="nf">layer-activation</span> <span class="nv">new-hidden-neurons</span> <span class="nv">hidden-output-strengths</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt;  [0.02315019005321053 0.027608061500083565]</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">new-output-neurons</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">layer-activation</span> <span class="nv">new-hidden-neurons</span> <span class="nv">hidden-output-strengths</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Alright!  We have our answer
[0.02315019005321053 0.027608061500083565].
Notice that the values are pretty much the same.  This is because we
haven&rsquo;t trained our network to do anything yet.</p>

<h2>Backwards Propagation</h2>

<p>To train our network, we have to let it know what the answer,(or
target), should be, so we can calculate the errors and finally update
our connection strengths. For this simple example, let&rsquo;s just inverse
the data &ndash; so given an input of [1 0] should give us an output of
[0 1].</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">targets</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span><span class="p">])</span>
</span></code></pre></td></tr></table></div></figure>


<p>`</p>

<h3>Calculate the errors of the output layer</h3>

<p>The first errors that we need to calculate are the ones for the output
layer.  This is found by subtracting the target value form the actual
value and then multiplying by the gradient/ derivative of the
activation function</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">output-deltas</span> <span class="p">[</span><span class="nv">targets</span> <span class="nv">outputs</span><span class="p">]</span>
</span><span class='line'>  <span class="s">&quot;measures the delta errors for the output layer (Desired value â€“ actual value) and multiplying it by the gradient of the activation function&quot;</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">* </span><span class="p">(</span><span class="nf">mapv</span> <span class="nv">dactivation-fn</span> <span class="nv">outputs</span><span class="p">)</span>
</span><span class='line'>     <span class="p">(</span><span class="nb">- </span><span class="nv">targets</span> <span class="nv">outputs</span><span class="p">)))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nf">output-deltas</span> <span class="nv">targets</span> <span class="nv">new-output-neurons</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt; [-0.023137783141771645 0.9716507764442904]</span>
</span></code></pre></td></tr></table></div></figure>


<p>`</p>

<p>Great let&rsquo;s remember this output deltas for later</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">odeltas</span> <span class="p">(</span><span class="nf">output-deltas</span> <span class="nv">targets</span> <span class="nv">new-output-neurons</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Calculate the errors of the hidden layer</h3>

<p>The errors of the hidden layer are based off the deltas that we just
found from the output layer.  In fact, for each hidden neuron, the
error delta is the gradient of the activation function multiplied by
the weighted sum of the ouput deltas of connected ouput neurons and
it&rsquo;s connection strength.  This should remind you of the forward
propagation of the inputs &ndash; but this time we are doing it backwards
with the error deltas.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">hlayer-deltas</span> <span class="p">[</span><span class="nv">odeltas</span> <span class="nv">neurons</span> <span class="nv">strengths</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">* </span><span class="p">(</span><span class="nf">mapv</span> <span class="nv">dactivation-fn</span> <span class="nv">neurons</span><span class="p">)</span>
</span><span class='line'>     <span class="p">(</span><span class="nf">mapv</span> <span class="o">#</span><span class="p">(</span><span class="nb">reduce + </span><span class="nv">%</span><span class="p">)</span>
</span><span class='line'>           <span class="p">(</span><span class="nb">* </span><span class="nv">odeltas</span> <span class="nv">strengths</span><span class="p">))))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nf">hlayer-deltas</span>
</span><span class='line'>    <span class="nv">odeltas</span>
</span><span class='line'>    <span class="nv">new-hidden-neurons</span>
</span><span class='line'>    <span class="nv">hidden-output-strengths</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt;  [0.14982559238071416 0.027569216735265096 0.018880751432503236]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Great let&rsquo;s remember the hidden layer error deltas for later</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">hdeltas</span> <span class="p">(</span><span class="nf">hlayer-deltas</span>
</span><span class='line'>              <span class="nv">odeltas</span>
</span><span class='line'>              <span class="nv">new-hidden-neurons</span>
</span><span class='line'>              <span class="nv">hidden-output-strengths</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Updating the connection strengths</h3>

<p>Great!  We have all the error deltas, now we are ready to go ahead and
update the connection strengths.  In general this is the same process
for both the hidden-output connections and the input-hidden
connections.</p>

<ul>
<li>weight-change = error-delta * neuron-value</li>
<li>new-weight = weight + learning rate * weight-change</li>
</ul>


<p>The learning rate controls how fast the weigths and errors should be
adjusted.  It the learning rate is too high, then there is the danger
that it will converge to fit the solution too fast and not find the
best solution.  If the learning rate is too low, it may never actually
converge to the right solution given the training data that it is
using. For this example, let&rsquo;s use a training rate of 0.2</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">update-strengths</span> <span class="p">[</span><span class="nv">deltas</span> <span class="nv">neurons</span> <span class="nv">strengths</span> <span class="nv">lrate</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">+ </span><span class="nv">strengths</span> <span class="p">(</span><span class="nb">* </span><span class="nv">lrate</span>
</span><span class='line'>                  <span class="p">(</span><span class="nf">mapv</span> <span class="o">#</span><span class="p">(</span><span class="nb">* </span><span class="nv">deltas</span> <span class="nv">%</span><span class="p">)</span> <span class="nv">neurons</span><span class="p">))))</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Update the hidden-output strengths</h3>

<p>Updating this layer we are going to look at</p>

<ul>
<li>weight-change = odelta * hidden value</li>
<li>new-weight = weight + (learning rate * weight-change)</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="nf">update-strengths</span>
</span><span class='line'>       <span class="nv">odeltas</span>
</span><span class='line'>       <span class="nv">new-hidden-neurons</span>
</span><span class='line'>       <span class="nv">hidden-output-strengths</span>
</span><span class='line'>       <span class="nv">learning-rate</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt; [[0.14944734341306073 0.18320832546991603]</span>
</span><span class='line'>    <span class="p">[</span><span class="mf">0.019086634528619688</span> <span class="mf">0.06835597662949369</span><span class="p">]</span>
</span><span class='line'>    <span class="p">[</span><span class="mf">0.009401783798869296</span> <span class="mf">0.04512156124675721</span><span class="p">]]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Of course, let&rsquo;s remember these values too</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">new-hidden-output-strengths</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">update-strengths</span>
</span><span class='line'>       <span class="nv">odeltas</span>
</span><span class='line'>       <span class="nv">new-hidden-neurons</span>
</span><span class='line'>       <span class="nv">hidden-output-strengths</span>
</span><span class='line'>       <span class="nv">learning-rate</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Update the input-hidden strengths</h3>

<p>We are going to do the same thing with the input-hidden strengths too.</p>

<ul>
<li>weight-change = hdelta * input value</li>
<li>new-weight = weight + (learning rate * weight-change)</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'> <span class="p">(</span><span class="nf">update-strengths</span>
</span><span class='line'>           <span class="nv">hdeltas</span>
</span><span class='line'>           <span class="nv">input-neurons</span>
</span><span class='line'>           <span class="nv">input-hidden-strengths</span>
</span><span class='line'>           <span class="nv">learning-rate</span><span class="p">)</span>
</span><span class='line'><span class="c1">;=&gt;  [[0.14996511847614283 0.20551384334705303 0.13377615028650064]</span>
</span><span class='line'>           <span class="p">[</span><span class="mf">0.01</span> <span class="mf">0.02</span> <span class="mf">0.03</span><span class="p">]]</span>
</span></code></pre></td></tr></table></div></figure>


<p>These are our new strengths</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">new-input-hidden-strengths</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">update-strengths</span>
</span><span class='line'>       <span class="nv">hdeltas</span>
</span><span class='line'>       <span class="nv">input-neurons</span>
</span><span class='line'>       <span class="nv">input-hidden-strengths</span>
</span><span class='line'>       <span class="nv">learning-rate</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Putting the pieces together</h2>

<p>We have done it!  In our simple example we have:</p>

<ul>
<li>Forward propagated the input to get the output</li>
<li>Calculated the errors from the target through backpropogation</li>
<li>Updated the connection strengths/ weights</li>
</ul>


<p>We just need to put all the pieces together. We&rsquo;ll do this with the
values that we got earlier to make sure it is all working.</p>

<h3>Construct a network representation</h3>

<p>It would be nice if we could represent an entire neural network in a
data structure.  That way the whole transformation of feeding forward
and training the network could give us a new network back.
So lets define the data structure as
[input-neurons input-hidden-strengths hidden-neurons hidden-output-strengths output-neurons].</p>

<p>We will start off with all the values of the neurons being zero.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">nn</span> <span class="p">[</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">input-hidden-strengths</span> <span class="nv">hidden-neurons</span>
</span><span class='line'><span class="nv">hidden-output-strengths</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span><span class="p">]])</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Generalized feed forward</h3>

<p>Now we can make a feed forward function that takes this network and
constructs a new network based on input values and the
layer-activation function that we defined earlier.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">feed-forward</span> <span class="p">[</span><span class="nv">input</span> <span class="nv">network</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="k">let </span><span class="p">[[</span><span class="nv">in</span> <span class="nv">i-h-strengths</span> <span class="nv">h</span> <span class="nv">h-o-strengths</span> <span class="nv">out</span><span class="p">]</span> <span class="nv">network</span>
</span><span class='line'>        <span class="nv">new-h</span> <span class="p">(</span><span class="nf">layer-activation</span> <span class="nv">input</span> <span class="nv">i-h-strengths</span><span class="p">)</span>
</span><span class='line'>        <span class="nv">new-o</span> <span class="p">(</span><span class="nf">layer-activation</span> <span class="nv">new-h</span> <span class="nv">h-o-strengths</span><span class="p">)]</span>
</span><span class='line'>    <span class="p">[</span><span class="nv">input</span> <span class="nv">i-h-strengths</span> <span class="nv">new-h</span> <span class="nv">h-o-strengths</span> <span class="nv">new-o</span><span class="p">]))</span>
</span></code></pre></td></tr></table></div></figure>


<p>This should match up with the values that we got earlier when we were
just working on the individual pieces.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="nf">testing</span> <span class="s">&quot;feed forward&quot;</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">is</span> <span class="p">(</span><span class="nb">== </span><span class="p">[</span><span class="nv">input-neurons</span> <span class="nv">input-hidden-strengths</span> <span class="nv">new-hidden-neurons</span> <span class="nv">hidden-output-strengths</span> <span class="nv">new-output-neurons</span><span class="p">]</span>
</span><span class='line'>          <span class="p">(</span><span class="nf">feed-forward</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">nn</span><span class="p">))))</span>
</span></code></pre></td></tr></table></div></figure>


<p>`</p>

<h3>Generalized update weights / connection strengths</h3>

<p>We can make a similiar update-weights function that calculate the
errors and returns back a new network with the updated weights</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">update-weights</span> <span class="p">[</span><span class="nv">network</span> <span class="nv">target</span> <span class="nv">learning-rate</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="k">let </span><span class="p">[[</span> <span class="nv">in</span> <span class="nv">i-h-strengths</span> <span class="nv">h</span> <span class="nv">h-o-strengths</span> <span class="nv">out</span><span class="p">]</span> <span class="nv">network</span>
</span><span class='line'>        <span class="nv">o-deltas</span> <span class="p">(</span><span class="nf">output-deltas</span> <span class="nv">target</span> <span class="nv">out</span><span class="p">)</span>
</span><span class='line'>        <span class="nv">h-deltas</span> <span class="p">(</span><span class="nf">hlayer-deltas</span> <span class="nv">o-deltas</span> <span class="nv">h</span> <span class="nv">h-o-strengths</span><span class="p">)</span>
</span><span class='line'>        <span class="nv">n-h-o-strengths</span> <span class="p">(</span><span class="nf">update-strengths</span>
</span><span class='line'>                         <span class="nv">o-deltas</span>
</span><span class='line'>                         <span class="nv">h</span>
</span><span class='line'>                         <span class="nv">h-o-strengths</span>
</span><span class='line'>                         <span class="nv">learning-rate</span><span class="p">)</span>
</span><span class='line'>        <span class="nv">n-i-h-strengths</span> <span class="p">(</span><span class="nf">update-strengths</span>
</span><span class='line'>                         <span class="nv">h-deltas</span>
</span><span class='line'>                         <span class="nv">in</span>
</span><span class='line'>                         <span class="nv">i-h-strengths</span>
</span><span class='line'>                         <span class="nv">learning-rate</span><span class="p">)]</span>
</span><span class='line'>    <span class="p">[</span><span class="nv">in</span> <span class="nv">n-i-h-strengths</span> <span class="nv">h</span> <span class="nv">n-h-o-strengths</span> <span class="nv">out</span><span class="p">]))</span>
</span></code></pre></td></tr></table></div></figure>


<p>This too should match up with the pieces from the earlier examples.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="nf">testing</span> <span class="s">&quot;update-weights&quot;</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">is</span> <span class="p">(</span> <span class="nb">== </span><span class="p">[</span><span class="nv">input-neurons</span>
</span><span class='line'>            <span class="nv">new-input-hidden-strengths</span>
</span><span class='line'>            <span class="nv">new-hidden-neurons</span>
</span><span class='line'>            <span class="nv">new-hidden-output-strengths</span>
</span><span class='line'>            <span class="nv">new-output-neurons</span><span class="p">]</span>
</span><span class='line'>           <span class="p">(</span><span class="nf">update-weights</span> <span class="p">(</span><span class="nf">feed-forward</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">nn</span><span class="p">)</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span><span class="p">]</span> <span class="mf">0.2</span><span class="p">))))</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Generalized train network</h3>

<p>Now we can make a function that takes input and a target and feeds the
input forward and then updates the weights.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">train-network</span> <span class="p">[</span><span class="nv">network</span> <span class="nv">input</span> <span class="nv">target</span> <span class="nv">learning-rate</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">update-weights</span> <span class="p">(</span><span class="nf">feed-forward</span> <span class="nv">input</span> <span class="nv">network</span><span class="p">)</span> <span class="nv">target</span> <span class="nv">learning-rate</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nf">testing</span> <span class="s">&quot;train-network&quot;</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">is</span> <span class="p">(</span><span class="nb">== </span><span class="p">[</span><span class="nv">input-neurons</span>
</span><span class='line'>            <span class="nv">new-input-hidden-strengths</span>
</span><span class='line'>            <span class="nv">new-hidden-neurons</span>
</span><span class='line'>            <span class="nv">new-hidden-output-strengths</span>
</span><span class='line'>           <span class="nv">new-output-neurons</span><span class="p">]</span>
</span><span class='line'>          <span class="p">(</span><span class="nf">train-network</span> <span class="nv">nn</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span><span class="p">]</span> <span class="mf">0.2</span><span class="p">))))</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Try it out!</h2>

<p>We are ready to try it out!  Let&rsquo;s train our network on a few examples
of inversing the data</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">n1</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">nn</span>
</span><span class='line'>     <span class="p">(</span><span class="nf">train-network</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span><span class="p">]</span> <span class="mf">0.5</span><span class="p">)</span>
</span><span class='line'>     <span class="p">(</span><span class="nf">train-network</span> <span class="p">[</span><span class="mf">0.5</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="mf">0.5</span><span class="p">]</span> <span class="mf">0.5</span><span class="p">)</span>
</span><span class='line'>     <span class="p">(</span><span class="nf">train-network</span> <span class="p">[</span><span class="mf">0.25</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="mf">0.25</span><span class="p">]</span> <span class="mf">0.5</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure>


<p>We&rsquo;ll also make a helper function that just returns the output
neurons for the feed-forward function.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">ff</span> <span class="p">[</span><span class="nv">input</span> <span class="nv">network</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">last </span><span class="p">(</span><span class="nf">feed-forward</span> <span class="nv">input</span> <span class="nv">network</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s look at the results of the untrained and the trained networks</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="c1">;;untrained</span>
</span><span class='line'><span class="p">(</span><span class="nf">ff</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">nn</span><span class="p">)</span> <span class="c1">;=&gt; [0.02315019005321053 0.027608061500083565]</span>
</span><span class='line'><span class="c1">;;trained</span>
</span><span class='line'><span class="p">(</span><span class="nf">ff</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">n1</span><span class="p">)</span> <span class="c1">;=&gt; [0.03765676393050254 0.10552175312900794]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Whoa!  The trained example isn&rsquo;t perfect, but we can see that it is
getting closer to the right answer.  It is learning!</p>

<h2>MOR Training Data</h2>

<p>Well this is really cool and it is working.  But it would be nicer to
be able to present a set of training data for it to learn on.  For
example, it would be nice to have a training data structure look like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">[</span> <span class="p">[</span><span class="nv">input</span> <span class="nv">target</span><span class="p">]</span> <span class="p">[</span><span class="nv">input</span> <span class="nv">target</span><span class="p">]</span> <span class="nv">...</span> <span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s go ahead and define that.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">train-data</span> <span class="p">[</span><span class="nv">network</span> <span class="nv">data</span> <span class="nv">learning-rate</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nb">if-let </span><span class="p">[[</span><span class="nv">input</span> <span class="nv">target</span><span class="p">]</span> <span class="p">(</span><span class="nb">first </span><span class="nv">data</span><span class="p">)]</span>
</span><span class='line'>    <span class="p">(</span><span class="nf">recur</span>
</span><span class='line'>     <span class="p">(</span><span class="nf">train-network</span> <span class="nv">network</span> <span class="nv">input</span> <span class="nv">target</span> <span class="nv">learning-rate</span><span class="p">)</span>
</span><span class='line'>     <span class="p">(</span><span class="nb">rest </span><span class="nv">data</span><span class="p">)</span>
</span><span class='line'>     <span class="nv">learning-rate</span><span class="p">)</span>
</span><span class='line'>    <span class="nv">network</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s try that out on the example earlier</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">n2</span> <span class="p">(</span><span class="nf">train-data</span> <span class="nv">nn</span> <span class="p">[</span>
</span><span class='line'>                        <span class="p">[[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span><span class="p">]]</span>
</span><span class='line'>                        <span class="p">[[</span><span class="mf">0.5</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="mf">0.5</span><span class="p">]]</span>
</span><span class='line'>                        <span class="p">[[</span><span class="mf">0.25</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="mf">0.25</span><span class="p">]</span> <span class="p">]</span>
</span><span class='line'>                        <span class="p">]</span>
</span><span class='line'>                    <span class="mf">0.5</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nf">ff</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">n2</span><span class="p">)</span> <span class="c1">;=&gt; [0.03765676393050254 0.10552175312900794]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Cool. We can now train on data sets. That means we can construct data
sets out of infinite lazy sequences too.  Let&rsquo;s make a lazy training
set of inputs and their inverse.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">inverse-data</span> <span class="p">[]</span>
</span><span class='line'>  <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">n</span> <span class="p">(</span><span class="nb">rand </span><span class="mi">1</span><span class="p">)]</span>
</span><span class='line'>    <span class="p">[[</span><span class="nv">n</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="mi">0</span> <span class="nv">n</span><span class="p">]]))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s see how well our network is doing after we train it with some
more data</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">n3</span> <span class="p">(</span><span class="nf">train-data</span> <span class="nv">nn</span> <span class="p">(</span><span class="nf">repeatedly</span> <span class="mi">400</span> <span class="nv">inverse-data</span><span class="p">)</span> <span class="mf">0.5</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="nf">ff</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">n3</span><span class="p">)</span> <span class="c1">;=&gt; [-4.958278484025221E-4 0.8211647699205362]</span>
</span><span class='line'><span class="p">(</span><span class="nf">ff</span> <span class="p">[</span><span class="mf">0.5</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">n3</span><span class="p">)</span> <span class="c1">;=&gt; [2.1645760787874696E-4 0.5579396715416916]</span>
</span><span class='line'><span class="p">(</span><span class="nf">ff</span> <span class="p">[</span><span class="mf">0.25</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">n3</span><span class="p">)</span> <span class="c1">;=&gt; [1.8183385523103048E-4 0.31130601296149013]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Wow. The more examples it sees, the better that network is doing at
learning what to do!</p>

<h3>General Construct Network</h3>

<p>The only piece that we are missing now is to have a function that will
create a general neural network for us.  We can choose how many input
nerurons, hidden neurons, and output neurons and have a network
constructed with random weights.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">gen-strengths</span> <span class="p">[</span><span class="nv">to</span> <span class="nv">from</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">l</span> <span class="p">(</span><span class="nb">* </span><span class="nv">to</span> <span class="nv">from</span><span class="p">)]</span>
</span><span class='line'>    <span class="p">(</span><span class="nb">map </span><span class="nv">vec</span> <span class="p">(</span><span class="nf">partition</span> <span class="nv">from</span> <span class="p">(</span><span class="nf">repeatedly</span> <span class="nv">l</span> <span class="o">#</span><span class="p">(</span><span class="nb">rand </span><span class="p">(</span><span class="nb">/ </span><span class="mi">1</span> <span class="nv">l</span><span class="p">)))))))</span>
</span><span class='line'>
</span><span class='line'><span class="p">(</span><span class="kd">defn </span><span class="nv">construct-network</span> <span class="p">[</span><span class="nv">num-in</span> <span class="nv">num-hidden</span> <span class="nv">num-out</span><span class="p">]</span>
</span><span class='line'>  <span class="p">(</span><span class="nf">vec</span> <span class="p">(</span><span class="nb">map </span><span class="nv">vec</span> <span class="p">[(</span><span class="nb">repeat </span><span class="nv">num-in</span> <span class="mi">0</span><span class="p">)</span>
</span><span class='line'>             <span class="p">(</span><span class="nf">gen-strengths</span> <span class="nv">num-in</span> <span class="nv">num-hidden</span><span class="p">)</span>
</span><span class='line'>             <span class="p">(</span><span class="nb">repeat </span><span class="nv">num-hidden</span> <span class="mi">0</span><span class="p">)</span>
</span><span class='line'>             <span class="p">(</span><span class="nf">gen-strengths</span> <span class="nv">num-hidden</span> <span class="nv">num-out</span><span class="p">)</span>
</span><span class='line'>             <span class="p">(</span><span class="nb">repeat </span><span class="nv">num-out</span> <span class="mi">0</span><span class="p">)])))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now we can construct our network from scratch and train it.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='clojure'><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">tnn</span> <span class="p">(</span><span class="nf">construct-network</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">2</span><span class="p">))</span>
</span><span class='line'><span class="p">(</span><span class="k">def </span><span class="nv">n5</span> <span class="p">(</span><span class="nf">train-data</span> <span class="nv">tnn</span> <span class="p">(</span><span class="nf">repeatedly</span> <span class="mi">1000</span> <span class="nv">inverse-data</span><span class="p">)</span> <span class="mf">0.2</span><span class="p">))</span>
</span><span class='line'><span class="p">(</span><span class="nf">ff</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">]</span> <span class="nv">n4</span><span class="p">)</span> <span class="c1">;=&gt; [-4.954958580800465E-4 0.8160149309699489]</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that&rsquo;s it.  We have constucted a neural network with core.matrix</p>

<h2>Want more?</h2>

<p>I put together a github library based on the neural network code in
the posts.  It is called <a href="https://github.com/gigasquid/k9">K9</a>, named
after Dr. Who&rsquo;s best dog friend.  You can find the examples we have
gone through in the tests.  There is also an example program using it
in the examples directory.  It learns what colors are based on thier
RGB value.</p>

<p>There are a couple web resources I would recommend if you want to look
farther as well.</p>

<ul>
<li><a href="http://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/">Basic Network Tutorial</a></li>
<li><a href="http://www.youtube.com/watch?v=QJ1qgCr09j8&amp;feature=player_embedded">Mike Anderson&rsquo;s Clojure Conj talk on Neural Networks</a></li>
</ul>


<p>Go forth and create Neural Networks!</p>
</div>


      <footer class="post-footer">
        <p class="meta text-muted">
          
  



<figure class="author-image">
    <span class="img" href="/about" style="background-image: url(/images/avatar.jpg)"><span class="hidden">Picture</span></span>
</figure>

<section class="author">
    <h4><span class="byline author vcard" itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="fn" itemprop="name">Carin Meier</span></span></h4>

    <div class="author-meta">
        <span class="author-link icon-link"><i class="fa fa-link" aria-hidden="true"></i> <a href="http://gigasquid.github.io">http://gigasquid.github.io</a></span>
    </div>
</section>

<hr>

<section class="share">
    
    <h4>Share this post</h4>
    
    <a class="fa fa-twitter" href="https://twitter.com/intent/tweet?url=http://gigasquid.github.io/blog/2013/12/02/neural-networks-in-clojure-with-core-dot-matrix/;" 
        onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
        <span class="hidden">Twitter</span>
    </a>
    <a class="fa fa-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://gigasquid.github.io/blog/2013/12/02/neural-networks-in-clojure-with-core-dot-matrix/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
        <span class="hidden">Facebook</span>
    </a>
    <a class="fa fa-google-plus" href="https://plus.google.com/share?url=http://gigasquid.github.io/blog/2013/12/02/neural-networks-in-clojure-with-core-dot-matrix/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
        <span class="hidden">Google+</span>
    </a>
    
</section>




<!--
<footer class="post-footer">


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=Instant%20Movie%20Streamer%20v3%20Release&amp;url=http://iyask.me/instant-movie-streamer-v3-release/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://iyask.me/instant-movie-streamer-v3-release/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://iyask.me/instant-movie-streamer-v3-release/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>


        </footer>


-->
          












<span class="glyphicon glyphicon-calendar"></span> <time datetime="2013-12-02T19:28:00-05:00"  data-updated="true" itemprop="datePublished dateCreated">Mon  2 Dec 2013,  7:28 PM</time>
          <br>

<span class="glyphicon glyphicon-tags"></span>&nbsp;
<span class="categories">
  
    <a class='category' href='/blog/categories/all/'>All</a>, <a class='category' href='/blog/categories/clojure/'>Clojure</a>, <a class='category' href='/blog/categories/machine-learning/'>Machine Learning</a>
  
</span>


        </p>
        
          <div class="pager">
            
            
              
                <a href="/blog/2013/11/02/embrace-and-reach/" class="col-xs-12 col-md-4 btn btn-default" title="Previous Post: Embrace and Reach"> 
                  <div class="text-muted">
                    <small>Previous Post</small>
                  </div>
                  <div class="pager-title">
                    <h4>Embrace and Reach</h4>
                  </div>
                </a>
              
            
            
            
              
              <a href="/blog/2013/12/26/guide-to-leaving-your-mac-laptop/" class="col-xs-12 col-md-4 btn btn-default pull-right" title="Next Post: Guide to Leaving your Mac Laptop">
                <div class="text-muted">
                  <small>Next Post</small>
                </div>
                <div class="pager-title">
                  <h4>Guide to Leaving your Mac Laptop</h4>
                </div>
              </a>
              
            
            
          </div>
        
      </footer>
    </article>
    
      <section>
        <h2>Comments</h2>
        <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
      </section>
    
  </div>
</div>

        </div>
      </div>
    </div>
    <footer role="contentinfo"><div class="container">
    <p class="text-muted credits">
  Copyright &copy; 2019 - Carin Meier<br>
  <small>
      <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>,
      <span class="credit">customized with <a href="https://github.com/bhrigu123/abacus">abacus theme</a></span>.
  </small>
</p>

</div>
</footer>
    

<script type="text/javascript">
      var disqus_shortname = 'squidsblog';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://gigasquid.github.io/blog/2013/12/02/neural-networks-in-clojure-with-core-dot-matrix/';
        var disqus_url = 'http://gigasquid.github.io/blog/2013/12/02/neural-networks-in-clojure-with-core-dot-matrix/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>


<script src="/assets/bootstrap/dist/js/bootstrap.min.js"></script>
<script src="/javascripts/modernizr.js"></script>


  </body>
</html>
